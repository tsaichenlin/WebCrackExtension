{"ast":null,"code":"import { AIMessage, HumanMessage, coerceMessageLikeToMessage } from \"../messages/index.js\";\nimport { RUN_KEY } from \"../outputs.js\";\nimport { BaseLanguageModel } from \"./base.js\";\nimport { CallbackManager } from \"../callbacks/manager.js\";\n/**\n * Creates a transform stream for encoding chat message chunks.\n * @deprecated Use {@link BytesOutputParser} instead\n * @returns A TransformStream instance that encodes chat message chunks.\n */\nexport function createChatMessageChunkEncoderStream() {\n  const textEncoder = new TextEncoder();\n  return new TransformStream({\n    transform(chunk, controller) {\n      controller.enqueue(textEncoder.encode(typeof chunk.content === \"string\" ? chunk.content : JSON.stringify(chunk.content)));\n    }\n  });\n}\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\nexport class BaseChatModel extends BaseLanguageModel {\n  constructor(fields) {\n    super(fields);\n    // Only ever instantiated in main LangChain\n    Object.defineProperty(this, \"lc_namespace\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: [\"langchain\", \"chat_models\", this._llmType()]\n    });\n  }\n  _separateRunnableConfigFromCallOptions(options) {\n    const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n    if (callOptions?.timeout && !callOptions.signal) {\n      callOptions.signal = AbortSignal.timeout(callOptions.timeout);\n    }\n    return [runnableConfig, callOptions];\n  }\n  /**\n   * Invokes the chat model with a single input.\n   * @param input The input for the language model.\n   * @param options The call options.\n   * @returns A Promise that resolves to a BaseMessageChunk.\n   */\n  async invoke(input, options) {\n    const promptValue = BaseChatModel._convertInputToPromptValue(input);\n    const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n    const chatGeneration = result.generations[0][0];\n    // TODO: Remove cast after figuring out inheritance\n    return chatGeneration.message;\n  }\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(_messages, _options, _runManager) {\n    throw new Error(\"Not implemented.\");\n  }\n  async *_streamIterator(input, options) {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (this._streamResponseChunks === BaseChatModel.prototype._streamResponseChunks) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseChatModel._convertInputToPromptValue(input);\n      const messages = prompt.toChatMessages();\n      const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(options);\n      const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, {\n        verbose: this.verbose\n      });\n      const extra = {\n        options: callOptions,\n        invocation_params: this?.invocationParams(callOptions),\n        batch_size: 1\n      };\n      const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), [messages], undefined, undefined, extra, undefined, undefined, runnableConfig.runName);\n      let generationChunk;\n      try {\n        for await (const chunk of this._streamResponseChunks(messages, callOptions, runManagers?.[0])) {\n          yield chunk.message;\n          if (!generationChunk) {\n            generationChunk = chunk;\n          } else {\n            generationChunk = generationChunk.concat(chunk);\n          }\n        }\n      } catch (err) {\n        await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMError(err)));\n        throw err;\n      }\n      await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMEnd({\n        // TODO: Remove cast after figuring out inheritance\n        generations: [[generationChunk]]\n      })));\n    }\n  }\n  /** @ignore */\n  async _generateUncached(messages, parsedOptions, handledOptions) {\n    const baseMessages = messages.map(messageList => messageList.map(coerceMessageLikeToMessage));\n    // create callback manager and start run\n    const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, {\n      verbose: this.verbose\n    });\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this?.invocationParams(parsedOptions),\n      batch_size: 1\n    };\n    const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), baseMessages, undefined, undefined, extra, undefined, undefined, handledOptions.runName);\n    // generate results\n    const results = await Promise.allSettled(baseMessages.map((messageList, i) => this._generate(messageList, {\n      ...parsedOptions,\n      promptIndex: i\n    }, runManagers?.[i])));\n    // handle results\n    const generations = [];\n    const llmOutputs = [];\n    await Promise.all(results.map(async (pResult, i) => {\n      if (pResult.status === \"fulfilled\") {\n        const result = pResult.value;\n        generations[i] = result.generations;\n        llmOutputs[i] = result.llmOutput;\n        return runManagers?.[i]?.handleLLMEnd({\n          generations: [result.generations],\n          llmOutput: result.llmOutput\n        });\n      } else {\n        // status === \"rejected\"\n        await runManagers?.[i]?.handleLLMError(pResult.reason);\n        return Promise.reject(pResult.reason);\n      }\n    }));\n    // create combined output\n    const output = {\n      generations,\n      llmOutput: llmOutputs.length ? this._combineLLMOutput?.(...llmOutputs) : undefined\n    };\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers ? {\n        runIds: runManagers?.map(manager => manager.runId)\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  async _generateCached({\n    messages,\n    cache,\n    llmStringKey,\n    parsedOptions,\n    handledOptions\n  }) {\n    const baseMessages = messages.map(messageList => messageList.map(coerceMessageLikeToMessage));\n    // create callback manager and start run\n    const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, {\n      verbose: this.verbose\n    });\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this?.invocationParams(parsedOptions),\n      batch_size: 1,\n      cached: true\n    };\n    const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), baseMessages, undefined, undefined, extra, undefined, undefined, handledOptions.runName);\n    // generate results\n    const missingPromptIndices = [];\n    const results = await Promise.allSettled(baseMessages.map(async (baseMessage, index) => {\n      // Join all content into one string for the prompt index\n      const prompt = BaseChatModel._convertInputToPromptValue(baseMessage).toString();\n      const result = await cache.lookup(prompt, llmStringKey);\n      if (result == null) {\n        missingPromptIndices.push(index);\n      }\n      return result;\n    }));\n    // Map run managers to the results before filtering out null results\n    // Null results are just absent from the cache.\n    const cachedResults = results.map((result, index) => ({\n      result,\n      runManager: runManagers?.[index]\n    })).filter(({\n      result\n    }) => result.status === \"fulfilled\" && result.value != null || result.status === \"rejected\");\n    // Handle results and call run managers\n    const generations = [];\n    await Promise.all(cachedResults.map(async ({\n      result: promiseResult,\n      runManager\n    }, i) => {\n      if (promiseResult.status === \"fulfilled\") {\n        const result = promiseResult.value;\n        generations[i] = result;\n        if (result.length) {\n          await runManager?.handleLLMNewToken(result[0].text);\n        }\n        return runManager?.handleLLMEnd({\n          generations: [result]\n        });\n      } else {\n        // status === \"rejected\"\n        await runManager?.handleLLMError(promiseResult.reason);\n        return Promise.reject(promiseResult.reason);\n      }\n    }));\n    const output = {\n      generations,\n      missingPromptIndices\n    };\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers ? {\n        runIds: runManagers?.map(manager => manager.runId)\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  /**\n   * Generates chat based on the input messages.\n   * @param messages An array of arrays of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generate(messages, options, callbacks) {\n    // parse call options\n    let parsedOptions;\n    if (Array.isArray(options)) {\n      parsedOptions = {\n        stop: options\n      };\n    } else {\n      parsedOptions = options;\n    }\n    const baseMessages = messages.map(messageList => messageList.map(coerceMessageLikeToMessage));\n    const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(parsedOptions);\n    runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n    if (!this.cache) {\n      return this._generateUncached(baseMessages, callOptions, runnableConfig);\n    }\n    const {\n      cache\n    } = this;\n    const llmStringKey = this._getSerializedCacheKeyParametersForCall(callOptions);\n    const {\n      generations,\n      missingPromptIndices\n    } = await this._generateCached({\n      messages: baseMessages,\n      cache,\n      llmStringKey,\n      parsedOptions: callOptions,\n      handledOptions: runnableConfig\n    });\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      const results = await this._generateUncached(missingPromptIndices.map(i => baseMessages[i]), callOptions, runnableConfig);\n      await Promise.all(results.generations.map(async (generation, index) => {\n        const promptIndex = missingPromptIndices[index];\n        generations[promptIndex] = generation;\n        // Join all content into one string for the prompt index\n        const prompt = BaseChatModel._convertInputToPromptValue(baseMessages[promptIndex]).toString();\n        return cache.update(prompt, llmStringKey, generation);\n      }));\n      llmOutput = results.llmOutput ?? {};\n    }\n    return {\n      generations,\n      llmOutput\n    };\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options) {\n    return {};\n  }\n  _modelType() {\n    return \"base_chat_model\";\n  }\n  /**\n   * @deprecated\n   * Return a json-like object representing this LLM.\n   */\n  serialize() {\n    return {\n      ...this.invocationParams(),\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n  }\n  /**\n   * Generates a prompt based on the input prompt values.\n   * @param promptValues An array of BasePromptValue instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generatePrompt(promptValues, options, callbacks) {\n    const promptMessages = promptValues.map(promptValue => promptValue.toChatMessages());\n    return this.generate(promptMessages, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Makes a single call to the chat model.\n   * @param messages An array of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async call(messages, options, callbacks) {\n    const result = await this.generate([messages.map(coerceMessageLikeToMessage)], options, callbacks);\n    const generations = result.generations;\n    return generations[0][0].message;\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Makes a single call to the chat model with a prompt value.\n   * @param promptValue The value of the prompt.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async callPrompt(promptValue, options, callbacks) {\n    const promptMessages = promptValue.toChatMessages();\n    return this.call(promptMessages, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Predicts the next message based on the input messages.\n   * @param messages An array of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async predictMessages(messages, options, callbacks) {\n    return this.call(messages, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Predicts the next message based on a text input.\n   * @param text The text input.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a string.\n   */\n  async predict(text, options, callbacks) {\n    const message = new HumanMessage(text);\n    const result = await this.call([message], options, callbacks);\n    if (typeof result.content !== \"string\") {\n      throw new Error(\"Cannot use predict when output is not a string.\");\n    }\n    return result.content;\n  }\n}\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\nexport class SimpleChatModel extends BaseChatModel {\n  async _generate(messages, options, runManager) {\n    const text = await this._call(messages, options, runManager);\n    const message = new AIMessage(text);\n    if (typeof message.content !== \"string\") {\n      throw new Error(\"Cannot generate with a simple chat model when output is not a string.\");\n    }\n    return {\n      generations: [{\n        text: message.content,\n        message\n      }]\n    };\n  }\n}","map":{"version":3,"names":["AIMessage","HumanMessage","coerceMessageLikeToMessage","RUN_KEY","BaseLanguageModel","CallbackManager","createChatMessageChunkEncoderStream","textEncoder","TextEncoder","TransformStream","transform","chunk","controller","enqueue","encode","content","JSON","stringify","BaseChatModel","constructor","fields","Object","defineProperty","enumerable","configurable","writable","value","_llmType","_separateRunnableConfigFromCallOptions","options","runnableConfig","callOptions","timeout","signal","AbortSignal","invoke","input","promptValue","_convertInputToPromptValue","result","generatePrompt","callbacks","chatGeneration","generations","message","_streamResponseChunks","_messages","_options","_runManager","Error","_streamIterator","prototype","prompt","messages","toChatMessages","callbackManager_","configure","tags","metadata","verbose","extra","invocation_params","invocationParams","batch_size","runManagers","handleChatModelStart","toJSON","undefined","runName","generationChunk","concat","err","Promise","all","map","runManager","handleLLMError","handleLLMEnd","_generateUncached","parsedOptions","handledOptions","baseMessages","messageList","results","allSettled","i","_generate","promptIndex","llmOutputs","pResult","status","llmOutput","reason","reject","output","length","_combineLLMOutput","runIds","manager","runId","_generateCached","cache","llmStringKey","cached","missingPromptIndices","baseMessage","index","toString","lookup","push","cachedResults","filter","promiseResult","handleLLMNewToken","text","generate","Array","isArray","stop","_getSerializedCacheKeyParametersForCall","generation","update","_modelType","serialize","_type","_model","promptValues","promptMessages","call","callPrompt","predictMessages","predict","SimpleChatModel","_call"],"sources":["/Users/mandylin/Desktop/WebCrack React/webcrack/node_modules/@langchain/core/dist/language_models/chat_models.js"],"sourcesContent":["import { AIMessage, HumanMessage, coerceMessageLikeToMessage, } from \"../messages/index.js\";\nimport { RUN_KEY, } from \"../outputs.js\";\nimport { BaseLanguageModel, } from \"./base.js\";\nimport { CallbackManager, } from \"../callbacks/manager.js\";\n/**\n * Creates a transform stream for encoding chat message chunks.\n * @deprecated Use {@link BytesOutputParser} instead\n * @returns A TransformStream instance that encodes chat message chunks.\n */\nexport function createChatMessageChunkEncoderStream() {\n    const textEncoder = new TextEncoder();\n    return new TransformStream({\n        transform(chunk, controller) {\n            controller.enqueue(textEncoder.encode(typeof chunk.content === \"string\"\n                ? chunk.content\n                : JSON.stringify(chunk.content)));\n        },\n    });\n}\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\nexport class BaseChatModel extends BaseLanguageModel {\n    constructor(fields) {\n        super(fields);\n        // Only ever instantiated in main LangChain\n        Object.defineProperty(this, \"lc_namespace\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: [\"langchain\", \"chat_models\", this._llmType()]\n        });\n    }\n    _separateRunnableConfigFromCallOptions(options) {\n        const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n        if (callOptions?.timeout && !callOptions.signal) {\n            callOptions.signal = AbortSignal.timeout(callOptions.timeout);\n        }\n        return [runnableConfig, callOptions];\n    }\n    /**\n     * Invokes the chat model with a single input.\n     * @param input The input for the language model.\n     * @param options The call options.\n     * @returns A Promise that resolves to a BaseMessageChunk.\n     */\n    async invoke(input, options) {\n        const promptValue = BaseChatModel._convertInputToPromptValue(input);\n        const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n        const chatGeneration = result.generations[0][0];\n        // TODO: Remove cast after figuring out inheritance\n        return chatGeneration.message;\n    }\n    // eslint-disable-next-line require-yield\n    async *_streamResponseChunks(_messages, _options, _runManager) {\n        throw new Error(\"Not implemented.\");\n    }\n    async *_streamIterator(input, options) {\n        // Subclass check required to avoid double callbacks with default implementation\n        if (this._streamResponseChunks ===\n            BaseChatModel.prototype._streamResponseChunks) {\n            yield this.invoke(input, options);\n        }\n        else {\n            const prompt = BaseChatModel._convertInputToPromptValue(input);\n            const messages = prompt.toChatMessages();\n            const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(options);\n            const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, { verbose: this.verbose });\n            const extra = {\n                options: callOptions,\n                invocation_params: this?.invocationParams(callOptions),\n                batch_size: 1,\n            };\n            const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), [messages], undefined, undefined, extra, undefined, undefined, runnableConfig.runName);\n            let generationChunk;\n            try {\n                for await (const chunk of this._streamResponseChunks(messages, callOptions, runManagers?.[0])) {\n                    yield chunk.message;\n                    if (!generationChunk) {\n                        generationChunk = chunk;\n                    }\n                    else {\n                        generationChunk = generationChunk.concat(chunk);\n                    }\n                }\n            }\n            catch (err) {\n                await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n                throw err;\n            }\n            await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMEnd({\n                // TODO: Remove cast after figuring out inheritance\n                generations: [[generationChunk]],\n            })));\n        }\n    }\n    /** @ignore */\n    async _generateUncached(messages, parsedOptions, handledOptions) {\n        const baseMessages = messages.map((messageList) => messageList.map(coerceMessageLikeToMessage));\n        // create callback manager and start run\n        const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: parsedOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n            batch_size: 1,\n        };\n        const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), baseMessages, undefined, undefined, extra, undefined, undefined, handledOptions.runName);\n        // generate results\n        const results = await Promise.allSettled(baseMessages.map((messageList, i) => this._generate(messageList, { ...parsedOptions, promptIndex: i }, runManagers?.[i])));\n        // handle results\n        const generations = [];\n        const llmOutputs = [];\n        await Promise.all(results.map(async (pResult, i) => {\n            if (pResult.status === \"fulfilled\") {\n                const result = pResult.value;\n                generations[i] = result.generations;\n                llmOutputs[i] = result.llmOutput;\n                return runManagers?.[i]?.handleLLMEnd({\n                    generations: [result.generations],\n                    llmOutput: result.llmOutput,\n                });\n            }\n            else {\n                // status === \"rejected\"\n                await runManagers?.[i]?.handleLLMError(pResult.reason);\n                return Promise.reject(pResult.reason);\n            }\n        }));\n        // create combined output\n        const output = {\n            generations,\n            llmOutput: llmOutputs.length\n                ? this._combineLLMOutput?.(...llmOutputs)\n                : undefined,\n        };\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManagers\n                ? { runIds: runManagers?.map((manager) => manager.runId) }\n                : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    async _generateCached({ messages, cache, llmStringKey, parsedOptions, handledOptions, }) {\n        const baseMessages = messages.map((messageList) => messageList.map(coerceMessageLikeToMessage));\n        // create callback manager and start run\n        const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: parsedOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n            batch_size: 1,\n            cached: true,\n        };\n        const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), baseMessages, undefined, undefined, extra, undefined, undefined, handledOptions.runName);\n        // generate results\n        const missingPromptIndices = [];\n        const results = await Promise.allSettled(baseMessages.map(async (baseMessage, index) => {\n            // Join all content into one string for the prompt index\n            const prompt = BaseChatModel._convertInputToPromptValue(baseMessage).toString();\n            const result = await cache.lookup(prompt, llmStringKey);\n            if (result == null) {\n                missingPromptIndices.push(index);\n            }\n            return result;\n        }));\n        // Map run managers to the results before filtering out null results\n        // Null results are just absent from the cache.\n        const cachedResults = results\n            .map((result, index) => ({ result, runManager: runManagers?.[index] }))\n            .filter(({ result }) => (result.status === \"fulfilled\" && result.value != null) ||\n            result.status === \"rejected\");\n        // Handle results and call run managers\n        const generations = [];\n        await Promise.all(cachedResults.map(async ({ result: promiseResult, runManager }, i) => {\n            if (promiseResult.status === \"fulfilled\") {\n                const result = promiseResult.value;\n                generations[i] = result;\n                if (result.length) {\n                    await runManager?.handleLLMNewToken(result[0].text);\n                }\n                return runManager?.handleLLMEnd({\n                    generations: [result],\n                });\n            }\n            else {\n                // status === \"rejected\"\n                await runManager?.handleLLMError(promiseResult.reason);\n                return Promise.reject(promiseResult.reason);\n            }\n        }));\n        const output = {\n            generations,\n            missingPromptIndices,\n        };\n        // This defines RUN_KEY as a non-enumerable property on the output object\n        // so that it is not serialized when the output is stringified, and so that\n        // it isnt included when listing the keys of the output object.\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManagers\n                ? { runIds: runManagers?.map((manager) => manager.runId) }\n                : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    /**\n     * Generates chat based on the input messages.\n     * @param messages An array of arrays of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to an LLMResult.\n     */\n    async generate(messages, options, callbacks) {\n        // parse call options\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else {\n            parsedOptions = options;\n        }\n        const baseMessages = messages.map((messageList) => messageList.map(coerceMessageLikeToMessage));\n        const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(parsedOptions);\n        runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n        if (!this.cache) {\n            return this._generateUncached(baseMessages, callOptions, runnableConfig);\n        }\n        const { cache } = this;\n        const llmStringKey = this._getSerializedCacheKeyParametersForCall(callOptions);\n        const { generations, missingPromptIndices } = await this._generateCached({\n            messages: baseMessages,\n            cache,\n            llmStringKey,\n            parsedOptions: callOptions,\n            handledOptions: runnableConfig,\n        });\n        let llmOutput = {};\n        if (missingPromptIndices.length > 0) {\n            const results = await this._generateUncached(missingPromptIndices.map((i) => baseMessages[i]), callOptions, runnableConfig);\n            await Promise.all(results.generations.map(async (generation, index) => {\n                const promptIndex = missingPromptIndices[index];\n                generations[promptIndex] = generation;\n                // Join all content into one string for the prompt index\n                const prompt = BaseChatModel._convertInputToPromptValue(baseMessages[promptIndex]).toString();\n                return cache.update(prompt, llmStringKey, generation);\n            }));\n            llmOutput = results.llmOutput ?? {};\n        }\n        return { generations, llmOutput };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    invocationParams(_options) {\n        return {};\n    }\n    _modelType() {\n        return \"base_chat_model\";\n    }\n    /**\n     * @deprecated\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this.invocationParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    /**\n     * Generates a prompt based on the input prompt values.\n     * @param promptValues An array of BasePromptValue instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to an LLMResult.\n     */\n    async generatePrompt(promptValues, options, callbacks) {\n        const promptMessages = promptValues.map((promptValue) => promptValue.toChatMessages());\n        return this.generate(promptMessages, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Makes a single call to the chat model.\n     * @param messages An array of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async call(messages, options, callbacks) {\n        const result = await this.generate([messages.map(coerceMessageLikeToMessage)], options, callbacks);\n        const generations = result.generations;\n        return generations[0][0].message;\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Makes a single call to the chat model with a prompt value.\n     * @param promptValue The value of the prompt.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async callPrompt(promptValue, options, callbacks) {\n        const promptMessages = promptValue.toChatMessages();\n        return this.call(promptMessages, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Predicts the next message based on the input messages.\n     * @param messages An array of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async predictMessages(messages, options, callbacks) {\n        return this.call(messages, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Predicts the next message based on a text input.\n     * @param text The text input.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a string.\n     */\n    async predict(text, options, callbacks) {\n        const message = new HumanMessage(text);\n        const result = await this.call([message], options, callbacks);\n        if (typeof result.content !== \"string\") {\n            throw new Error(\"Cannot use predict when output is not a string.\");\n        }\n        return result.content;\n    }\n}\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\nexport class SimpleChatModel extends BaseChatModel {\n    async _generate(messages, options, runManager) {\n        const text = await this._call(messages, options, runManager);\n        const message = new AIMessage(text);\n        if (typeof message.content !== \"string\") {\n            throw new Error(\"Cannot generate with a simple chat model when output is not a string.\");\n        }\n        return {\n            generations: [\n                {\n                    text: message.content,\n                    message,\n                },\n            ],\n        };\n    }\n}\n"],"mappings":"AAAA,SAASA,SAAS,EAAEC,YAAY,EAAEC,0BAA0B,QAAS,sBAAsB;AAC3F,SAASC,OAAO,QAAS,eAAe;AACxC,SAASC,iBAAiB,QAAS,WAAW;AAC9C,SAASC,eAAe,QAAS,yBAAyB;AAC1D;AACA;AACA;AACA;AACA;AACA,OAAO,SAASC,mCAAmCA,CAAA,EAAG;EAClD,MAAMC,WAAW,GAAG,IAAIC,WAAW,CAAC,CAAC;EACrC,OAAO,IAAIC,eAAe,CAAC;IACvBC,SAASA,CAACC,KAAK,EAAEC,UAAU,EAAE;MACzBA,UAAU,CAACC,OAAO,CAACN,WAAW,CAACO,MAAM,CAAC,OAAOH,KAAK,CAACI,OAAO,KAAK,QAAQ,GACjEJ,KAAK,CAACI,OAAO,GACbC,IAAI,CAACC,SAAS,CAACN,KAAK,CAACI,OAAO,CAAC,CAAC,CAAC;IACzC;EACJ,CAAC,CAAC;AACN;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMG,aAAa,SAASd,iBAAiB,CAAC;EACjDe,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb;IACAC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,CAAC,WAAW,EAAE,aAAa,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC;IACvD,CAAC,CAAC;EACN;EACAC,sCAAsCA,CAACC,OAAO,EAAE;IAC5C,MAAM,CAACC,cAAc,EAAEC,WAAW,CAAC,GAAG,KAAK,CAACH,sCAAsC,CAACC,OAAO,CAAC;IAC3F,IAAIE,WAAW,EAAEC,OAAO,IAAI,CAACD,WAAW,CAACE,MAAM,EAAE;MAC7CF,WAAW,CAACE,MAAM,GAAGC,WAAW,CAACF,OAAO,CAACD,WAAW,CAACC,OAAO,CAAC;IACjE;IACA,OAAO,CAACF,cAAc,EAAEC,WAAW,CAAC;EACxC;EACA;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMI,MAAMA,CAACC,KAAK,EAAEP,OAAO,EAAE;IACzB,MAAMQ,WAAW,GAAGnB,aAAa,CAACoB,0BAA0B,CAACF,KAAK,CAAC;IACnE,MAAMG,MAAM,GAAG,MAAM,IAAI,CAACC,cAAc,CAAC,CAACH,WAAW,CAAC,EAAER,OAAO,EAAEA,OAAO,EAAEY,SAAS,CAAC;IACpF,MAAMC,cAAc,GAAGH,MAAM,CAACI,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAC/C;IACA,OAAOD,cAAc,CAACE,OAAO;EACjC;EACA;EACA,OAAOC,qBAAqBA,CAACC,SAAS,EAAEC,QAAQ,EAAEC,WAAW,EAAE;IAC3D,MAAM,IAAIC,KAAK,CAAC,kBAAkB,CAAC;EACvC;EACA,OAAOC,eAAeA,CAACd,KAAK,EAAEP,OAAO,EAAE;IACnC;IACA,IAAI,IAAI,CAACgB,qBAAqB,KAC1B3B,aAAa,CAACiC,SAAS,CAACN,qBAAqB,EAAE;MAC/C,MAAM,IAAI,CAACV,MAAM,CAACC,KAAK,EAAEP,OAAO,CAAC;IACrC,CAAC,MACI;MACD,MAAMuB,MAAM,GAAGlC,aAAa,CAACoB,0BAA0B,CAACF,KAAK,CAAC;MAC9D,MAAMiB,QAAQ,GAAGD,MAAM,CAACE,cAAc,CAAC,CAAC;MACxC,MAAM,CAACxB,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACH,sCAAsC,CAACC,OAAO,CAAC;MAC1F,MAAM0B,gBAAgB,GAAG,MAAMlD,eAAe,CAACmD,SAAS,CAAC1B,cAAc,CAACW,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEX,cAAc,CAAC2B,IAAI,EAAE,IAAI,CAACA,IAAI,EAAE3B,cAAc,CAAC4B,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;QAAEC,OAAO,EAAE,IAAI,CAACA;MAAQ,CAAC,CAAC;MACrM,MAAMC,KAAK,GAAG;QACV/B,OAAO,EAAEE,WAAW;QACpB8B,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAAC/B,WAAW,CAAC;QACtDgC,UAAU,EAAE;MAChB,CAAC;MACD,MAAMC,WAAW,GAAG,MAAMT,gBAAgB,EAAEU,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAE,CAACb,QAAQ,CAAC,EAAEc,SAAS,EAAEA,SAAS,EAAEP,KAAK,EAAEO,SAAS,EAAEA,SAAS,EAAErC,cAAc,CAACsC,OAAO,CAAC;MACtK,IAAIC,eAAe;MACnB,IAAI;QACA,WAAW,MAAM1D,KAAK,IAAI,IAAI,CAACkC,qBAAqB,CAACQ,QAAQ,EAAEtB,WAAW,EAAEiC,WAAW,GAAG,CAAC,CAAC,CAAC,EAAE;UAC3F,MAAMrD,KAAK,CAACiC,OAAO;UACnB,IAAI,CAACyB,eAAe,EAAE;YAClBA,eAAe,GAAG1D,KAAK;UAC3B,CAAC,MACI;YACD0D,eAAe,GAAGA,eAAe,CAACC,MAAM,CAAC3D,KAAK,CAAC;UACnD;QACJ;MACJ,CAAC,CACD,OAAO4D,GAAG,EAAE;QACR,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACT,WAAW,IAAI,EAAE,EAAEU,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEC,cAAc,CAACL,GAAG,CAAC,CAAC,CAAC;QAC3F,MAAMA,GAAG;MACb;MACA,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACT,WAAW,IAAI,EAAE,EAAEU,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEE,YAAY,CAAC;QAC/E;QACAlC,WAAW,EAAE,CAAC,CAAC0B,eAAe,CAAC;MACnC,CAAC,CAAC,CAAC,CAAC;IACR;EACJ;EACA;EACA,MAAMS,iBAAiBA,CAACzB,QAAQ,EAAE0B,aAAa,EAAEC,cAAc,EAAE;IAC7D,MAAMC,YAAY,GAAG5B,QAAQ,CAACqB,GAAG,CAAEQ,WAAW,IAAKA,WAAW,CAACR,GAAG,CAACxE,0BAA0B,CAAC,CAAC;IAC/F;IACA,MAAMqD,gBAAgB,GAAG,MAAMlD,eAAe,CAACmD,SAAS,CAACwB,cAAc,CAACvC,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEuC,cAAc,CAACvB,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEuB,cAAc,CAACtB,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;MAAEC,OAAO,EAAE,IAAI,CAACA;IAAQ,CAAC,CAAC;IACrM,MAAMC,KAAK,GAAG;MACV/B,OAAO,EAAEkD,aAAa;MACtBlB,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAACiB,aAAa,CAAC;MACxDhB,UAAU,EAAE;IAChB,CAAC;IACD,MAAMC,WAAW,GAAG,MAAMT,gBAAgB,EAAEU,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEe,YAAY,EAAEd,SAAS,EAAEA,SAAS,EAAEP,KAAK,EAAEO,SAAS,EAAEA,SAAS,EAAEa,cAAc,CAACZ,OAAO,CAAC;IACxK;IACA,MAAMe,OAAO,GAAG,MAAMX,OAAO,CAACY,UAAU,CAACH,YAAY,CAACP,GAAG,CAAC,CAACQ,WAAW,EAAEG,CAAC,KAAK,IAAI,CAACC,SAAS,CAACJ,WAAW,EAAE;MAAE,GAAGH,aAAa;MAAEQ,WAAW,EAAEF;IAAE,CAAC,EAAErB,WAAW,GAAGqB,CAAC,CAAC,CAAC,CAAC,CAAC;IACnK;IACA,MAAM1C,WAAW,GAAG,EAAE;IACtB,MAAM6C,UAAU,GAAG,EAAE;IACrB,MAAMhB,OAAO,CAACC,GAAG,CAACU,OAAO,CAACT,GAAG,CAAC,OAAOe,OAAO,EAAEJ,CAAC,KAAK;MAChD,IAAII,OAAO,CAACC,MAAM,KAAK,WAAW,EAAE;QAChC,MAAMnD,MAAM,GAAGkD,OAAO,CAAC/D,KAAK;QAC5BiB,WAAW,CAAC0C,CAAC,CAAC,GAAG9C,MAAM,CAACI,WAAW;QACnC6C,UAAU,CAACH,CAAC,CAAC,GAAG9C,MAAM,CAACoD,SAAS;QAChC,OAAO3B,WAAW,GAAGqB,CAAC,CAAC,EAAER,YAAY,CAAC;UAClClC,WAAW,EAAE,CAACJ,MAAM,CAACI,WAAW,CAAC;UACjCgD,SAAS,EAAEpD,MAAM,CAACoD;QACtB,CAAC,CAAC;MACN,CAAC,MACI;QACD;QACA,MAAM3B,WAAW,GAAGqB,CAAC,CAAC,EAAET,cAAc,CAACa,OAAO,CAACG,MAAM,CAAC;QACtD,OAAOpB,OAAO,CAACqB,MAAM,CAACJ,OAAO,CAACG,MAAM,CAAC;MACzC;IACJ,CAAC,CAAC,CAAC;IACH;IACA,MAAME,MAAM,GAAG;MACXnD,WAAW;MACXgD,SAAS,EAAEH,UAAU,CAACO,MAAM,GACtB,IAAI,CAACC,iBAAiB,GAAG,GAAGR,UAAU,CAAC,GACvCrB;IACV,CAAC;IACD9C,MAAM,CAACC,cAAc,CAACwE,MAAM,EAAE3F,OAAO,EAAE;MACnCuB,KAAK,EAAEsC,WAAW,GACZ;QAAEiC,MAAM,EAAEjC,WAAW,EAAEU,GAAG,CAAEwB,OAAO,IAAKA,OAAO,CAACC,KAAK;MAAE,CAAC,GACxDhC,SAAS;MACf3C,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOsE,MAAM;EACjB;EACA,MAAMM,eAAeA,CAAC;IAAE/C,QAAQ;IAAEgD,KAAK;IAAEC,YAAY;IAAEvB,aAAa;IAAEC;EAAgB,CAAC,EAAE;IACrF,MAAMC,YAAY,GAAG5B,QAAQ,CAACqB,GAAG,CAAEQ,WAAW,IAAKA,WAAW,CAACR,GAAG,CAACxE,0BAA0B,CAAC,CAAC;IAC/F;IACA,MAAMqD,gBAAgB,GAAG,MAAMlD,eAAe,CAACmD,SAAS,CAACwB,cAAc,CAACvC,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEuC,cAAc,CAACvB,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEuB,cAAc,CAACtB,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;MAAEC,OAAO,EAAE,IAAI,CAACA;IAAQ,CAAC,CAAC;IACrM,MAAMC,KAAK,GAAG;MACV/B,OAAO,EAAEkD,aAAa;MACtBlB,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAACiB,aAAa,CAAC;MACxDhB,UAAU,EAAE,CAAC;MACbwC,MAAM,EAAE;IACZ,CAAC;IACD,MAAMvC,WAAW,GAAG,MAAMT,gBAAgB,EAAEU,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEe,YAAY,EAAEd,SAAS,EAAEA,SAAS,EAAEP,KAAK,EAAEO,SAAS,EAAEA,SAAS,EAAEa,cAAc,CAACZ,OAAO,CAAC;IACxK;IACA,MAAMoC,oBAAoB,GAAG,EAAE;IAC/B,MAAMrB,OAAO,GAAG,MAAMX,OAAO,CAACY,UAAU,CAACH,YAAY,CAACP,GAAG,CAAC,OAAO+B,WAAW,EAAEC,KAAK,KAAK;MACpF;MACA,MAAMtD,MAAM,GAAGlC,aAAa,CAACoB,0BAA0B,CAACmE,WAAW,CAAC,CAACE,QAAQ,CAAC,CAAC;MAC/E,MAAMpE,MAAM,GAAG,MAAM8D,KAAK,CAACO,MAAM,CAACxD,MAAM,EAAEkD,YAAY,CAAC;MACvD,IAAI/D,MAAM,IAAI,IAAI,EAAE;QAChBiE,oBAAoB,CAACK,IAAI,CAACH,KAAK,CAAC;MACpC;MACA,OAAOnE,MAAM;IACjB,CAAC,CAAC,CAAC;IACH;IACA;IACA,MAAMuE,aAAa,GAAG3B,OAAO,CACxBT,GAAG,CAAC,CAACnC,MAAM,EAAEmE,KAAK,MAAM;MAAEnE,MAAM;MAAEoC,UAAU,EAAEX,WAAW,GAAG0C,KAAK;IAAE,CAAC,CAAC,CAAC,CACtEK,MAAM,CAAC,CAAC;MAAExE;IAAO,CAAC,KAAMA,MAAM,CAACmD,MAAM,KAAK,WAAW,IAAInD,MAAM,CAACb,KAAK,IAAI,IAAI,IAC9Ea,MAAM,CAACmD,MAAM,KAAK,UAAU,CAAC;IACjC;IACA,MAAM/C,WAAW,GAAG,EAAE;IACtB,MAAM6B,OAAO,CAACC,GAAG,CAACqC,aAAa,CAACpC,GAAG,CAAC,OAAO;MAAEnC,MAAM,EAAEyE,aAAa;MAAErC;IAAW,CAAC,EAAEU,CAAC,KAAK;MACpF,IAAI2B,aAAa,CAACtB,MAAM,KAAK,WAAW,EAAE;QACtC,MAAMnD,MAAM,GAAGyE,aAAa,CAACtF,KAAK;QAClCiB,WAAW,CAAC0C,CAAC,CAAC,GAAG9C,MAAM;QACvB,IAAIA,MAAM,CAACwD,MAAM,EAAE;UACf,MAAMpB,UAAU,EAAEsC,iBAAiB,CAAC1E,MAAM,CAAC,CAAC,CAAC,CAAC2E,IAAI,CAAC;QACvD;QACA,OAAOvC,UAAU,EAAEE,YAAY,CAAC;UAC5BlC,WAAW,EAAE,CAACJ,MAAM;QACxB,CAAC,CAAC;MACN,CAAC,MACI;QACD;QACA,MAAMoC,UAAU,EAAEC,cAAc,CAACoC,aAAa,CAACpB,MAAM,CAAC;QACtD,OAAOpB,OAAO,CAACqB,MAAM,CAACmB,aAAa,CAACpB,MAAM,CAAC;MAC/C;IACJ,CAAC,CAAC,CAAC;IACH,MAAME,MAAM,GAAG;MACXnD,WAAW;MACX6D;IACJ,CAAC;IACD;IACA;IACA;IACAnF,MAAM,CAACC,cAAc,CAACwE,MAAM,EAAE3F,OAAO,EAAE;MACnCuB,KAAK,EAAEsC,WAAW,GACZ;QAAEiC,MAAM,EAAEjC,WAAW,EAAEU,GAAG,CAAEwB,OAAO,IAAKA,OAAO,CAACC,KAAK;MAAE,CAAC,GACxDhC,SAAS;MACf3C,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOsE,MAAM;EACjB;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAMqB,QAAQA,CAAC9D,QAAQ,EAAExB,OAAO,EAAEY,SAAS,EAAE;IACzC;IACA,IAAIsC,aAAa;IACjB,IAAIqC,KAAK,CAACC,OAAO,CAACxF,OAAO,CAAC,EAAE;MACxBkD,aAAa,GAAG;QAAEuC,IAAI,EAAEzF;MAAQ,CAAC;IACrC,CAAC,MACI;MACDkD,aAAa,GAAGlD,OAAO;IAC3B;IACA,MAAMoD,YAAY,GAAG5B,QAAQ,CAACqB,GAAG,CAAEQ,WAAW,IAAKA,WAAW,CAACR,GAAG,CAACxE,0BAA0B,CAAC,CAAC;IAC/F,MAAM,CAAC4B,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACH,sCAAsC,CAACmD,aAAa,CAAC;IAChGjD,cAAc,CAACW,SAAS,GAAGX,cAAc,CAACW,SAAS,IAAIA,SAAS;IAChE,IAAI,CAAC,IAAI,CAAC4D,KAAK,EAAE;MACb,OAAO,IAAI,CAACvB,iBAAiB,CAACG,YAAY,EAAElD,WAAW,EAAED,cAAc,CAAC;IAC5E;IACA,MAAM;MAAEuE;IAAM,CAAC,GAAG,IAAI;IACtB,MAAMC,YAAY,GAAG,IAAI,CAACiB,uCAAuC,CAACxF,WAAW,CAAC;IAC9E,MAAM;MAAEY,WAAW;MAAE6D;IAAqB,CAAC,GAAG,MAAM,IAAI,CAACJ,eAAe,CAAC;MACrE/C,QAAQ,EAAE4B,YAAY;MACtBoB,KAAK;MACLC,YAAY;MACZvB,aAAa,EAAEhD,WAAW;MAC1BiD,cAAc,EAAElD;IACpB,CAAC,CAAC;IACF,IAAI6D,SAAS,GAAG,CAAC,CAAC;IAClB,IAAIa,oBAAoB,CAACT,MAAM,GAAG,CAAC,EAAE;MACjC,MAAMZ,OAAO,GAAG,MAAM,IAAI,CAACL,iBAAiB,CAAC0B,oBAAoB,CAAC9B,GAAG,CAAEW,CAAC,IAAKJ,YAAY,CAACI,CAAC,CAAC,CAAC,EAAEtD,WAAW,EAAED,cAAc,CAAC;MAC3H,MAAM0C,OAAO,CAACC,GAAG,CAACU,OAAO,CAACxC,WAAW,CAAC+B,GAAG,CAAC,OAAO8C,UAAU,EAAEd,KAAK,KAAK;QACnE,MAAMnB,WAAW,GAAGiB,oBAAoB,CAACE,KAAK,CAAC;QAC/C/D,WAAW,CAAC4C,WAAW,CAAC,GAAGiC,UAAU;QACrC;QACA,MAAMpE,MAAM,GAAGlC,aAAa,CAACoB,0BAA0B,CAAC2C,YAAY,CAACM,WAAW,CAAC,CAAC,CAACoB,QAAQ,CAAC,CAAC;QAC7F,OAAON,KAAK,CAACoB,MAAM,CAACrE,MAAM,EAAEkD,YAAY,EAAEkB,UAAU,CAAC;MACzD,CAAC,CAAC,CAAC;MACH7B,SAAS,GAAGR,OAAO,CAACQ,SAAS,IAAI,CAAC,CAAC;IACvC;IACA,OAAO;MAAEhD,WAAW;MAAEgD;IAAU,CAAC;EACrC;EACA;AACJ;AACA;EACI;EACA7B,gBAAgBA,CAACf,QAAQ,EAAE;IACvB,OAAO,CAAC,CAAC;EACb;EACA2E,UAAUA,CAAA,EAAG;IACT,OAAO,iBAAiB;EAC5B;EACA;AACJ;AACA;AACA;EACIC,SAASA,CAAA,EAAG;IACR,OAAO;MACH,GAAG,IAAI,CAAC7D,gBAAgB,CAAC,CAAC;MAC1B8D,KAAK,EAAE,IAAI,CAACjG,QAAQ,CAAC,CAAC;MACtBkG,MAAM,EAAE,IAAI,CAACH,UAAU,CAAC;IAC5B,CAAC;EACL;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAMlF,cAAcA,CAACsF,YAAY,EAAEjG,OAAO,EAAEY,SAAS,EAAE;IACnD,MAAMsF,cAAc,GAAGD,YAAY,CAACpD,GAAG,CAAErC,WAAW,IAAKA,WAAW,CAACiB,cAAc,CAAC,CAAC,CAAC;IACtF,OAAO,IAAI,CAAC6D,QAAQ,CAACY,cAAc,EAAElG,OAAO,EAAEY,SAAS,CAAC;EAC5D;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMuF,IAAIA,CAAC3E,QAAQ,EAAExB,OAAO,EAAEY,SAAS,EAAE;IACrC,MAAMF,MAAM,GAAG,MAAM,IAAI,CAAC4E,QAAQ,CAAC,CAAC9D,QAAQ,CAACqB,GAAG,CAACxE,0BAA0B,CAAC,CAAC,EAAE2B,OAAO,EAAEY,SAAS,CAAC;IAClG,MAAME,WAAW,GAAGJ,MAAM,CAACI,WAAW;IACtC,OAAOA,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,OAAO;EACpC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMqF,UAAUA,CAAC5F,WAAW,EAAER,OAAO,EAAEY,SAAS,EAAE;IAC9C,MAAMsF,cAAc,GAAG1F,WAAW,CAACiB,cAAc,CAAC,CAAC;IACnD,OAAO,IAAI,CAAC0E,IAAI,CAACD,cAAc,EAAElG,OAAO,EAAEY,SAAS,CAAC;EACxD;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMyF,eAAeA,CAAC7E,QAAQ,EAAExB,OAAO,EAAEY,SAAS,EAAE;IAChD,OAAO,IAAI,CAACuF,IAAI,CAAC3E,QAAQ,EAAExB,OAAO,EAAEY,SAAS,CAAC;EAClD;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAM0F,OAAOA,CAACjB,IAAI,EAAErF,OAAO,EAAEY,SAAS,EAAE;IACpC,MAAMG,OAAO,GAAG,IAAI3C,YAAY,CAACiH,IAAI,CAAC;IACtC,MAAM3E,MAAM,GAAG,MAAM,IAAI,CAACyF,IAAI,CAAC,CAACpF,OAAO,CAAC,EAAEf,OAAO,EAAEY,SAAS,CAAC;IAC7D,IAAI,OAAOF,MAAM,CAACxB,OAAO,KAAK,QAAQ,EAAE;MACpC,MAAM,IAAIkC,KAAK,CAAC,iDAAiD,CAAC;IACtE;IACA,OAAOV,MAAM,CAACxB,OAAO;EACzB;AACJ;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMqH,eAAe,SAASlH,aAAa,CAAC;EAC/C,MAAMoE,SAASA,CAACjC,QAAQ,EAAExB,OAAO,EAAE8C,UAAU,EAAE;IAC3C,MAAMuC,IAAI,GAAG,MAAM,IAAI,CAACmB,KAAK,CAAChF,QAAQ,EAAExB,OAAO,EAAE8C,UAAU,CAAC;IAC5D,MAAM/B,OAAO,GAAG,IAAI5C,SAAS,CAACkH,IAAI,CAAC;IACnC,IAAI,OAAOtE,OAAO,CAAC7B,OAAO,KAAK,QAAQ,EAAE;MACrC,MAAM,IAAIkC,KAAK,CAAC,uEAAuE,CAAC;IAC5F;IACA,OAAO;MACHN,WAAW,EAAE,CACT;QACIuE,IAAI,EAAEtE,OAAO,CAAC7B,OAAO;QACrB6B;MACJ,CAAC;IAET,CAAC;EACL;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}