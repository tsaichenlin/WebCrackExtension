{"ast":null,"code":"import { AIMessage, HumanMessage, coerceMessageLikeToMessage } from \"../messages/index.js\";\nimport { RUN_KEY } from \"../outputs.js\";\nimport { BaseLanguageModel } from \"./base.js\";\nimport { CallbackManager } from \"../callbacks/manager.js\";\n/**\n * Creates a transform stream for encoding chat message chunks.\n * @deprecated Use {@link BytesOutputParser} instead\n * @returns A TransformStream instance that encodes chat message chunks.\n */\nexport function createChatMessageChunkEncoderStream() {\n  const textEncoder = new TextEncoder();\n  return new TransformStream({\n    transform(chunk, controller) {\n      controller.enqueue(textEncoder.encode(typeof chunk.content === \"string\" ? chunk.content : JSON.stringify(chunk.content)));\n    }\n  });\n}\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\nexport class BaseChatModel extends BaseLanguageModel {\n  constructor(fields) {\n    super(fields);\n    // Only ever instantiated in main LangChain\n    Object.defineProperty(this, \"lc_namespace\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: [\"langchain\", \"chat_models\", this._llmType()]\n    });\n  }\n  _separateRunnableConfigFromCallOptions(options) {\n    const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n    if (callOptions !== null && callOptions !== void 0 && callOptions.timeout && !callOptions.signal) {\n      callOptions.signal = AbortSignal.timeout(callOptions.timeout);\n    }\n    return [runnableConfig, callOptions];\n  }\n  /**\n   * Invokes the chat model with a single input.\n   * @param input The input for the language model.\n   * @param options The call options.\n   * @returns A Promise that resolves to a BaseMessageChunk.\n   */\n  async invoke(input, options) {\n    const promptValue = BaseChatModel._convertInputToPromptValue(input);\n    const result = await this.generatePrompt([promptValue], options, options === null || options === void 0 ? void 0 : options.callbacks);\n    const chatGeneration = result.generations[0][0];\n    // TODO: Remove cast after figuring out inheritance\n    return chatGeneration.message;\n  }\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(_messages, _options, _runManager) {\n    throw new Error(\"Not implemented.\");\n  }\n  async *_streamIterator(input, options) {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (this._streamResponseChunks === BaseChatModel.prototype._streamResponseChunks) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseChatModel._convertInputToPromptValue(input);\n      const messages = prompt.toChatMessages();\n      const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(options);\n      const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, {\n        verbose: this.verbose\n      });\n      const extra = {\n        options: callOptions,\n        invocation_params: this === null || this === void 0 ? void 0 : this.invocationParams(callOptions),\n        batch_size: 1\n      };\n      const runManagers = await (callbackManager_ === null || callbackManager_ === void 0 ? void 0 : callbackManager_.handleChatModelStart(this.toJSON(), [messages], undefined, undefined, extra, undefined, undefined, runnableConfig.runName));\n      let generationChunk;\n      try {\n        for await (const chunk of this._streamResponseChunks(messages, callOptions, runManagers === null || runManagers === void 0 ? void 0 : runManagers[0])) {\n          yield chunk.message;\n          if (!generationChunk) {\n            generationChunk = chunk;\n          } else {\n            generationChunk = generationChunk.concat(chunk);\n          }\n        }\n      } catch (err) {\n        await Promise.all((runManagers !== null && runManagers !== void 0 ? runManagers : []).map(runManager => runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMError(err)));\n        throw err;\n      }\n      await Promise.all((runManagers !== null && runManagers !== void 0 ? runManagers : []).map(runManager => runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMEnd({\n        // TODO: Remove cast after figuring out inheritance\n        generations: [[generationChunk]]\n      })));\n    }\n  }\n  /** @ignore */\n  async _generateUncached(messages, parsedOptions, handledOptions) {\n    var _this$_combineLLMOutp;\n    const baseMessages = messages.map(messageList => messageList.map(coerceMessageLikeToMessage));\n    // create callback manager and start run\n    const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, {\n      verbose: this.verbose\n    });\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this === null || this === void 0 ? void 0 : this.invocationParams(parsedOptions),\n      batch_size: 1\n    };\n    const runManagers = await (callbackManager_ === null || callbackManager_ === void 0 ? void 0 : callbackManager_.handleChatModelStart(this.toJSON(), baseMessages, undefined, undefined, extra, undefined, undefined, handledOptions.runName));\n    // generate results\n    const results = await Promise.allSettled(baseMessages.map((messageList, i) => this._generate(messageList, {\n      ...parsedOptions,\n      promptIndex: i\n    }, runManagers === null || runManagers === void 0 ? void 0 : runManagers[i])));\n    // handle results\n    const generations = [];\n    const llmOutputs = [];\n    await Promise.all(results.map(async (pResult, i) => {\n      if (pResult.status === \"fulfilled\") {\n        var _runManagers$i;\n        const result = pResult.value;\n        generations[i] = result.generations;\n        llmOutputs[i] = result.llmOutput;\n        return runManagers === null || runManagers === void 0 || (_runManagers$i = runManagers[i]) === null || _runManagers$i === void 0 ? void 0 : _runManagers$i.handleLLMEnd({\n          generations: [result.generations],\n          llmOutput: result.llmOutput\n        });\n      } else {\n        var _runManagers$i2;\n        // status === \"rejected\"\n        await (runManagers === null || runManagers === void 0 || (_runManagers$i2 = runManagers[i]) === null || _runManagers$i2 === void 0 ? void 0 : _runManagers$i2.handleLLMError(pResult.reason));\n        return Promise.reject(pResult.reason);\n      }\n    }));\n    // create combined output\n    const output = {\n      generations,\n      llmOutput: llmOutputs.length ? (_this$_combineLLMOutp = this._combineLLMOutput) === null || _this$_combineLLMOutp === void 0 ? void 0 : _this$_combineLLMOutp.call(this, ...llmOutputs) : undefined\n    };\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers ? {\n        runIds: runManagers === null || runManagers === void 0 ? void 0 : runManagers.map(manager => manager.runId)\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  async _generateCached(_ref) {\n    let {\n      messages,\n      cache,\n      llmStringKey,\n      parsedOptions,\n      handledOptions\n    } = _ref;\n    const baseMessages = messages.map(messageList => messageList.map(coerceMessageLikeToMessage));\n    // create callback manager and start run\n    const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, {\n      verbose: this.verbose\n    });\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this === null || this === void 0 ? void 0 : this.invocationParams(parsedOptions),\n      batch_size: 1,\n      cached: true\n    };\n    const runManagers = await (callbackManager_ === null || callbackManager_ === void 0 ? void 0 : callbackManager_.handleChatModelStart(this.toJSON(), baseMessages, undefined, undefined, extra, undefined, undefined, handledOptions.runName));\n    // generate results\n    const missingPromptIndices = [];\n    const results = await Promise.allSettled(baseMessages.map(async (baseMessage, index) => {\n      // Join all content into one string for the prompt index\n      const prompt = BaseChatModel._convertInputToPromptValue(baseMessage).toString();\n      const result = await cache.lookup(prompt, llmStringKey);\n      if (result == null) {\n        missingPromptIndices.push(index);\n      }\n      return result;\n    }));\n    // Map run managers to the results before filtering out null results\n    // Null results are just absent from the cache.\n    const cachedResults = results.map((result, index) => ({\n      result,\n      runManager: runManagers === null || runManagers === void 0 ? void 0 : runManagers[index]\n    })).filter(_ref2 => {\n      let {\n        result\n      } = _ref2;\n      return result.status === \"fulfilled\" && result.value != null || result.status === \"rejected\";\n    });\n    // Handle results and call run managers\n    const generations = [];\n    await Promise.all(cachedResults.map(async (_ref3, i) => {\n      let {\n        result: promiseResult,\n        runManager\n      } = _ref3;\n      if (promiseResult.status === \"fulfilled\") {\n        const result = promiseResult.value;\n        generations[i] = result;\n        if (result.length) {\n          await (runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMNewToken(result[0].text));\n        }\n        return runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMEnd({\n          generations: [result]\n        });\n      } else {\n        // status === \"rejected\"\n        await (runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMError(promiseResult.reason));\n        return Promise.reject(promiseResult.reason);\n      }\n    }));\n    const output = {\n      generations,\n      missingPromptIndices\n    };\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers ? {\n        runIds: runManagers === null || runManagers === void 0 ? void 0 : runManagers.map(manager => manager.runId)\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  /**\n   * Generates chat based on the input messages.\n   * @param messages An array of arrays of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generate(messages, options, callbacks) {\n    var _runnableConfig$callb;\n    // parse call options\n    let parsedOptions;\n    if (Array.isArray(options)) {\n      parsedOptions = {\n        stop: options\n      };\n    } else {\n      parsedOptions = options;\n    }\n    const baseMessages = messages.map(messageList => messageList.map(coerceMessageLikeToMessage));\n    const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(parsedOptions);\n    runnableConfig.callbacks = (_runnableConfig$callb = runnableConfig.callbacks) !== null && _runnableConfig$callb !== void 0 ? _runnableConfig$callb : callbacks;\n    if (!this.cache) {\n      return this._generateUncached(baseMessages, callOptions, runnableConfig);\n    }\n    const {\n      cache\n    } = this;\n    const llmStringKey = this._getSerializedCacheKeyParametersForCall(callOptions);\n    const {\n      generations,\n      missingPromptIndices\n    } = await this._generateCached({\n      messages: baseMessages,\n      cache,\n      llmStringKey,\n      parsedOptions: callOptions,\n      handledOptions: runnableConfig\n    });\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      var _results$llmOutput;\n      const results = await this._generateUncached(missingPromptIndices.map(i => baseMessages[i]), callOptions, runnableConfig);\n      await Promise.all(results.generations.map(async (generation, index) => {\n        const promptIndex = missingPromptIndices[index];\n        generations[promptIndex] = generation;\n        // Join all content into one string for the prompt index\n        const prompt = BaseChatModel._convertInputToPromptValue(baseMessages[promptIndex]).toString();\n        return cache.update(prompt, llmStringKey, generation);\n      }));\n      llmOutput = (_results$llmOutput = results.llmOutput) !== null && _results$llmOutput !== void 0 ? _results$llmOutput : {};\n    }\n    return {\n      generations,\n      llmOutput\n    };\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options) {\n    return {};\n  }\n  _modelType() {\n    return \"base_chat_model\";\n  }\n  /**\n   * @deprecated\n   * Return a json-like object representing this LLM.\n   */\n  serialize() {\n    return {\n      ...this.invocationParams(),\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n  }\n  /**\n   * Generates a prompt based on the input prompt values.\n   * @param promptValues An array of BasePromptValue instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generatePrompt(promptValues, options, callbacks) {\n    const promptMessages = promptValues.map(promptValue => promptValue.toChatMessages());\n    return this.generate(promptMessages, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Makes a single call to the chat model.\n   * @param messages An array of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async call(messages, options, callbacks) {\n    const result = await this.generate([messages.map(coerceMessageLikeToMessage)], options, callbacks);\n    const generations = result.generations;\n    return generations[0][0].message;\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Makes a single call to the chat model with a prompt value.\n   * @param promptValue The value of the prompt.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async callPrompt(promptValue, options, callbacks) {\n    const promptMessages = promptValue.toChatMessages();\n    return this.call(promptMessages, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Predicts the next message based on the input messages.\n   * @param messages An array of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async predictMessages(messages, options, callbacks) {\n    return this.call(messages, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Predicts the next message based on a text input.\n   * @param text The text input.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a string.\n   */\n  async predict(text, options, callbacks) {\n    const message = new HumanMessage(text);\n    const result = await this.call([message], options, callbacks);\n    if (typeof result.content !== \"string\") {\n      throw new Error(\"Cannot use predict when output is not a string.\");\n    }\n    return result.content;\n  }\n}\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\nexport class SimpleChatModel extends BaseChatModel {\n  async _generate(messages, options, runManager) {\n    const text = await this._call(messages, options, runManager);\n    const message = new AIMessage(text);\n    if (typeof message.content !== \"string\") {\n      throw new Error(\"Cannot generate with a simple chat model when output is not a string.\");\n    }\n    return {\n      generations: [{\n        text: message.content,\n        message\n      }]\n    };\n  }\n}","map":{"version":3,"names":["AIMessage","HumanMessage","coerceMessageLikeToMessage","RUN_KEY","BaseLanguageModel","CallbackManager","createChatMessageChunkEncoderStream","textEncoder","TextEncoder","TransformStream","transform","chunk","controller","enqueue","encode","content","JSON","stringify","BaseChatModel","constructor","fields","Object","defineProperty","enumerable","configurable","writable","value","_llmType","_separateRunnableConfigFromCallOptions","options","runnableConfig","callOptions","timeout","signal","AbortSignal","invoke","input","promptValue","_convertInputToPromptValue","result","generatePrompt","callbacks","chatGeneration","generations","message","_streamResponseChunks","_messages","_options","_runManager","Error","_streamIterator","prototype","prompt","messages","toChatMessages","callbackManager_","configure","tags","metadata","verbose","extra","invocation_params","invocationParams","batch_size","runManagers","handleChatModelStart","toJSON","undefined","runName","generationChunk","concat","err","Promise","all","map","runManager","handleLLMError","handleLLMEnd","_generateUncached","parsedOptions","handledOptions","_this$_combineLLMOutp","baseMessages","messageList","results","allSettled","i","_generate","promptIndex","llmOutputs","pResult","status","_runManagers$i","llmOutput","_runManagers$i2","reason","reject","output","length","_combineLLMOutput","call","runIds","manager","runId","_generateCached","_ref","cache","llmStringKey","cached","missingPromptIndices","baseMessage","index","toString","lookup","push","cachedResults","filter","_ref2","_ref3","promiseResult","handleLLMNewToken","text","generate","_runnableConfig$callb","Array","isArray","stop","_getSerializedCacheKeyParametersForCall","_results$llmOutput","generation","update","_modelType","serialize","_type","_model","promptValues","promptMessages","callPrompt","predictMessages","predict","SimpleChatModel","_call"],"sources":["/Users/mandylin/Desktop/WebCrack React 2/webcrack/node_modules/@langchain/core/dist/language_models/chat_models.js"],"sourcesContent":["import { AIMessage, HumanMessage, coerceMessageLikeToMessage, } from \"../messages/index.js\";\nimport { RUN_KEY, } from \"../outputs.js\";\nimport { BaseLanguageModel, } from \"./base.js\";\nimport { CallbackManager, } from \"../callbacks/manager.js\";\n/**\n * Creates a transform stream for encoding chat message chunks.\n * @deprecated Use {@link BytesOutputParser} instead\n * @returns A TransformStream instance that encodes chat message chunks.\n */\nexport function createChatMessageChunkEncoderStream() {\n    const textEncoder = new TextEncoder();\n    return new TransformStream({\n        transform(chunk, controller) {\n            controller.enqueue(textEncoder.encode(typeof chunk.content === \"string\"\n                ? chunk.content\n                : JSON.stringify(chunk.content)));\n        },\n    });\n}\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\nexport class BaseChatModel extends BaseLanguageModel {\n    constructor(fields) {\n        super(fields);\n        // Only ever instantiated in main LangChain\n        Object.defineProperty(this, \"lc_namespace\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: [\"langchain\", \"chat_models\", this._llmType()]\n        });\n    }\n    _separateRunnableConfigFromCallOptions(options) {\n        const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n        if (callOptions?.timeout && !callOptions.signal) {\n            callOptions.signal = AbortSignal.timeout(callOptions.timeout);\n        }\n        return [runnableConfig, callOptions];\n    }\n    /**\n     * Invokes the chat model with a single input.\n     * @param input The input for the language model.\n     * @param options The call options.\n     * @returns A Promise that resolves to a BaseMessageChunk.\n     */\n    async invoke(input, options) {\n        const promptValue = BaseChatModel._convertInputToPromptValue(input);\n        const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n        const chatGeneration = result.generations[0][0];\n        // TODO: Remove cast after figuring out inheritance\n        return chatGeneration.message;\n    }\n    // eslint-disable-next-line require-yield\n    async *_streamResponseChunks(_messages, _options, _runManager) {\n        throw new Error(\"Not implemented.\");\n    }\n    async *_streamIterator(input, options) {\n        // Subclass check required to avoid double callbacks with default implementation\n        if (this._streamResponseChunks ===\n            BaseChatModel.prototype._streamResponseChunks) {\n            yield this.invoke(input, options);\n        }\n        else {\n            const prompt = BaseChatModel._convertInputToPromptValue(input);\n            const messages = prompt.toChatMessages();\n            const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(options);\n            const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, { verbose: this.verbose });\n            const extra = {\n                options: callOptions,\n                invocation_params: this?.invocationParams(callOptions),\n                batch_size: 1,\n            };\n            const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), [messages], undefined, undefined, extra, undefined, undefined, runnableConfig.runName);\n            let generationChunk;\n            try {\n                for await (const chunk of this._streamResponseChunks(messages, callOptions, runManagers?.[0])) {\n                    yield chunk.message;\n                    if (!generationChunk) {\n                        generationChunk = chunk;\n                    }\n                    else {\n                        generationChunk = generationChunk.concat(chunk);\n                    }\n                }\n            }\n            catch (err) {\n                await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n                throw err;\n            }\n            await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMEnd({\n                // TODO: Remove cast after figuring out inheritance\n                generations: [[generationChunk]],\n            })));\n        }\n    }\n    /** @ignore */\n    async _generateUncached(messages, parsedOptions, handledOptions) {\n        const baseMessages = messages.map((messageList) => messageList.map(coerceMessageLikeToMessage));\n        // create callback manager and start run\n        const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: parsedOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n            batch_size: 1,\n        };\n        const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), baseMessages, undefined, undefined, extra, undefined, undefined, handledOptions.runName);\n        // generate results\n        const results = await Promise.allSettled(baseMessages.map((messageList, i) => this._generate(messageList, { ...parsedOptions, promptIndex: i }, runManagers?.[i])));\n        // handle results\n        const generations = [];\n        const llmOutputs = [];\n        await Promise.all(results.map(async (pResult, i) => {\n            if (pResult.status === \"fulfilled\") {\n                const result = pResult.value;\n                generations[i] = result.generations;\n                llmOutputs[i] = result.llmOutput;\n                return runManagers?.[i]?.handleLLMEnd({\n                    generations: [result.generations],\n                    llmOutput: result.llmOutput,\n                });\n            }\n            else {\n                // status === \"rejected\"\n                await runManagers?.[i]?.handleLLMError(pResult.reason);\n                return Promise.reject(pResult.reason);\n            }\n        }));\n        // create combined output\n        const output = {\n            generations,\n            llmOutput: llmOutputs.length\n                ? this._combineLLMOutput?.(...llmOutputs)\n                : undefined,\n        };\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManagers\n                ? { runIds: runManagers?.map((manager) => manager.runId) }\n                : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    async _generateCached({ messages, cache, llmStringKey, parsedOptions, handledOptions, }) {\n        const baseMessages = messages.map((messageList) => messageList.map(coerceMessageLikeToMessage));\n        // create callback manager and start run\n        const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: parsedOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n            batch_size: 1,\n            cached: true,\n        };\n        const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), baseMessages, undefined, undefined, extra, undefined, undefined, handledOptions.runName);\n        // generate results\n        const missingPromptIndices = [];\n        const results = await Promise.allSettled(baseMessages.map(async (baseMessage, index) => {\n            // Join all content into one string for the prompt index\n            const prompt = BaseChatModel._convertInputToPromptValue(baseMessage).toString();\n            const result = await cache.lookup(prompt, llmStringKey);\n            if (result == null) {\n                missingPromptIndices.push(index);\n            }\n            return result;\n        }));\n        // Map run managers to the results before filtering out null results\n        // Null results are just absent from the cache.\n        const cachedResults = results\n            .map((result, index) => ({ result, runManager: runManagers?.[index] }))\n            .filter(({ result }) => (result.status === \"fulfilled\" && result.value != null) ||\n            result.status === \"rejected\");\n        // Handle results and call run managers\n        const generations = [];\n        await Promise.all(cachedResults.map(async ({ result: promiseResult, runManager }, i) => {\n            if (promiseResult.status === \"fulfilled\") {\n                const result = promiseResult.value;\n                generations[i] = result;\n                if (result.length) {\n                    await runManager?.handleLLMNewToken(result[0].text);\n                }\n                return runManager?.handleLLMEnd({\n                    generations: [result],\n                });\n            }\n            else {\n                // status === \"rejected\"\n                await runManager?.handleLLMError(promiseResult.reason);\n                return Promise.reject(promiseResult.reason);\n            }\n        }));\n        const output = {\n            generations,\n            missingPromptIndices,\n        };\n        // This defines RUN_KEY as a non-enumerable property on the output object\n        // so that it is not serialized when the output is stringified, and so that\n        // it isnt included when listing the keys of the output object.\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManagers\n                ? { runIds: runManagers?.map((manager) => manager.runId) }\n                : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    /**\n     * Generates chat based on the input messages.\n     * @param messages An array of arrays of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to an LLMResult.\n     */\n    async generate(messages, options, callbacks) {\n        // parse call options\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else {\n            parsedOptions = options;\n        }\n        const baseMessages = messages.map((messageList) => messageList.map(coerceMessageLikeToMessage));\n        const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(parsedOptions);\n        runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n        if (!this.cache) {\n            return this._generateUncached(baseMessages, callOptions, runnableConfig);\n        }\n        const { cache } = this;\n        const llmStringKey = this._getSerializedCacheKeyParametersForCall(callOptions);\n        const { generations, missingPromptIndices } = await this._generateCached({\n            messages: baseMessages,\n            cache,\n            llmStringKey,\n            parsedOptions: callOptions,\n            handledOptions: runnableConfig,\n        });\n        let llmOutput = {};\n        if (missingPromptIndices.length > 0) {\n            const results = await this._generateUncached(missingPromptIndices.map((i) => baseMessages[i]), callOptions, runnableConfig);\n            await Promise.all(results.generations.map(async (generation, index) => {\n                const promptIndex = missingPromptIndices[index];\n                generations[promptIndex] = generation;\n                // Join all content into one string for the prompt index\n                const prompt = BaseChatModel._convertInputToPromptValue(baseMessages[promptIndex]).toString();\n                return cache.update(prompt, llmStringKey, generation);\n            }));\n            llmOutput = results.llmOutput ?? {};\n        }\n        return { generations, llmOutput };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    invocationParams(_options) {\n        return {};\n    }\n    _modelType() {\n        return \"base_chat_model\";\n    }\n    /**\n     * @deprecated\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this.invocationParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    /**\n     * Generates a prompt based on the input prompt values.\n     * @param promptValues An array of BasePromptValue instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to an LLMResult.\n     */\n    async generatePrompt(promptValues, options, callbacks) {\n        const promptMessages = promptValues.map((promptValue) => promptValue.toChatMessages());\n        return this.generate(promptMessages, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Makes a single call to the chat model.\n     * @param messages An array of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async call(messages, options, callbacks) {\n        const result = await this.generate([messages.map(coerceMessageLikeToMessage)], options, callbacks);\n        const generations = result.generations;\n        return generations[0][0].message;\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Makes a single call to the chat model with a prompt value.\n     * @param promptValue The value of the prompt.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async callPrompt(promptValue, options, callbacks) {\n        const promptMessages = promptValue.toChatMessages();\n        return this.call(promptMessages, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Predicts the next message based on the input messages.\n     * @param messages An array of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async predictMessages(messages, options, callbacks) {\n        return this.call(messages, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Predicts the next message based on a text input.\n     * @param text The text input.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a string.\n     */\n    async predict(text, options, callbacks) {\n        const message = new HumanMessage(text);\n        const result = await this.call([message], options, callbacks);\n        if (typeof result.content !== \"string\") {\n            throw new Error(\"Cannot use predict when output is not a string.\");\n        }\n        return result.content;\n    }\n}\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\nexport class SimpleChatModel extends BaseChatModel {\n    async _generate(messages, options, runManager) {\n        const text = await this._call(messages, options, runManager);\n        const message = new AIMessage(text);\n        if (typeof message.content !== \"string\") {\n            throw new Error(\"Cannot generate with a simple chat model when output is not a string.\");\n        }\n        return {\n            generations: [\n                {\n                    text: message.content,\n                    message,\n                },\n            ],\n        };\n    }\n}\n"],"mappings":"AAAA,SAASA,SAAS,EAAEC,YAAY,EAAEC,0BAA0B,QAAS,sBAAsB;AAC3F,SAASC,OAAO,QAAS,eAAe;AACxC,SAASC,iBAAiB,QAAS,WAAW;AAC9C,SAASC,eAAe,QAAS,yBAAyB;AAC1D;AACA;AACA;AACA;AACA;AACA,OAAO,SAASC,mCAAmCA,CAAA,EAAG;EAClD,MAAMC,WAAW,GAAG,IAAIC,WAAW,CAAC,CAAC;EACrC,OAAO,IAAIC,eAAe,CAAC;IACvBC,SAASA,CAACC,KAAK,EAAEC,UAAU,EAAE;MACzBA,UAAU,CAACC,OAAO,CAACN,WAAW,CAACO,MAAM,CAAC,OAAOH,KAAK,CAACI,OAAO,KAAK,QAAQ,GACjEJ,KAAK,CAACI,OAAO,GACbC,IAAI,CAACC,SAAS,CAACN,KAAK,CAACI,OAAO,CAAC,CAAC,CAAC;IACzC;EACJ,CAAC,CAAC;AACN;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMG,aAAa,SAASd,iBAAiB,CAAC;EACjDe,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb;IACAC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,CAAC,WAAW,EAAE,aAAa,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC;IACvD,CAAC,CAAC;EACN;EACAC,sCAAsCA,CAACC,OAAO,EAAE;IAC5C,MAAM,CAACC,cAAc,EAAEC,WAAW,CAAC,GAAG,KAAK,CAACH,sCAAsC,CAACC,OAAO,CAAC;IAC3F,IAAIE,WAAW,aAAXA,WAAW,eAAXA,WAAW,CAAEC,OAAO,IAAI,CAACD,WAAW,CAACE,MAAM,EAAE;MAC7CF,WAAW,CAACE,MAAM,GAAGC,WAAW,CAACF,OAAO,CAACD,WAAW,CAACC,OAAO,CAAC;IACjE;IACA,OAAO,CAACF,cAAc,EAAEC,WAAW,CAAC;EACxC;EACA;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMI,MAAMA,CAACC,KAAK,EAAEP,OAAO,EAAE;IACzB,MAAMQ,WAAW,GAAGnB,aAAa,CAACoB,0BAA0B,CAACF,KAAK,CAAC;IACnE,MAAMG,MAAM,GAAG,MAAM,IAAI,CAACC,cAAc,CAAC,CAACH,WAAW,CAAC,EAAER,OAAO,EAAEA,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEY,SAAS,CAAC;IACpF,MAAMC,cAAc,GAAGH,MAAM,CAACI,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAC/C;IACA,OAAOD,cAAc,CAACE,OAAO;EACjC;EACA;EACA,OAAOC,qBAAqBA,CAACC,SAAS,EAAEC,QAAQ,EAAEC,WAAW,EAAE;IAC3D,MAAM,IAAIC,KAAK,CAAC,kBAAkB,CAAC;EACvC;EACA,OAAOC,eAAeA,CAACd,KAAK,EAAEP,OAAO,EAAE;IACnC;IACA,IAAI,IAAI,CAACgB,qBAAqB,KAC1B3B,aAAa,CAACiC,SAAS,CAACN,qBAAqB,EAAE;MAC/C,MAAM,IAAI,CAACV,MAAM,CAACC,KAAK,EAAEP,OAAO,CAAC;IACrC,CAAC,MACI;MACD,MAAMuB,MAAM,GAAGlC,aAAa,CAACoB,0BAA0B,CAACF,KAAK,CAAC;MAC9D,MAAMiB,QAAQ,GAAGD,MAAM,CAACE,cAAc,CAAC,CAAC;MACxC,MAAM,CAACxB,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACH,sCAAsC,CAACC,OAAO,CAAC;MAC1F,MAAM0B,gBAAgB,GAAG,MAAMlD,eAAe,CAACmD,SAAS,CAAC1B,cAAc,CAACW,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEX,cAAc,CAAC2B,IAAI,EAAE,IAAI,CAACA,IAAI,EAAE3B,cAAc,CAAC4B,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;QAAEC,OAAO,EAAE,IAAI,CAACA;MAAQ,CAAC,CAAC;MACrM,MAAMC,KAAK,GAAG;QACV/B,OAAO,EAAEE,WAAW;QACpB8B,iBAAiB,EAAE,IAAI,aAAJ,IAAI,uBAAJ,IAAI,CAAEC,gBAAgB,CAAC/B,WAAW,CAAC;QACtDgC,UAAU,EAAE;MAChB,CAAC;MACD,MAAMC,WAAW,GAAG,OAAMT,gBAAgB,aAAhBA,gBAAgB,uBAAhBA,gBAAgB,CAAEU,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAE,CAACb,QAAQ,CAAC,EAAEc,SAAS,EAAEA,SAAS,EAAEP,KAAK,EAAEO,SAAS,EAAEA,SAAS,EAAErC,cAAc,CAACsC,OAAO,CAAC;MACtK,IAAIC,eAAe;MACnB,IAAI;QACA,WAAW,MAAM1D,KAAK,IAAI,IAAI,CAACkC,qBAAqB,CAACQ,QAAQ,EAAEtB,WAAW,EAAEiC,WAAW,aAAXA,WAAW,uBAAXA,WAAW,CAAG,CAAC,CAAC,CAAC,EAAE;UAC3F,MAAMrD,KAAK,CAACiC,OAAO;UACnB,IAAI,CAACyB,eAAe,EAAE;YAClBA,eAAe,GAAG1D,KAAK;UAC3B,CAAC,MACI;YACD0D,eAAe,GAAGA,eAAe,CAACC,MAAM,CAAC3D,KAAK,CAAC;UACnD;QACJ;MACJ,CAAC,CACD,OAAO4D,GAAG,EAAE;QACR,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACT,WAAW,aAAXA,WAAW,cAAXA,WAAW,GAAI,EAAE,EAAEU,GAAG,CAAEC,UAAU,IAAKA,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEC,cAAc,CAACL,GAAG,CAAC,CAAC,CAAC;QAC3F,MAAMA,GAAG;MACb;MACA,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACT,WAAW,aAAXA,WAAW,cAAXA,WAAW,GAAI,EAAE,EAAEU,GAAG,CAAEC,UAAU,IAAKA,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEE,YAAY,CAAC;QAC/E;QACAlC,WAAW,EAAE,CAAC,CAAC0B,eAAe,CAAC;MACnC,CAAC,CAAC,CAAC,CAAC;IACR;EACJ;EACA;EACA,MAAMS,iBAAiBA,CAACzB,QAAQ,EAAE0B,aAAa,EAAEC,cAAc,EAAE;IAAA,IAAAC,qBAAA;IAC7D,MAAMC,YAAY,GAAG7B,QAAQ,CAACqB,GAAG,CAAES,WAAW,IAAKA,WAAW,CAACT,GAAG,CAACxE,0BAA0B,CAAC,CAAC;IAC/F;IACA,MAAMqD,gBAAgB,GAAG,MAAMlD,eAAe,CAACmD,SAAS,CAACwB,cAAc,CAACvC,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEuC,cAAc,CAACvB,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEuB,cAAc,CAACtB,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;MAAEC,OAAO,EAAE,IAAI,CAACA;IAAQ,CAAC,CAAC;IACrM,MAAMC,KAAK,GAAG;MACV/B,OAAO,EAAEkD,aAAa;MACtBlB,iBAAiB,EAAE,IAAI,aAAJ,IAAI,uBAAJ,IAAI,CAAEC,gBAAgB,CAACiB,aAAa,CAAC;MACxDhB,UAAU,EAAE;IAChB,CAAC;IACD,MAAMC,WAAW,GAAG,OAAMT,gBAAgB,aAAhBA,gBAAgB,uBAAhBA,gBAAgB,CAAEU,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEgB,YAAY,EAAEf,SAAS,EAAEA,SAAS,EAAEP,KAAK,EAAEO,SAAS,EAAEA,SAAS,EAAEa,cAAc,CAACZ,OAAO,CAAC;IACxK;IACA,MAAMgB,OAAO,GAAG,MAAMZ,OAAO,CAACa,UAAU,CAACH,YAAY,CAACR,GAAG,CAAC,CAACS,WAAW,EAAEG,CAAC,KAAK,IAAI,CAACC,SAAS,CAACJ,WAAW,EAAE;MAAE,GAAGJ,aAAa;MAAES,WAAW,EAAEF;IAAE,CAAC,EAAEtB,WAAW,aAAXA,WAAW,uBAAXA,WAAW,CAAGsB,CAAC,CAAC,CAAC,CAAC,CAAC;IACnK;IACA,MAAM3C,WAAW,GAAG,EAAE;IACtB,MAAM8C,UAAU,GAAG,EAAE;IACrB,MAAMjB,OAAO,CAACC,GAAG,CAACW,OAAO,CAACV,GAAG,CAAC,OAAOgB,OAAO,EAAEJ,CAAC,KAAK;MAChD,IAAII,OAAO,CAACC,MAAM,KAAK,WAAW,EAAE;QAAA,IAAAC,cAAA;QAChC,MAAMrD,MAAM,GAAGmD,OAAO,CAAChE,KAAK;QAC5BiB,WAAW,CAAC2C,CAAC,CAAC,GAAG/C,MAAM,CAACI,WAAW;QACnC8C,UAAU,CAACH,CAAC,CAAC,GAAG/C,MAAM,CAACsD,SAAS;QAChC,OAAO7B,WAAW,aAAXA,WAAW,gBAAA4B,cAAA,GAAX5B,WAAW,CAAGsB,CAAC,CAAC,cAAAM,cAAA,uBAAhBA,cAAA,CAAkBf,YAAY,CAAC;UAClClC,WAAW,EAAE,CAACJ,MAAM,CAACI,WAAW,CAAC;UACjCkD,SAAS,EAAEtD,MAAM,CAACsD;QACtB,CAAC,CAAC;MACN,CAAC,MACI;QAAA,IAAAC,eAAA;QACD;QACA,OAAM9B,WAAW,aAAXA,WAAW,gBAAA8B,eAAA,GAAX9B,WAAW,CAAGsB,CAAC,CAAC,cAAAQ,eAAA,uBAAhBA,eAAA,CAAkBlB,cAAc,CAACc,OAAO,CAACK,MAAM,CAAC;QACtD,OAAOvB,OAAO,CAACwB,MAAM,CAACN,OAAO,CAACK,MAAM,CAAC;MACzC;IACJ,CAAC,CAAC,CAAC;IACH;IACA,MAAME,MAAM,GAAG;MACXtD,WAAW;MACXkD,SAAS,EAAEJ,UAAU,CAACS,MAAM,IAAAjB,qBAAA,GACtB,IAAI,CAACkB,iBAAiB,cAAAlB,qBAAA,uBAAtBA,qBAAA,CAAAmB,IAAA,KAAI,EAAqB,GAAGX,UAAU,CAAC,GACvCtB;IACV,CAAC;IACD9C,MAAM,CAACC,cAAc,CAAC2E,MAAM,EAAE9F,OAAO,EAAE;MACnCuB,KAAK,EAAEsC,WAAW,GACZ;QAAEqC,MAAM,EAAErC,WAAW,aAAXA,WAAW,uBAAXA,WAAW,CAAEU,GAAG,CAAE4B,OAAO,IAAKA,OAAO,CAACC,KAAK;MAAE,CAAC,GACxDpC,SAAS;MACf3C,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOyE,MAAM;EACjB;EACA,MAAMO,eAAeA,CAAAC,IAAA,EAAoE;IAAA,IAAnE;MAAEpD,QAAQ;MAAEqD,KAAK;MAAEC,YAAY;MAAE5B,aAAa;MAAEC;IAAgB,CAAC,GAAAyB,IAAA;IACnF,MAAMvB,YAAY,GAAG7B,QAAQ,CAACqB,GAAG,CAAES,WAAW,IAAKA,WAAW,CAACT,GAAG,CAACxE,0BAA0B,CAAC,CAAC;IAC/F;IACA,MAAMqD,gBAAgB,GAAG,MAAMlD,eAAe,CAACmD,SAAS,CAACwB,cAAc,CAACvC,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEuC,cAAc,CAACvB,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEuB,cAAc,CAACtB,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;MAAEC,OAAO,EAAE,IAAI,CAACA;IAAQ,CAAC,CAAC;IACrM,MAAMC,KAAK,GAAG;MACV/B,OAAO,EAAEkD,aAAa;MACtBlB,iBAAiB,EAAE,IAAI,aAAJ,IAAI,uBAAJ,IAAI,CAAEC,gBAAgB,CAACiB,aAAa,CAAC;MACxDhB,UAAU,EAAE,CAAC;MACb6C,MAAM,EAAE;IACZ,CAAC;IACD,MAAM5C,WAAW,GAAG,OAAMT,gBAAgB,aAAhBA,gBAAgB,uBAAhBA,gBAAgB,CAAEU,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEgB,YAAY,EAAEf,SAAS,EAAEA,SAAS,EAAEP,KAAK,EAAEO,SAAS,EAAEA,SAAS,EAAEa,cAAc,CAACZ,OAAO,CAAC;IACxK;IACA,MAAMyC,oBAAoB,GAAG,EAAE;IAC/B,MAAMzB,OAAO,GAAG,MAAMZ,OAAO,CAACa,UAAU,CAACH,YAAY,CAACR,GAAG,CAAC,OAAOoC,WAAW,EAAEC,KAAK,KAAK;MACpF;MACA,MAAM3D,MAAM,GAAGlC,aAAa,CAACoB,0BAA0B,CAACwE,WAAW,CAAC,CAACE,QAAQ,CAAC,CAAC;MAC/E,MAAMzE,MAAM,GAAG,MAAMmE,KAAK,CAACO,MAAM,CAAC7D,MAAM,EAAEuD,YAAY,CAAC;MACvD,IAAIpE,MAAM,IAAI,IAAI,EAAE;QAChBsE,oBAAoB,CAACK,IAAI,CAACH,KAAK,CAAC;MACpC;MACA,OAAOxE,MAAM;IACjB,CAAC,CAAC,CAAC;IACH;IACA;IACA,MAAM4E,aAAa,GAAG/B,OAAO,CACxBV,GAAG,CAAC,CAACnC,MAAM,EAAEwE,KAAK,MAAM;MAAExE,MAAM;MAAEoC,UAAU,EAAEX,WAAW,aAAXA,WAAW,uBAAXA,WAAW,CAAG+C,KAAK;IAAE,CAAC,CAAC,CAAC,CACtEK,MAAM,CAACC,KAAA;MAAA,IAAC;QAAE9E;MAAO,CAAC,GAAA8E,KAAA;MAAA,OAAM9E,MAAM,CAACoD,MAAM,KAAK,WAAW,IAAIpD,MAAM,CAACb,KAAK,IAAI,IAAI,IAC9Ea,MAAM,CAACoD,MAAM,KAAK,UAAU;IAAA,EAAC;IACjC;IACA,MAAMhD,WAAW,GAAG,EAAE;IACtB,MAAM6B,OAAO,CAACC,GAAG,CAAC0C,aAAa,CAACzC,GAAG,CAAC,OAAA4C,KAAA,EAA8ChC,CAAC,KAAK;MAAA,IAA7C;QAAE/C,MAAM,EAAEgF,aAAa;QAAE5C;MAAW,CAAC,GAAA2C,KAAA;MAC5E,IAAIC,aAAa,CAAC5B,MAAM,KAAK,WAAW,EAAE;QACtC,MAAMpD,MAAM,GAAGgF,aAAa,CAAC7F,KAAK;QAClCiB,WAAW,CAAC2C,CAAC,CAAC,GAAG/C,MAAM;QACvB,IAAIA,MAAM,CAAC2D,MAAM,EAAE;UACf,OAAMvB,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAE6C,iBAAiB,CAACjF,MAAM,CAAC,CAAC,CAAC,CAACkF,IAAI,CAAC;QACvD;QACA,OAAO9C,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEE,YAAY,CAAC;UAC5BlC,WAAW,EAAE,CAACJ,MAAM;QACxB,CAAC,CAAC;MACN,CAAC,MACI;QACD;QACA,OAAMoC,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEC,cAAc,CAAC2C,aAAa,CAACxB,MAAM,CAAC;QACtD,OAAOvB,OAAO,CAACwB,MAAM,CAACuB,aAAa,CAACxB,MAAM,CAAC;MAC/C;IACJ,CAAC,CAAC,CAAC;IACH,MAAME,MAAM,GAAG;MACXtD,WAAW;MACXkE;IACJ,CAAC;IACD;IACA;IACA;IACAxF,MAAM,CAACC,cAAc,CAAC2E,MAAM,EAAE9F,OAAO,EAAE;MACnCuB,KAAK,EAAEsC,WAAW,GACZ;QAAEqC,MAAM,EAAErC,WAAW,aAAXA,WAAW,uBAAXA,WAAW,CAAEU,GAAG,CAAE4B,OAAO,IAAKA,OAAO,CAACC,KAAK;MAAE,CAAC,GACxDpC,SAAS;MACf3C,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOyE,MAAM;EACjB;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAMyB,QAAQA,CAACrE,QAAQ,EAAExB,OAAO,EAAEY,SAAS,EAAE;IAAA,IAAAkF,qBAAA;IACzC;IACA,IAAI5C,aAAa;IACjB,IAAI6C,KAAK,CAACC,OAAO,CAAChG,OAAO,CAAC,EAAE;MACxBkD,aAAa,GAAG;QAAE+C,IAAI,EAAEjG;MAAQ,CAAC;IACrC,CAAC,MACI;MACDkD,aAAa,GAAGlD,OAAO;IAC3B;IACA,MAAMqD,YAAY,GAAG7B,QAAQ,CAACqB,GAAG,CAAES,WAAW,IAAKA,WAAW,CAACT,GAAG,CAACxE,0BAA0B,CAAC,CAAC;IAC/F,MAAM,CAAC4B,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACH,sCAAsC,CAACmD,aAAa,CAAC;IAChGjD,cAAc,CAACW,SAAS,IAAAkF,qBAAA,GAAG7F,cAAc,CAACW,SAAS,cAAAkF,qBAAA,cAAAA,qBAAA,GAAIlF,SAAS;IAChE,IAAI,CAAC,IAAI,CAACiE,KAAK,EAAE;MACb,OAAO,IAAI,CAAC5B,iBAAiB,CAACI,YAAY,EAAEnD,WAAW,EAAED,cAAc,CAAC;IAC5E;IACA,MAAM;MAAE4E;IAAM,CAAC,GAAG,IAAI;IACtB,MAAMC,YAAY,GAAG,IAAI,CAACoB,uCAAuC,CAAChG,WAAW,CAAC;IAC9E,MAAM;MAAEY,WAAW;MAAEkE;IAAqB,CAAC,GAAG,MAAM,IAAI,CAACL,eAAe,CAAC;MACrEnD,QAAQ,EAAE6B,YAAY;MACtBwB,KAAK;MACLC,YAAY;MACZ5B,aAAa,EAAEhD,WAAW;MAC1BiD,cAAc,EAAElD;IACpB,CAAC,CAAC;IACF,IAAI+D,SAAS,GAAG,CAAC,CAAC;IAClB,IAAIgB,oBAAoB,CAACX,MAAM,GAAG,CAAC,EAAE;MAAA,IAAA8B,kBAAA;MACjC,MAAM5C,OAAO,GAAG,MAAM,IAAI,CAACN,iBAAiB,CAAC+B,oBAAoB,CAACnC,GAAG,CAAEY,CAAC,IAAKJ,YAAY,CAACI,CAAC,CAAC,CAAC,EAAEvD,WAAW,EAAED,cAAc,CAAC;MAC3H,MAAM0C,OAAO,CAACC,GAAG,CAACW,OAAO,CAACzC,WAAW,CAAC+B,GAAG,CAAC,OAAOuD,UAAU,EAAElB,KAAK,KAAK;QACnE,MAAMvB,WAAW,GAAGqB,oBAAoB,CAACE,KAAK,CAAC;QAC/CpE,WAAW,CAAC6C,WAAW,CAAC,GAAGyC,UAAU;QACrC;QACA,MAAM7E,MAAM,GAAGlC,aAAa,CAACoB,0BAA0B,CAAC4C,YAAY,CAACM,WAAW,CAAC,CAAC,CAACwB,QAAQ,CAAC,CAAC;QAC7F,OAAON,KAAK,CAACwB,MAAM,CAAC9E,MAAM,EAAEuD,YAAY,EAAEsB,UAAU,CAAC;MACzD,CAAC,CAAC,CAAC;MACHpC,SAAS,IAAAmC,kBAAA,GAAG5C,OAAO,CAACS,SAAS,cAAAmC,kBAAA,cAAAA,kBAAA,GAAI,CAAC,CAAC;IACvC;IACA,OAAO;MAAErF,WAAW;MAAEkD;IAAU,CAAC;EACrC;EACA;AACJ;AACA;EACI;EACA/B,gBAAgBA,CAACf,QAAQ,EAAE;IACvB,OAAO,CAAC,CAAC;EACb;EACAoF,UAAUA,CAAA,EAAG;IACT,OAAO,iBAAiB;EAC5B;EACA;AACJ;AACA;AACA;EACIC,SAASA,CAAA,EAAG;IACR,OAAO;MACH,GAAG,IAAI,CAACtE,gBAAgB,CAAC,CAAC;MAC1BuE,KAAK,EAAE,IAAI,CAAC1G,QAAQ,CAAC,CAAC;MACtB2G,MAAM,EAAE,IAAI,CAACH,UAAU,CAAC;IAC5B,CAAC;EACL;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAM3F,cAAcA,CAAC+F,YAAY,EAAE1G,OAAO,EAAEY,SAAS,EAAE;IACnD,MAAM+F,cAAc,GAAGD,YAAY,CAAC7D,GAAG,CAAErC,WAAW,IAAKA,WAAW,CAACiB,cAAc,CAAC,CAAC,CAAC;IACtF,OAAO,IAAI,CAACoE,QAAQ,CAACc,cAAc,EAAE3G,OAAO,EAAEY,SAAS,CAAC;EAC5D;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAM2D,IAAIA,CAAC/C,QAAQ,EAAExB,OAAO,EAAEY,SAAS,EAAE;IACrC,MAAMF,MAAM,GAAG,MAAM,IAAI,CAACmF,QAAQ,CAAC,CAACrE,QAAQ,CAACqB,GAAG,CAACxE,0BAA0B,CAAC,CAAC,EAAE2B,OAAO,EAAEY,SAAS,CAAC;IAClG,MAAME,WAAW,GAAGJ,MAAM,CAACI,WAAW;IACtC,OAAOA,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,OAAO;EACpC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAM6F,UAAUA,CAACpG,WAAW,EAAER,OAAO,EAAEY,SAAS,EAAE;IAC9C,MAAM+F,cAAc,GAAGnG,WAAW,CAACiB,cAAc,CAAC,CAAC;IACnD,OAAO,IAAI,CAAC8C,IAAI,CAACoC,cAAc,EAAE3G,OAAO,EAAEY,SAAS,CAAC;EACxD;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMiG,eAAeA,CAACrF,QAAQ,EAAExB,OAAO,EAAEY,SAAS,EAAE;IAChD,OAAO,IAAI,CAAC2D,IAAI,CAAC/C,QAAQ,EAAExB,OAAO,EAAEY,SAAS,CAAC;EAClD;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMkG,OAAOA,CAAClB,IAAI,EAAE5F,OAAO,EAAEY,SAAS,EAAE;IACpC,MAAMG,OAAO,GAAG,IAAI3C,YAAY,CAACwH,IAAI,CAAC;IACtC,MAAMlF,MAAM,GAAG,MAAM,IAAI,CAAC6D,IAAI,CAAC,CAACxD,OAAO,CAAC,EAAEf,OAAO,EAAEY,SAAS,CAAC;IAC7D,IAAI,OAAOF,MAAM,CAACxB,OAAO,KAAK,QAAQ,EAAE;MACpC,MAAM,IAAIkC,KAAK,CAAC,iDAAiD,CAAC;IACtE;IACA,OAAOV,MAAM,CAACxB,OAAO;EACzB;AACJ;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM6H,eAAe,SAAS1H,aAAa,CAAC;EAC/C,MAAMqE,SAASA,CAAClC,QAAQ,EAAExB,OAAO,EAAE8C,UAAU,EAAE;IAC3C,MAAM8C,IAAI,GAAG,MAAM,IAAI,CAACoB,KAAK,CAACxF,QAAQ,EAAExB,OAAO,EAAE8C,UAAU,CAAC;IAC5D,MAAM/B,OAAO,GAAG,IAAI5C,SAAS,CAACyH,IAAI,CAAC;IACnC,IAAI,OAAO7E,OAAO,CAAC7B,OAAO,KAAK,QAAQ,EAAE;MACrC,MAAM,IAAIkC,KAAK,CAAC,uEAAuE,CAAC;IAC5F;IACA,OAAO;MACHN,WAAW,EAAE,CACT;QACI8E,IAAI,EAAE7E,OAAO,CAAC7B,OAAO;QACrB6B;MACJ,CAAC;IAET,CAAC;EACL;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}