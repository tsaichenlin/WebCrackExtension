{"ast":null,"code":"import { AIMessage, getBufferString } from \"../messages/index.js\";\nimport { RUN_KEY, GenerationChunk } from \"../outputs.js\";\nimport { CallbackManager } from \"../callbacks/manager.js\";\nimport { BaseLanguageModel } from \"./base.js\";\n/**\n * LLM Wrapper. Takes in a prompt (or prompts) and returns a string.\n */\nexport class BaseLLM extends BaseLanguageModel {\n  constructor(_ref) {\n    let {\n      concurrency,\n      ...rest\n    } = _ref;\n    super(concurrency ? {\n      maxConcurrency: concurrency,\n      ...rest\n    } : rest);\n    // Only ever instantiated in main LangChain\n    Object.defineProperty(this, \"lc_namespace\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: [\"langchain\", \"llms\", this._llmType()]\n    });\n  }\n  /**\n   * This method takes an input and options, and returns a string. It\n   * converts the input to a prompt value and generates a result based on\n   * the prompt.\n   * @param input Input for the LLM.\n   * @param options Options for the LLM call.\n   * @returns A string result based on the prompt.\n   */\n  async invoke(input, options) {\n    const promptValue = BaseLLM._convertInputToPromptValue(input);\n    const result = await this.generatePrompt([promptValue], options, options === null || options === void 0 ? void 0 : options.callbacks);\n    return result.generations[0][0].text;\n  }\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(_input, _options, _runManager) {\n    throw new Error(\"Not implemented.\");\n  }\n  _separateRunnableConfigFromCallOptions(options) {\n    const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n    if (callOptions !== null && callOptions !== void 0 && callOptions.timeout && !callOptions.signal) {\n      callOptions.signal = AbortSignal.timeout(callOptions.timeout);\n    }\n    return [runnableConfig, callOptions];\n  }\n  async *_streamIterator(input, options) {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (this._streamResponseChunks === BaseLLM.prototype._streamResponseChunks) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseLLM._convertInputToPromptValue(input);\n      const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(options);\n      const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, {\n        verbose: this.verbose\n      });\n      const extra = {\n        options: callOptions,\n        invocation_params: this === null || this === void 0 ? void 0 : this.invocationParams(callOptions),\n        batch_size: 1\n      };\n      const runManagers = await (callbackManager_ === null || callbackManager_ === void 0 ? void 0 : callbackManager_.handleLLMStart(this.toJSON(), [prompt.toString()], undefined, undefined, extra, undefined, undefined, runnableConfig.runName));\n      let generation = new GenerationChunk({\n        text: \"\"\n      });\n      try {\n        for await (const chunk of this._streamResponseChunks(input.toString(), callOptions, runManagers === null || runManagers === void 0 ? void 0 : runManagers[0])) {\n          if (!generation) {\n            generation = chunk;\n          } else {\n            generation = generation.concat(chunk);\n          }\n          if (typeof chunk.text === \"string\") {\n            yield chunk.text;\n          }\n        }\n      } catch (err) {\n        await Promise.all((runManagers !== null && runManagers !== void 0 ? runManagers : []).map(runManager => runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMError(err)));\n        throw err;\n      }\n      await Promise.all((runManagers !== null && runManagers !== void 0 ? runManagers : []).map(runManager => runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMEnd({\n        generations: [[generation]]\n      })));\n    }\n  }\n  /**\n   * This method takes prompt values, options, and callbacks, and generates\n   * a result based on the prompts.\n   * @param promptValues Prompt values for the LLM.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns An LLMResult based on the prompts.\n   */\n  async generatePrompt(promptValues, options, callbacks) {\n    const prompts = promptValues.map(promptValue => promptValue.toString());\n    return this.generate(prompts, options, callbacks);\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options) {\n    return {};\n  }\n  _flattenLLMResult(llmResult) {\n    const llmResults = [];\n    for (let i = 0; i < llmResult.generations.length; i += 1) {\n      const genList = llmResult.generations[i];\n      if (i === 0) {\n        llmResults.push({\n          generations: [genList],\n          llmOutput: llmResult.llmOutput\n        });\n      } else {\n        const llmOutput = llmResult.llmOutput ? {\n          ...llmResult.llmOutput,\n          tokenUsage: {}\n        } : undefined;\n        llmResults.push({\n          generations: [genList],\n          llmOutput\n        });\n      }\n    }\n    return llmResults;\n  }\n  /** @ignore */\n  async _generateUncached(prompts, parsedOptions, handledOptions) {\n    const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, {\n      verbose: this.verbose\n    });\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this === null || this === void 0 ? void 0 : this.invocationParams(parsedOptions),\n      batch_size: prompts.length\n    };\n    const runManagers = await (callbackManager_ === null || callbackManager_ === void 0 ? void 0 : callbackManager_.handleLLMStart(this.toJSON(), prompts, undefined, undefined, extra, undefined, undefined, handledOptions === null || handledOptions === void 0 ? void 0 : handledOptions.runName));\n    let output;\n    try {\n      output = await this._generate(prompts, parsedOptions, runManagers === null || runManagers === void 0 ? void 0 : runManagers[0]);\n    } catch (err) {\n      await Promise.all((runManagers !== null && runManagers !== void 0 ? runManagers : []).map(runManager => runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMError(err)));\n      throw err;\n    }\n    const flattenedOutputs = this._flattenLLMResult(output);\n    await Promise.all((runManagers !== null && runManagers !== void 0 ? runManagers : []).map((runManager, i) => runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMEnd(flattenedOutputs[i])));\n    const runIds = (runManagers === null || runManagers === void 0 ? void 0 : runManagers.map(manager => manager.runId)) || undefined;\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runIds ? {\n        runIds\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  async _generateCached(_ref2) {\n    let {\n      prompts,\n      cache,\n      llmStringKey,\n      parsedOptions,\n      handledOptions\n    } = _ref2;\n    const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, {\n      verbose: this.verbose\n    });\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this === null || this === void 0 ? void 0 : this.invocationParams(parsedOptions),\n      batch_size: prompts.length,\n      cached: true\n    };\n    const runManagers = await (callbackManager_ === null || callbackManager_ === void 0 ? void 0 : callbackManager_.handleLLMStart(this.toJSON(), prompts, undefined, undefined, extra, undefined, undefined, handledOptions === null || handledOptions === void 0 ? void 0 : handledOptions.runName));\n    // generate results\n    const missingPromptIndices = [];\n    const results = await Promise.allSettled(prompts.map(async (prompt, index) => {\n      const result = await cache.lookup(prompt, llmStringKey);\n      if (result == null) {\n        missingPromptIndices.push(index);\n      }\n      return result;\n    }));\n    // Map run managers to the results before filtering out null results\n    // Null results are just absent from the cache.\n    const cachedResults = results.map((result, index) => ({\n      result,\n      runManager: runManagers === null || runManagers === void 0 ? void 0 : runManagers[index]\n    })).filter(_ref3 => {\n      let {\n        result\n      } = _ref3;\n      return result.status === \"fulfilled\" && result.value != null || result.status === \"rejected\";\n    });\n    // Handle results and call run managers\n    const generations = [];\n    await Promise.all(cachedResults.map(async (_ref4, i) => {\n      let {\n        result: promiseResult,\n        runManager\n      } = _ref4;\n      if (promiseResult.status === \"fulfilled\") {\n        const result = promiseResult.value;\n        generations[i] = result;\n        if (result.length) {\n          await (runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMNewToken(result[0].text));\n        }\n        return runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMEnd({\n          generations: [result]\n        });\n      } else {\n        // status === \"rejected\"\n        await (runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMError(promiseResult.reason));\n        return Promise.reject(promiseResult.reason);\n      }\n    }));\n    const output = {\n      generations,\n      missingPromptIndices\n    };\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers ? {\n        runIds: runManagers === null || runManagers === void 0 ? void 0 : runManagers.map(manager => manager.runId)\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  /**\n   * Run the LLM on the given prompts and input, handling caching.\n   */\n  async generate(prompts, options, callbacks) {\n    var _runnableConfig$callb;\n    if (!Array.isArray(prompts)) {\n      throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n    }\n    let parsedOptions;\n    if (Array.isArray(options)) {\n      parsedOptions = {\n        stop: options\n      };\n    } else {\n      parsedOptions = options;\n    }\n    const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(parsedOptions);\n    runnableConfig.callbacks = (_runnableConfig$callb = runnableConfig.callbacks) !== null && _runnableConfig$callb !== void 0 ? _runnableConfig$callb : callbacks;\n    if (!this.cache) {\n      return this._generateUncached(prompts, callOptions, runnableConfig);\n    }\n    const {\n      cache\n    } = this;\n    const llmStringKey = this._getSerializedCacheKeyParametersForCall(callOptions);\n    const {\n      generations,\n      missingPromptIndices\n    } = await this._generateCached({\n      prompts,\n      cache,\n      llmStringKey,\n      parsedOptions: callOptions,\n      handledOptions: runnableConfig\n    });\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      var _results$llmOutput;\n      const results = await this._generateUncached(missingPromptIndices.map(i => prompts[i]), callOptions, runnableConfig);\n      await Promise.all(results.generations.map(async (generation, index) => {\n        const promptIndex = missingPromptIndices[index];\n        generations[promptIndex] = generation;\n        return cache.update(prompts[promptIndex], llmStringKey, generation);\n      }));\n      llmOutput = (_results$llmOutput = results.llmOutput) !== null && _results$llmOutput !== void 0 ? _results$llmOutput : {};\n    }\n    return {\n      generations,\n      llmOutput\n    };\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   * Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n   */\n  async call(prompt, options, callbacks) {\n    const {\n      generations\n    } = await this.generate([prompt], options, callbacks);\n    return generations[0][0].text;\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * This method is similar to `call`, but it's used for making predictions\n   * based on the input text.\n   * @param text Input text for the prediction.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns A prediction based on the input text.\n   */\n  async predict(text, options, callbacks) {\n    return this.call(text, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * This method takes a list of messages, options, and callbacks, and\n   * returns a predicted message.\n   * @param messages A list of messages for the prediction.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns A predicted message based on the list of messages.\n   */\n  async predictMessages(messages, options, callbacks) {\n    const text = getBufferString(messages);\n    const prediction = await this.call(text, options, callbacks);\n    return new AIMessage(prediction);\n  }\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams() {\n    return {};\n  }\n  /**\n   * @deprecated\n   * Return a json-like object representing this LLM.\n   */\n  serialize() {\n    return {\n      ...this._identifyingParams(),\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n  }\n  _modelType() {\n    return \"base_llm\";\n  }\n}\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nexport class LLM extends BaseLLM {\n  async _generate(prompts, options, runManager) {\n    const generations = await Promise.all(prompts.map((prompt, promptIndex) => this._call(prompt, {\n      ...options,\n      promptIndex\n    }, runManager).then(text => [{\n      text\n    }])));\n    return {\n      generations\n    };\n  }\n}","map":{"version":3,"names":["AIMessage","getBufferString","RUN_KEY","GenerationChunk","CallbackManager","BaseLanguageModel","BaseLLM","constructor","_ref","concurrency","rest","maxConcurrency","Object","defineProperty","enumerable","configurable","writable","value","_llmType","invoke","input","options","promptValue","_convertInputToPromptValue","result","generatePrompt","callbacks","generations","text","_streamResponseChunks","_input","_options","_runManager","Error","_separateRunnableConfigFromCallOptions","runnableConfig","callOptions","timeout","signal","AbortSignal","_streamIterator","prototype","prompt","callbackManager_","configure","tags","metadata","verbose","extra","invocation_params","invocationParams","batch_size","runManagers","handleLLMStart","toJSON","toString","undefined","runName","generation","chunk","concat","err","Promise","all","map","runManager","handleLLMError","handleLLMEnd","promptValues","prompts","generate","_flattenLLMResult","llmResult","llmResults","i","length","genList","push","llmOutput","tokenUsage","_generateUncached","parsedOptions","handledOptions","output","_generate","flattenedOutputs","runIds","manager","runId","_generateCached","_ref2","cache","llmStringKey","cached","missingPromptIndices","results","allSettled","index","lookup","cachedResults","filter","_ref3","status","_ref4","promiseResult","handleLLMNewToken","reason","reject","_runnableConfig$callb","Array","isArray","stop","_getSerializedCacheKeyParametersForCall","_results$llmOutput","promptIndex","update","call","predict","predictMessages","messages","prediction","_identifyingParams","serialize","_type","_model","_modelType","LLM","_call","then"],"sources":["/Users/mandylin/Desktop/WebCrack React 2/webcrack/node_modules/@langchain/core/dist/language_models/llms.js"],"sourcesContent":["import { AIMessage, getBufferString, } from \"../messages/index.js\";\nimport { RUN_KEY, GenerationChunk, } from \"../outputs.js\";\nimport { CallbackManager, } from \"../callbacks/manager.js\";\nimport { BaseLanguageModel, } from \"./base.js\";\n/**\n * LLM Wrapper. Takes in a prompt (or prompts) and returns a string.\n */\nexport class BaseLLM extends BaseLanguageModel {\n    constructor({ concurrency, ...rest }) {\n        super(concurrency ? { maxConcurrency: concurrency, ...rest } : rest);\n        // Only ever instantiated in main LangChain\n        Object.defineProperty(this, \"lc_namespace\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: [\"langchain\", \"llms\", this._llmType()]\n        });\n    }\n    /**\n     * This method takes an input and options, and returns a string. It\n     * converts the input to a prompt value and generates a result based on\n     * the prompt.\n     * @param input Input for the LLM.\n     * @param options Options for the LLM call.\n     * @returns A string result based on the prompt.\n     */\n    async invoke(input, options) {\n        const promptValue = BaseLLM._convertInputToPromptValue(input);\n        const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n        return result.generations[0][0].text;\n    }\n    // eslint-disable-next-line require-yield\n    async *_streamResponseChunks(_input, _options, _runManager) {\n        throw new Error(\"Not implemented.\");\n    }\n    _separateRunnableConfigFromCallOptions(options) {\n        const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n        if (callOptions?.timeout && !callOptions.signal) {\n            callOptions.signal = AbortSignal.timeout(callOptions.timeout);\n        }\n        return [runnableConfig, callOptions];\n    }\n    async *_streamIterator(input, options) {\n        // Subclass check required to avoid double callbacks with default implementation\n        if (this._streamResponseChunks === BaseLLM.prototype._streamResponseChunks) {\n            yield this.invoke(input, options);\n        }\n        else {\n            const prompt = BaseLLM._convertInputToPromptValue(input);\n            const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(options);\n            const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, { verbose: this.verbose });\n            const extra = {\n                options: callOptions,\n                invocation_params: this?.invocationParams(callOptions),\n                batch_size: 1,\n            };\n            const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), [prompt.toString()], undefined, undefined, extra, undefined, undefined, runnableConfig.runName);\n            let generation = new GenerationChunk({\n                text: \"\",\n            });\n            try {\n                for await (const chunk of this._streamResponseChunks(input.toString(), callOptions, runManagers?.[0])) {\n                    if (!generation) {\n                        generation = chunk;\n                    }\n                    else {\n                        generation = generation.concat(chunk);\n                    }\n                    if (typeof chunk.text === \"string\") {\n                        yield chunk.text;\n                    }\n                }\n            }\n            catch (err) {\n                await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n                throw err;\n            }\n            await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMEnd({\n                generations: [[generation]],\n            })));\n        }\n    }\n    /**\n     * This method takes prompt values, options, and callbacks, and generates\n     * a result based on the prompts.\n     * @param promptValues Prompt values for the LLM.\n     * @param options Options for the LLM call.\n     * @param callbacks Callbacks for the LLM call.\n     * @returns An LLMResult based on the prompts.\n     */\n    async generatePrompt(promptValues, options, callbacks) {\n        const prompts = promptValues.map((promptValue) => promptValue.toString());\n        return this.generate(prompts, options, callbacks);\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    invocationParams(_options) {\n        return {};\n    }\n    _flattenLLMResult(llmResult) {\n        const llmResults = [];\n        for (let i = 0; i < llmResult.generations.length; i += 1) {\n            const genList = llmResult.generations[i];\n            if (i === 0) {\n                llmResults.push({\n                    generations: [genList],\n                    llmOutput: llmResult.llmOutput,\n                });\n            }\n            else {\n                const llmOutput = llmResult.llmOutput\n                    ? { ...llmResult.llmOutput, tokenUsage: {} }\n                    : undefined;\n                llmResults.push({\n                    generations: [genList],\n                    llmOutput,\n                });\n            }\n        }\n        return llmResults;\n    }\n    /** @ignore */\n    async _generateUncached(prompts, parsedOptions, handledOptions) {\n        const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: parsedOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n            batch_size: prompts.length,\n        };\n        const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), prompts, undefined, undefined, extra, undefined, undefined, handledOptions?.runName);\n        let output;\n        try {\n            output = await this._generate(prompts, parsedOptions, runManagers?.[0]);\n        }\n        catch (err) {\n            await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n            throw err;\n        }\n        const flattenedOutputs = this._flattenLLMResult(output);\n        await Promise.all((runManagers ?? []).map((runManager, i) => runManager?.handleLLMEnd(flattenedOutputs[i])));\n        const runIds = runManagers?.map((manager) => manager.runId) || undefined;\n        // This defines RUN_KEY as a non-enumerable property on the output object\n        // so that it is not serialized when the output is stringified, and so that\n        // it isnt included when listing the keys of the output object.\n        Object.defineProperty(output, RUN_KEY, {\n            value: runIds ? { runIds } : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    async _generateCached({ prompts, cache, llmStringKey, parsedOptions, handledOptions, }) {\n        const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: parsedOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n            batch_size: prompts.length,\n            cached: true,\n        };\n        const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), prompts, undefined, undefined, extra, undefined, undefined, handledOptions?.runName);\n        // generate results\n        const missingPromptIndices = [];\n        const results = await Promise.allSettled(prompts.map(async (prompt, index) => {\n            const result = await cache.lookup(prompt, llmStringKey);\n            if (result == null) {\n                missingPromptIndices.push(index);\n            }\n            return result;\n        }));\n        // Map run managers to the results before filtering out null results\n        // Null results are just absent from the cache.\n        const cachedResults = results\n            .map((result, index) => ({ result, runManager: runManagers?.[index] }))\n            .filter(({ result }) => (result.status === \"fulfilled\" && result.value != null) ||\n            result.status === \"rejected\");\n        // Handle results and call run managers\n        const generations = [];\n        await Promise.all(cachedResults.map(async ({ result: promiseResult, runManager }, i) => {\n            if (promiseResult.status === \"fulfilled\") {\n                const result = promiseResult.value;\n                generations[i] = result;\n                if (result.length) {\n                    await runManager?.handleLLMNewToken(result[0].text);\n                }\n                return runManager?.handleLLMEnd({\n                    generations: [result],\n                });\n            }\n            else {\n                // status === \"rejected\"\n                await runManager?.handleLLMError(promiseResult.reason);\n                return Promise.reject(promiseResult.reason);\n            }\n        }));\n        const output = {\n            generations,\n            missingPromptIndices,\n        };\n        // This defines RUN_KEY as a non-enumerable property on the output object\n        // so that it is not serialized when the output is stringified, and so that\n        // it isnt included when listing the keys of the output object.\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManagers\n                ? { runIds: runManagers?.map((manager) => manager.runId) }\n                : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    /**\n     * Run the LLM on the given prompts and input, handling caching.\n     */\n    async generate(prompts, options, callbacks) {\n        if (!Array.isArray(prompts)) {\n            throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n        }\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else {\n            parsedOptions = options;\n        }\n        const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(parsedOptions);\n        runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n        if (!this.cache) {\n            return this._generateUncached(prompts, callOptions, runnableConfig);\n        }\n        const { cache } = this;\n        const llmStringKey = this._getSerializedCacheKeyParametersForCall(callOptions);\n        const { generations, missingPromptIndices } = await this._generateCached({\n            prompts,\n            cache,\n            llmStringKey,\n            parsedOptions: callOptions,\n            handledOptions: runnableConfig,\n        });\n        let llmOutput = {};\n        if (missingPromptIndices.length > 0) {\n            const results = await this._generateUncached(missingPromptIndices.map((i) => prompts[i]), callOptions, runnableConfig);\n            await Promise.all(results.generations.map(async (generation, index) => {\n                const promptIndex = missingPromptIndices[index];\n                generations[promptIndex] = generation;\n                return cache.update(prompts[promptIndex], llmStringKey, generation);\n            }));\n            llmOutput = results.llmOutput ?? {};\n        }\n        return { generations, llmOutput };\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     * Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n     */\n    async call(prompt, options, callbacks) {\n        const { generations } = await this.generate([prompt], options, callbacks);\n        return generations[0][0].text;\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * This method is similar to `call`, but it's used for making predictions\n     * based on the input text.\n     * @param text Input text for the prediction.\n     * @param options Options for the LLM call.\n     * @param callbacks Callbacks for the LLM call.\n     * @returns A prediction based on the input text.\n     */\n    async predict(text, options, callbacks) {\n        return this.call(text, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * This method takes a list of messages, options, and callbacks, and\n     * returns a predicted message.\n     * @param messages A list of messages for the prediction.\n     * @param options Options for the LLM call.\n     * @param callbacks Callbacks for the LLM call.\n     * @returns A predicted message based on the list of messages.\n     */\n    async predictMessages(messages, options, callbacks) {\n        const text = getBufferString(messages);\n        const prediction = await this.call(text, options, callbacks);\n        return new AIMessage(prediction);\n    }\n    /**\n     * Get the identifying parameters of the LLM.\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    _identifyingParams() {\n        return {};\n    }\n    /**\n     * @deprecated\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this._identifyingParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    _modelType() {\n        return \"base_llm\";\n    }\n}\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nexport class LLM extends BaseLLM {\n    async _generate(prompts, options, runManager) {\n        const generations = await Promise.all(prompts.map((prompt, promptIndex) => this._call(prompt, { ...options, promptIndex }, runManager).then((text) => [{ text }])));\n        return { generations };\n    }\n}\n"],"mappings":"AAAA,SAASA,SAAS,EAAEC,eAAe,QAAS,sBAAsB;AAClE,SAASC,OAAO,EAAEC,eAAe,QAAS,eAAe;AACzD,SAASC,eAAe,QAAS,yBAAyB;AAC1D,SAASC,iBAAiB,QAAS,WAAW;AAC9C;AACA;AACA;AACA,OAAO,MAAMC,OAAO,SAASD,iBAAiB,CAAC;EAC3CE,WAAWA,CAAAC,IAAA,EAA2B;IAAA,IAA1B;MAAEC,WAAW;MAAE,GAAGC;IAAK,CAAC,GAAAF,IAAA;IAChC,KAAK,CAACC,WAAW,GAAG;MAAEE,cAAc,EAAEF,WAAW;MAAE,GAAGC;IAAK,CAAC,GAAGA,IAAI,CAAC;IACpE;IACAE,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,CAAC,WAAW,EAAE,MAAM,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC;IAChD,CAAC,CAAC;EACN;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMC,MAAMA,CAACC,KAAK,EAAEC,OAAO,EAAE;IACzB,MAAMC,WAAW,GAAGhB,OAAO,CAACiB,0BAA0B,CAACH,KAAK,CAAC;IAC7D,MAAMI,MAAM,GAAG,MAAM,IAAI,CAACC,cAAc,CAAC,CAACH,WAAW,CAAC,EAAED,OAAO,EAAEA,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEK,SAAS,CAAC;IACpF,OAAOF,MAAM,CAACG,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,IAAI;EACxC;EACA;EACA,OAAOC,qBAAqBA,CAACC,MAAM,EAAEC,QAAQ,EAAEC,WAAW,EAAE;IACxD,MAAM,IAAIC,KAAK,CAAC,kBAAkB,CAAC;EACvC;EACAC,sCAAsCA,CAACb,OAAO,EAAE;IAC5C,MAAM,CAACc,cAAc,EAAEC,WAAW,CAAC,GAAG,KAAK,CAACF,sCAAsC,CAACb,OAAO,CAAC;IAC3F,IAAIe,WAAW,aAAXA,WAAW,eAAXA,WAAW,CAAEC,OAAO,IAAI,CAACD,WAAW,CAACE,MAAM,EAAE;MAC7CF,WAAW,CAACE,MAAM,GAAGC,WAAW,CAACF,OAAO,CAACD,WAAW,CAACC,OAAO,CAAC;IACjE;IACA,OAAO,CAACF,cAAc,EAAEC,WAAW,CAAC;EACxC;EACA,OAAOI,eAAeA,CAACpB,KAAK,EAAEC,OAAO,EAAE;IACnC;IACA,IAAI,IAAI,CAACQ,qBAAqB,KAAKvB,OAAO,CAACmC,SAAS,CAACZ,qBAAqB,EAAE;MACxE,MAAM,IAAI,CAACV,MAAM,CAACC,KAAK,EAAEC,OAAO,CAAC;IACrC,CAAC,MACI;MACD,MAAMqB,MAAM,GAAGpC,OAAO,CAACiB,0BAA0B,CAACH,KAAK,CAAC;MACxD,MAAM,CAACe,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACF,sCAAsC,CAACb,OAAO,CAAC;MAC1F,MAAMsB,gBAAgB,GAAG,MAAMvC,eAAe,CAACwC,SAAS,CAACT,cAAc,CAACT,SAAS,EAAE,IAAI,CAACA,SAAS,EAAES,cAAc,CAACU,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEV,cAAc,CAACW,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;QAAEC,OAAO,EAAE,IAAI,CAACA;MAAQ,CAAC,CAAC;MACrM,MAAMC,KAAK,GAAG;QACV3B,OAAO,EAAEe,WAAW;QACpBa,iBAAiB,EAAE,IAAI,aAAJ,IAAI,uBAAJ,IAAI,CAAEC,gBAAgB,CAACd,WAAW,CAAC;QACtDe,UAAU,EAAE;MAChB,CAAC;MACD,MAAMC,WAAW,GAAG,OAAMT,gBAAgB,aAAhBA,gBAAgB,uBAAhBA,gBAAgB,CAAEU,cAAc,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAE,CAACZ,MAAM,CAACa,QAAQ,CAAC,CAAC,CAAC,EAAEC,SAAS,EAAEA,SAAS,EAAER,KAAK,EAAEQ,SAAS,EAAEA,SAAS,EAAErB,cAAc,CAACsB,OAAO,CAAC;MACzK,IAAIC,UAAU,GAAG,IAAIvD,eAAe,CAAC;QACjCyB,IAAI,EAAE;MACV,CAAC,CAAC;MACF,IAAI;QACA,WAAW,MAAM+B,KAAK,IAAI,IAAI,CAAC9B,qBAAqB,CAACT,KAAK,CAACmC,QAAQ,CAAC,CAAC,EAAEnB,WAAW,EAAEgB,WAAW,aAAXA,WAAW,uBAAXA,WAAW,CAAG,CAAC,CAAC,CAAC,EAAE;UACnG,IAAI,CAACM,UAAU,EAAE;YACbA,UAAU,GAAGC,KAAK;UACtB,CAAC,MACI;YACDD,UAAU,GAAGA,UAAU,CAACE,MAAM,CAACD,KAAK,CAAC;UACzC;UACA,IAAI,OAAOA,KAAK,CAAC/B,IAAI,KAAK,QAAQ,EAAE;YAChC,MAAM+B,KAAK,CAAC/B,IAAI;UACpB;QACJ;MACJ,CAAC,CACD,OAAOiC,GAAG,EAAE;QACR,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACX,WAAW,aAAXA,WAAW,cAAXA,WAAW,GAAI,EAAE,EAAEY,GAAG,CAAEC,UAAU,IAAKA,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEC,cAAc,CAACL,GAAG,CAAC,CAAC,CAAC;QAC3F,MAAMA,GAAG;MACb;MACA,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACX,WAAW,aAAXA,WAAW,cAAXA,WAAW,GAAI,EAAE,EAAEY,GAAG,CAAEC,UAAU,IAAKA,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEE,YAAY,CAAC;QAC/ExC,WAAW,EAAE,CAAC,CAAC+B,UAAU,CAAC;MAC9B,CAAC,CAAC,CAAC,CAAC;IACR;EACJ;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMjC,cAAcA,CAAC2C,YAAY,EAAE/C,OAAO,EAAEK,SAAS,EAAE;IACnD,MAAM2C,OAAO,GAAGD,YAAY,CAACJ,GAAG,CAAE1C,WAAW,IAAKA,WAAW,CAACiC,QAAQ,CAAC,CAAC,CAAC;IACzE,OAAO,IAAI,CAACe,QAAQ,CAACD,OAAO,EAAEhD,OAAO,EAAEK,SAAS,CAAC;EACrD;EACA;AACJ;AACA;EACI;EACAwB,gBAAgBA,CAACnB,QAAQ,EAAE;IACvB,OAAO,CAAC,CAAC;EACb;EACAwC,iBAAiBA,CAACC,SAAS,EAAE;IACzB,MAAMC,UAAU,GAAG,EAAE;IACrB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGF,SAAS,CAAC7C,WAAW,CAACgD,MAAM,EAAED,CAAC,IAAI,CAAC,EAAE;MACtD,MAAME,OAAO,GAAGJ,SAAS,CAAC7C,WAAW,CAAC+C,CAAC,CAAC;MACxC,IAAIA,CAAC,KAAK,CAAC,EAAE;QACTD,UAAU,CAACI,IAAI,CAAC;UACZlD,WAAW,EAAE,CAACiD,OAAO,CAAC;UACtBE,SAAS,EAAEN,SAAS,CAACM;QACzB,CAAC,CAAC;MACN,CAAC,MACI;QACD,MAAMA,SAAS,GAAGN,SAAS,CAACM,SAAS,GAC/B;UAAE,GAAGN,SAAS,CAACM,SAAS;UAAEC,UAAU,EAAE,CAAC;QAAE,CAAC,GAC1CvB,SAAS;QACfiB,UAAU,CAACI,IAAI,CAAC;UACZlD,WAAW,EAAE,CAACiD,OAAO,CAAC;UACtBE;QACJ,CAAC,CAAC;MACN;IACJ;IACA,OAAOL,UAAU;EACrB;EACA;EACA,MAAMO,iBAAiBA,CAACX,OAAO,EAAEY,aAAa,EAAEC,cAAc,EAAE;IAC5D,MAAMvC,gBAAgB,GAAG,MAAMvC,eAAe,CAACwC,SAAS,CAACsC,cAAc,CAACxD,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEwD,cAAc,CAACrC,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEqC,cAAc,CAACpC,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;MAAEC,OAAO,EAAE,IAAI,CAACA;IAAQ,CAAC,CAAC;IACrM,MAAMC,KAAK,GAAG;MACV3B,OAAO,EAAE4D,aAAa;MACtBhC,iBAAiB,EAAE,IAAI,aAAJ,IAAI,uBAAJ,IAAI,CAAEC,gBAAgB,CAAC+B,aAAa,CAAC;MACxD9B,UAAU,EAAEkB,OAAO,CAACM;IACxB,CAAC;IACD,MAAMvB,WAAW,GAAG,OAAMT,gBAAgB,aAAhBA,gBAAgB,uBAAhBA,gBAAgB,CAAEU,cAAc,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEe,OAAO,EAAEb,SAAS,EAAEA,SAAS,EAAER,KAAK,EAAEQ,SAAS,EAAEA,SAAS,EAAE0B,cAAc,aAAdA,cAAc,uBAAdA,cAAc,CAAEzB,OAAO,CAAC;IAC9J,IAAI0B,MAAM;IACV,IAAI;MACAA,MAAM,GAAG,MAAM,IAAI,CAACC,SAAS,CAACf,OAAO,EAAEY,aAAa,EAAE7B,WAAW,aAAXA,WAAW,uBAAXA,WAAW,CAAG,CAAC,CAAC,CAAC;IAC3E,CAAC,CACD,OAAOS,GAAG,EAAE;MACR,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACX,WAAW,aAAXA,WAAW,cAAXA,WAAW,GAAI,EAAE,EAAEY,GAAG,CAAEC,UAAU,IAAKA,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEC,cAAc,CAACL,GAAG,CAAC,CAAC,CAAC;MAC3F,MAAMA,GAAG;IACb;IACA,MAAMwB,gBAAgB,GAAG,IAAI,CAACd,iBAAiB,CAACY,MAAM,CAAC;IACvD,MAAMrB,OAAO,CAACC,GAAG,CAAC,CAACX,WAAW,aAAXA,WAAW,cAAXA,WAAW,GAAI,EAAE,EAAEY,GAAG,CAAC,CAACC,UAAU,EAAES,CAAC,KAAKT,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEE,YAAY,CAACkB,gBAAgB,CAACX,CAAC,CAAC,CAAC,CAAC,CAAC;IAC5G,MAAMY,MAAM,GAAG,CAAAlC,WAAW,aAAXA,WAAW,uBAAXA,WAAW,CAAEY,GAAG,CAAEuB,OAAO,IAAKA,OAAO,CAACC,KAAK,CAAC,KAAIhC,SAAS;IACxE;IACA;IACA;IACA5C,MAAM,CAACC,cAAc,CAACsE,MAAM,EAAEjF,OAAO,EAAE;MACnCe,KAAK,EAAEqE,MAAM,GAAG;QAAEA;MAAO,CAAC,GAAG9B,SAAS;MACtCzC,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOoE,MAAM;EACjB;EACA,MAAMM,eAAeA,CAAAC,KAAA,EAAmE;IAAA,IAAlE;MAAErB,OAAO;MAAEsB,KAAK;MAAEC,YAAY;MAAEX,aAAa;MAAEC;IAAgB,CAAC,GAAAQ,KAAA;IAClF,MAAM/C,gBAAgB,GAAG,MAAMvC,eAAe,CAACwC,SAAS,CAACsC,cAAc,CAACxD,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEwD,cAAc,CAACrC,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEqC,cAAc,CAACpC,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;MAAEC,OAAO,EAAE,IAAI,CAACA;IAAQ,CAAC,CAAC;IACrM,MAAMC,KAAK,GAAG;MACV3B,OAAO,EAAE4D,aAAa;MACtBhC,iBAAiB,EAAE,IAAI,aAAJ,IAAI,uBAAJ,IAAI,CAAEC,gBAAgB,CAAC+B,aAAa,CAAC;MACxD9B,UAAU,EAAEkB,OAAO,CAACM,MAAM;MAC1BkB,MAAM,EAAE;IACZ,CAAC;IACD,MAAMzC,WAAW,GAAG,OAAMT,gBAAgB,aAAhBA,gBAAgB,uBAAhBA,gBAAgB,CAAEU,cAAc,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEe,OAAO,EAAEb,SAAS,EAAEA,SAAS,EAAER,KAAK,EAAEQ,SAAS,EAAEA,SAAS,EAAE0B,cAAc,aAAdA,cAAc,uBAAdA,cAAc,CAAEzB,OAAO,CAAC;IAC9J;IACA,MAAMqC,oBAAoB,GAAG,EAAE;IAC/B,MAAMC,OAAO,GAAG,MAAMjC,OAAO,CAACkC,UAAU,CAAC3B,OAAO,CAACL,GAAG,CAAC,OAAOtB,MAAM,EAAEuD,KAAK,KAAK;MAC1E,MAAMzE,MAAM,GAAG,MAAMmE,KAAK,CAACO,MAAM,CAACxD,MAAM,EAAEkD,YAAY,CAAC;MACvD,IAAIpE,MAAM,IAAI,IAAI,EAAE;QAChBsE,oBAAoB,CAACjB,IAAI,CAACoB,KAAK,CAAC;MACpC;MACA,OAAOzE,MAAM;IACjB,CAAC,CAAC,CAAC;IACH;IACA;IACA,MAAM2E,aAAa,GAAGJ,OAAO,CACxB/B,GAAG,CAAC,CAACxC,MAAM,EAAEyE,KAAK,MAAM;MAAEzE,MAAM;MAAEyC,UAAU,EAAEb,WAAW,aAAXA,WAAW,uBAAXA,WAAW,CAAG6C,KAAK;IAAE,CAAC,CAAC,CAAC,CACtEG,MAAM,CAACC,KAAA;MAAA,IAAC;QAAE7E;MAAO,CAAC,GAAA6E,KAAA;MAAA,OAAM7E,MAAM,CAAC8E,MAAM,KAAK,WAAW,IAAI9E,MAAM,CAACP,KAAK,IAAI,IAAI,IAC9EO,MAAM,CAAC8E,MAAM,KAAK,UAAU;IAAA,EAAC;IACjC;IACA,MAAM3E,WAAW,GAAG,EAAE;IACtB,MAAMmC,OAAO,CAACC,GAAG,CAACoC,aAAa,CAACnC,GAAG,CAAC,OAAAuC,KAAA,EAA8C7B,CAAC,KAAK;MAAA,IAA7C;QAAElD,MAAM,EAAEgF,aAAa;QAAEvC;MAAW,CAAC,GAAAsC,KAAA;MAC5E,IAAIC,aAAa,CAACF,MAAM,KAAK,WAAW,EAAE;QACtC,MAAM9E,MAAM,GAAGgF,aAAa,CAACvF,KAAK;QAClCU,WAAW,CAAC+C,CAAC,CAAC,GAAGlD,MAAM;QACvB,IAAIA,MAAM,CAACmD,MAAM,EAAE;UACf,OAAMV,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEwC,iBAAiB,CAACjF,MAAM,CAAC,CAAC,CAAC,CAACI,IAAI,CAAC;QACvD;QACA,OAAOqC,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEE,YAAY,CAAC;UAC5BxC,WAAW,EAAE,CAACH,MAAM;QACxB,CAAC,CAAC;MACN,CAAC,MACI;QACD;QACA,OAAMyC,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEC,cAAc,CAACsC,aAAa,CAACE,MAAM,CAAC;QACtD,OAAO5C,OAAO,CAAC6C,MAAM,CAACH,aAAa,CAACE,MAAM,CAAC;MAC/C;IACJ,CAAC,CAAC,CAAC;IACH,MAAMvB,MAAM,GAAG;MACXxD,WAAW;MACXmE;IACJ,CAAC;IACD;IACA;IACA;IACAlF,MAAM,CAACC,cAAc,CAACsE,MAAM,EAAEjF,OAAO,EAAE;MACnCe,KAAK,EAAEmC,WAAW,GACZ;QAAEkC,MAAM,EAAElC,WAAW,aAAXA,WAAW,uBAAXA,WAAW,CAAEY,GAAG,CAAEuB,OAAO,IAAKA,OAAO,CAACC,KAAK;MAAE,CAAC,GACxDhC,SAAS;MACfzC,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOoE,MAAM;EACjB;EACA;AACJ;AACA;EACI,MAAMb,QAAQA,CAACD,OAAO,EAAEhD,OAAO,EAAEK,SAAS,EAAE;IAAA,IAAAkF,qBAAA;IACxC,IAAI,CAACC,KAAK,CAACC,OAAO,CAACzC,OAAO,CAAC,EAAE;MACzB,MAAM,IAAIpC,KAAK,CAAC,iDAAiD,CAAC;IACtE;IACA,IAAIgD,aAAa;IACjB,IAAI4B,KAAK,CAACC,OAAO,CAACzF,OAAO,CAAC,EAAE;MACxB4D,aAAa,GAAG;QAAE8B,IAAI,EAAE1F;MAAQ,CAAC;IACrC,CAAC,MACI;MACD4D,aAAa,GAAG5D,OAAO;IAC3B;IACA,MAAM,CAACc,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACF,sCAAsC,CAAC+C,aAAa,CAAC;IAChG9C,cAAc,CAACT,SAAS,IAAAkF,qBAAA,GAAGzE,cAAc,CAACT,SAAS,cAAAkF,qBAAA,cAAAA,qBAAA,GAAIlF,SAAS;IAChE,IAAI,CAAC,IAAI,CAACiE,KAAK,EAAE;MACb,OAAO,IAAI,CAACX,iBAAiB,CAACX,OAAO,EAAEjC,WAAW,EAAED,cAAc,CAAC;IACvE;IACA,MAAM;MAAEwD;IAAM,CAAC,GAAG,IAAI;IACtB,MAAMC,YAAY,GAAG,IAAI,CAACoB,uCAAuC,CAAC5E,WAAW,CAAC;IAC9E,MAAM;MAAET,WAAW;MAAEmE;IAAqB,CAAC,GAAG,MAAM,IAAI,CAACL,eAAe,CAAC;MACrEpB,OAAO;MACPsB,KAAK;MACLC,YAAY;MACZX,aAAa,EAAE7C,WAAW;MAC1B8C,cAAc,EAAE/C;IACpB,CAAC,CAAC;IACF,IAAI2C,SAAS,GAAG,CAAC,CAAC;IAClB,IAAIgB,oBAAoB,CAACnB,MAAM,GAAG,CAAC,EAAE;MAAA,IAAAsC,kBAAA;MACjC,MAAMlB,OAAO,GAAG,MAAM,IAAI,CAACf,iBAAiB,CAACc,oBAAoB,CAAC9B,GAAG,CAAEU,CAAC,IAAKL,OAAO,CAACK,CAAC,CAAC,CAAC,EAAEtC,WAAW,EAAED,cAAc,CAAC;MACtH,MAAM2B,OAAO,CAACC,GAAG,CAACgC,OAAO,CAACpE,WAAW,CAACqC,GAAG,CAAC,OAAON,UAAU,EAAEuC,KAAK,KAAK;QACnE,MAAMiB,WAAW,GAAGpB,oBAAoB,CAACG,KAAK,CAAC;QAC/CtE,WAAW,CAACuF,WAAW,CAAC,GAAGxD,UAAU;QACrC,OAAOiC,KAAK,CAACwB,MAAM,CAAC9C,OAAO,CAAC6C,WAAW,CAAC,EAAEtB,YAAY,EAAElC,UAAU,CAAC;MACvE,CAAC,CAAC,CAAC;MACHoB,SAAS,IAAAmC,kBAAA,GAAGlB,OAAO,CAACjB,SAAS,cAAAmC,kBAAA,cAAAA,kBAAA,GAAI,CAAC,CAAC;IACvC;IACA,OAAO;MAAEtF,WAAW;MAAEmD;IAAU,CAAC;EACrC;EACA;AACJ;AACA;AACA;EACI,MAAMsC,IAAIA,CAAC1E,MAAM,EAAErB,OAAO,EAAEK,SAAS,EAAE;IACnC,MAAM;MAAEC;IAAY,CAAC,GAAG,MAAM,IAAI,CAAC2C,QAAQ,CAAC,CAAC5B,MAAM,CAAC,EAAErB,OAAO,EAAEK,SAAS,CAAC;IACzE,OAAOC,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,IAAI;EACjC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMyF,OAAOA,CAACzF,IAAI,EAAEP,OAAO,EAAEK,SAAS,EAAE;IACpC,OAAO,IAAI,CAAC0F,IAAI,CAACxF,IAAI,EAAEP,OAAO,EAAEK,SAAS,CAAC;EAC9C;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAM4F,eAAeA,CAACC,QAAQ,EAAElG,OAAO,EAAEK,SAAS,EAAE;IAChD,MAAME,IAAI,GAAG3B,eAAe,CAACsH,QAAQ,CAAC;IACtC,MAAMC,UAAU,GAAG,MAAM,IAAI,CAACJ,IAAI,CAACxF,IAAI,EAAEP,OAAO,EAAEK,SAAS,CAAC;IAC5D,OAAO,IAAI1B,SAAS,CAACwH,UAAU,CAAC;EACpC;EACA;AACJ;AACA;EACI;EACAC,kBAAkBA,CAAA,EAAG;IACjB,OAAO,CAAC,CAAC;EACb;EACA;AACJ;AACA;AACA;EACIC,SAASA,CAAA,EAAG;IACR,OAAO;MACH,GAAG,IAAI,CAACD,kBAAkB,CAAC,CAAC;MAC5BE,KAAK,EAAE,IAAI,CAACzG,QAAQ,CAAC,CAAC;MACtB0G,MAAM,EAAE,IAAI,CAACC,UAAU,CAAC;IAC5B,CAAC;EACL;EACAA,UAAUA,CAAA,EAAG;IACT,OAAO,UAAU;EACrB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,GAAG,SAASxH,OAAO,CAAC;EAC7B,MAAM8E,SAASA,CAACf,OAAO,EAAEhD,OAAO,EAAE4C,UAAU,EAAE;IAC1C,MAAMtC,WAAW,GAAG,MAAMmC,OAAO,CAACC,GAAG,CAACM,OAAO,CAACL,GAAG,CAAC,CAACtB,MAAM,EAAEwE,WAAW,KAAK,IAAI,CAACa,KAAK,CAACrF,MAAM,EAAE;MAAE,GAAGrB,OAAO;MAAE6F;IAAY,CAAC,EAAEjD,UAAU,CAAC,CAAC+D,IAAI,CAAEpG,IAAI,IAAK,CAAC;MAAEA;IAAK,CAAC,CAAC,CAAC,CAAC,CAAC;IACnK,OAAO;MAAED;IAAY,CAAC;EAC1B;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}