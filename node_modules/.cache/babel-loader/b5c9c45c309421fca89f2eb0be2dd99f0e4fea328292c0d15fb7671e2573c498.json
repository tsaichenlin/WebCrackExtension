{"ast":null,"code":"import { InMemoryCache } from \"../caches.js\";\nimport { StringPromptValue, ChatPromptValue } from \"../prompt_values.js\";\nimport { coerceMessageLikeToMessage } from \"../messages/index.js\";\nimport { AsyncCaller } from \"../utils/async_caller.js\";\nimport { encodingForModel } from \"../utils/tiktoken.js\";\nimport { Runnable } from \"../runnables/base.js\";\n// https://www.npmjs.com/package/js-tiktoken\nexport const getModelNameForTiktoken = modelName => {\n  if (modelName.startsWith(\"gpt-3.5-turbo-16k\")) {\n    return \"gpt-3.5-turbo-16k\";\n  }\n  if (modelName.startsWith(\"gpt-3.5-turbo-\")) {\n    return \"gpt-3.5-turbo\";\n  }\n  if (modelName.startsWith(\"gpt-4-32k\")) {\n    return \"gpt-4-32k\";\n  }\n  if (modelName.startsWith(\"gpt-4-\")) {\n    return \"gpt-4\";\n  }\n  return modelName;\n};\nexport const getEmbeddingContextSize = modelName => {\n  switch (modelName) {\n    case \"text-embedding-ada-002\":\n      return 8191;\n    default:\n      return 2046;\n  }\n};\nexport const getModelContextSize = modelName => {\n  switch (getModelNameForTiktoken(modelName)) {\n    case \"gpt-3.5-turbo-16k\":\n      return 16384;\n    case \"gpt-3.5-turbo\":\n      return 4096;\n    case \"gpt-4-32k\":\n      return 32768;\n    case \"gpt-4\":\n      return 8192;\n    case \"text-davinci-003\":\n      return 4097;\n    case \"text-curie-001\":\n      return 2048;\n    case \"text-babbage-001\":\n      return 2048;\n    case \"text-ada-001\":\n      return 2048;\n    case \"code-davinci-002\":\n      return 8000;\n    case \"code-cushman-001\":\n      return 2048;\n    default:\n      return 4097;\n  }\n};\nexport const calculateMaxTokens = async ({\n  prompt,\n  modelName\n}) => {\n  let numTokens;\n  try {\n    numTokens = (await encodingForModel(getModelNameForTiktoken(modelName))).encode(prompt).length;\n  } catch (error) {\n    console.warn(\"Failed to calculate number of tokens, falling back to approximate count\");\n    // fallback to approximate calculation if tiktoken is not available\n    // each token is ~4 characters: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them#\n    numTokens = Math.ceil(prompt.length / 4);\n  }\n  const maxTokens = getModelContextSize(modelName);\n  return maxTokens - numTokens;\n};\nconst getVerbosity = () => false;\n/**\n * Base class for language models, chains, tools.\n */\nexport class BaseLangChain extends Runnable {\n  get lc_attributes() {\n    return {\n      callbacks: undefined,\n      verbose: undefined\n    };\n  }\n  constructor(params) {\n    super(params);\n    /**\n     * Whether to print out response text.\n     */\n    Object.defineProperty(this, \"verbose\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"callbacks\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"tags\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"metadata\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.verbose = params.verbose ?? getVerbosity();\n    this.callbacks = params.callbacks;\n    this.tags = params.tags ?? [];\n    this.metadata = params.metadata ?? {};\n  }\n}\n/**\n * Base class for language models.\n */\nexport class BaseLanguageModel extends BaseLangChain {\n  /**\n   * Keys that the language model accepts as call options.\n   */\n  get callKeys() {\n    return [\"stop\", \"timeout\", \"signal\", \"tags\", \"metadata\", \"callbacks\"];\n  }\n  constructor({\n    callbacks,\n    callbackManager,\n    ...params\n  }) {\n    super({\n      callbacks: callbacks ?? callbackManager,\n      ...params\n    });\n    /**\n     * The async caller should be used by subclasses to make any async calls,\n     * which will thus benefit from the concurrency and retry logic.\n     */\n    Object.defineProperty(this, \"caller\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"cache\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"_encoding\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    if (typeof params.cache === \"object\") {\n      this.cache = params.cache;\n    } else if (params.cache) {\n      this.cache = InMemoryCache.global();\n    } else {\n      this.cache = undefined;\n    }\n    this.caller = new AsyncCaller(params ?? {});\n  }\n  async getNumTokens(content) {\n    // TODO: Figure out correct value.\n    if (typeof content !== \"string\") {\n      return 0;\n    }\n    // fallback to approximate calculation if tiktoken is not available\n    let numTokens = Math.ceil(content.length / 4);\n    if (!this._encoding) {\n      try {\n        this._encoding = await encodingForModel(\"modelName\" in this ? getModelNameForTiktoken(this.modelName) : \"gpt2\");\n      } catch (error) {\n        console.warn(\"Failed to calculate number of tokens, falling back to approximate count\", error);\n      }\n    }\n    if (this._encoding) {\n      try {\n        numTokens = this._encoding.encode(content).length;\n      } catch (error) {\n        console.warn(\"Failed to calculate number of tokens, falling back to approximate count\", error);\n      }\n    }\n    return numTokens;\n  }\n  static _convertInputToPromptValue(input) {\n    if (typeof input === \"string\") {\n      return new StringPromptValue(input);\n    } else if (Array.isArray(input)) {\n      return new ChatPromptValue(input.map(coerceMessageLikeToMessage));\n    } else {\n      return input;\n    }\n  }\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams() {\n    return {};\n  }\n  /**\n   * Create a unique cache key for a specific call to a specific language model.\n   * @param callOptions Call options for the model\n   * @returns A unique cache key.\n   */\n  _getSerializedCacheKeyParametersForCall(callOptions) {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const params = {\n      ...this._identifyingParams(),\n      ...callOptions,\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n    const filteredEntries = Object.entries(params).filter(([_, value]) => value !== undefined);\n    const serializedEntries = filteredEntries.map(([key, value]) => `${key}:${JSON.stringify(value)}`).sort().join(\",\");\n    return serializedEntries;\n  }\n  /**\n   * @deprecated\n   * Return a json-like object representing this LLM.\n   */\n  serialize() {\n    return {\n      ...this._identifyingParams(),\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n  }\n  /**\n   * @deprecated\n   * Load an LLM from a json-like object describing it.\n   */\n  static async deserialize(_data) {\n    throw new Error(\"Use .toJSON() instead\");\n  }\n}","map":{"version":3,"names":["InMemoryCache","StringPromptValue","ChatPromptValue","coerceMessageLikeToMessage","AsyncCaller","encodingForModel","Runnable","getModelNameForTiktoken","modelName","startsWith","getEmbeddingContextSize","getModelContextSize","calculateMaxTokens","prompt","numTokens","encode","length","error","console","warn","Math","ceil","maxTokens","getVerbosity","BaseLangChain","lc_attributes","callbacks","undefined","verbose","constructor","params","Object","defineProperty","enumerable","configurable","writable","value","tags","metadata","BaseLanguageModel","callKeys","callbackManager","cache","global","caller","getNumTokens","content","_encoding","_convertInputToPromptValue","input","Array","isArray","map","_identifyingParams","_getSerializedCacheKeyParametersForCall","callOptions","_type","_llmType","_model","_modelType","filteredEntries","entries","filter","_","serializedEntries","key","JSON","stringify","sort","join","serialize","deserialize","_data","Error"],"sources":["/Users/mandylin/Desktop/WebCrack React/webcrack/node_modules/@langchain/core/dist/language_models/base.js"],"sourcesContent":["import { InMemoryCache } from \"../caches.js\";\nimport { StringPromptValue, ChatPromptValue, } from \"../prompt_values.js\";\nimport { coerceMessageLikeToMessage, } from \"../messages/index.js\";\nimport { AsyncCaller } from \"../utils/async_caller.js\";\nimport { encodingForModel } from \"../utils/tiktoken.js\";\nimport { Runnable } from \"../runnables/base.js\";\n// https://www.npmjs.com/package/js-tiktoken\nexport const getModelNameForTiktoken = (modelName) => {\n    if (modelName.startsWith(\"gpt-3.5-turbo-16k\")) {\n        return \"gpt-3.5-turbo-16k\";\n    }\n    if (modelName.startsWith(\"gpt-3.5-turbo-\")) {\n        return \"gpt-3.5-turbo\";\n    }\n    if (modelName.startsWith(\"gpt-4-32k\")) {\n        return \"gpt-4-32k\";\n    }\n    if (modelName.startsWith(\"gpt-4-\")) {\n        return \"gpt-4\";\n    }\n    return modelName;\n};\nexport const getEmbeddingContextSize = (modelName) => {\n    switch (modelName) {\n        case \"text-embedding-ada-002\":\n            return 8191;\n        default:\n            return 2046;\n    }\n};\nexport const getModelContextSize = (modelName) => {\n    switch (getModelNameForTiktoken(modelName)) {\n        case \"gpt-3.5-turbo-16k\":\n            return 16384;\n        case \"gpt-3.5-turbo\":\n            return 4096;\n        case \"gpt-4-32k\":\n            return 32768;\n        case \"gpt-4\":\n            return 8192;\n        case \"text-davinci-003\":\n            return 4097;\n        case \"text-curie-001\":\n            return 2048;\n        case \"text-babbage-001\":\n            return 2048;\n        case \"text-ada-001\":\n            return 2048;\n        case \"code-davinci-002\":\n            return 8000;\n        case \"code-cushman-001\":\n            return 2048;\n        default:\n            return 4097;\n    }\n};\nexport const calculateMaxTokens = async ({ prompt, modelName, }) => {\n    let numTokens;\n    try {\n        numTokens = (await encodingForModel(getModelNameForTiktoken(modelName))).encode(prompt).length;\n    }\n    catch (error) {\n        console.warn(\"Failed to calculate number of tokens, falling back to approximate count\");\n        // fallback to approximate calculation if tiktoken is not available\n        // each token is ~4 characters: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them#\n        numTokens = Math.ceil(prompt.length / 4);\n    }\n    const maxTokens = getModelContextSize(modelName);\n    return maxTokens - numTokens;\n};\nconst getVerbosity = () => false;\n/**\n * Base class for language models, chains, tools.\n */\nexport class BaseLangChain extends Runnable {\n    get lc_attributes() {\n        return {\n            callbacks: undefined,\n            verbose: undefined,\n        };\n    }\n    constructor(params) {\n        super(params);\n        /**\n         * Whether to print out response text.\n         */\n        Object.defineProperty(this, \"verbose\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"callbacks\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"tags\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"metadata\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.verbose = params.verbose ?? getVerbosity();\n        this.callbacks = params.callbacks;\n        this.tags = params.tags ?? [];\n        this.metadata = params.metadata ?? {};\n    }\n}\n/**\n * Base class for language models.\n */\nexport class BaseLanguageModel extends BaseLangChain {\n    /**\n     * Keys that the language model accepts as call options.\n     */\n    get callKeys() {\n        return [\"stop\", \"timeout\", \"signal\", \"tags\", \"metadata\", \"callbacks\"];\n    }\n    constructor({ callbacks, callbackManager, ...params }) {\n        super({\n            callbacks: callbacks ?? callbackManager,\n            ...params,\n        });\n        /**\n         * The async caller should be used by subclasses to make any async calls,\n         * which will thus benefit from the concurrency and retry logic.\n         */\n        Object.defineProperty(this, \"caller\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"cache\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"_encoding\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        if (typeof params.cache === \"object\") {\n            this.cache = params.cache;\n        }\n        else if (params.cache) {\n            this.cache = InMemoryCache.global();\n        }\n        else {\n            this.cache = undefined;\n        }\n        this.caller = new AsyncCaller(params ?? {});\n    }\n    async getNumTokens(content) {\n        // TODO: Figure out correct value.\n        if (typeof content !== \"string\") {\n            return 0;\n        }\n        // fallback to approximate calculation if tiktoken is not available\n        let numTokens = Math.ceil(content.length / 4);\n        if (!this._encoding) {\n            try {\n                this._encoding = await encodingForModel(\"modelName\" in this\n                    ? getModelNameForTiktoken(this.modelName)\n                    : \"gpt2\");\n            }\n            catch (error) {\n                console.warn(\"Failed to calculate number of tokens, falling back to approximate count\", error);\n            }\n        }\n        if (this._encoding) {\n            try {\n                numTokens = this._encoding.encode(content).length;\n            }\n            catch (error) {\n                console.warn(\"Failed to calculate number of tokens, falling back to approximate count\", error);\n            }\n        }\n        return numTokens;\n    }\n    static _convertInputToPromptValue(input) {\n        if (typeof input === \"string\") {\n            return new StringPromptValue(input);\n        }\n        else if (Array.isArray(input)) {\n            return new ChatPromptValue(input.map(coerceMessageLikeToMessage));\n        }\n        else {\n            return input;\n        }\n    }\n    /**\n     * Get the identifying parameters of the LLM.\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    _identifyingParams() {\n        return {};\n    }\n    /**\n     * Create a unique cache key for a specific call to a specific language model.\n     * @param callOptions Call options for the model\n     * @returns A unique cache key.\n     */\n    _getSerializedCacheKeyParametersForCall(callOptions) {\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        const params = {\n            ...this._identifyingParams(),\n            ...callOptions,\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n        const filteredEntries = Object.entries(params).filter(([_, value]) => value !== undefined);\n        const serializedEntries = filteredEntries\n            .map(([key, value]) => `${key}:${JSON.stringify(value)}`)\n            .sort()\n            .join(\",\");\n        return serializedEntries;\n    }\n    /**\n     * @deprecated\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this._identifyingParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    /**\n     * @deprecated\n     * Load an LLM from a json-like object describing it.\n     */\n    static async deserialize(_data) {\n        throw new Error(\"Use .toJSON() instead\");\n    }\n}\n"],"mappings":"AAAA,SAASA,aAAa,QAAQ,cAAc;AAC5C,SAASC,iBAAiB,EAAEC,eAAe,QAAS,qBAAqB;AACzE,SAASC,0BAA0B,QAAS,sBAAsB;AAClE,SAASC,WAAW,QAAQ,0BAA0B;AACtD,SAASC,gBAAgB,QAAQ,sBAAsB;AACvD,SAASC,QAAQ,QAAQ,sBAAsB;AAC/C;AACA,OAAO,MAAMC,uBAAuB,GAAIC,SAAS,IAAK;EAClD,IAAIA,SAAS,CAACC,UAAU,CAAC,mBAAmB,CAAC,EAAE;IAC3C,OAAO,mBAAmB;EAC9B;EACA,IAAID,SAAS,CAACC,UAAU,CAAC,gBAAgB,CAAC,EAAE;IACxC,OAAO,eAAe;EAC1B;EACA,IAAID,SAAS,CAACC,UAAU,CAAC,WAAW,CAAC,EAAE;IACnC,OAAO,WAAW;EACtB;EACA,IAAID,SAAS,CAACC,UAAU,CAAC,QAAQ,CAAC,EAAE;IAChC,OAAO,OAAO;EAClB;EACA,OAAOD,SAAS;AACpB,CAAC;AACD,OAAO,MAAME,uBAAuB,GAAIF,SAAS,IAAK;EAClD,QAAQA,SAAS;IACb,KAAK,wBAAwB;MACzB,OAAO,IAAI;IACf;MACI,OAAO,IAAI;EACnB;AACJ,CAAC;AACD,OAAO,MAAMG,mBAAmB,GAAIH,SAAS,IAAK;EAC9C,QAAQD,uBAAuB,CAACC,SAAS,CAAC;IACtC,KAAK,mBAAmB;MACpB,OAAO,KAAK;IAChB,KAAK,eAAe;MAChB,OAAO,IAAI;IACf,KAAK,WAAW;MACZ,OAAO,KAAK;IAChB,KAAK,OAAO;MACR,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf,KAAK,gBAAgB;MACjB,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf,KAAK,cAAc;MACf,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf;MACI,OAAO,IAAI;EACnB;AACJ,CAAC;AACD,OAAO,MAAMI,kBAAkB,GAAG,MAAAA,CAAO;EAAEC,MAAM;EAAEL;AAAW,CAAC,KAAK;EAChE,IAAIM,SAAS;EACb,IAAI;IACAA,SAAS,GAAG,CAAC,MAAMT,gBAAgB,CAACE,uBAAuB,CAACC,SAAS,CAAC,CAAC,EAAEO,MAAM,CAACF,MAAM,CAAC,CAACG,MAAM;EAClG,CAAC,CACD,OAAOC,KAAK,EAAE;IACVC,OAAO,CAACC,IAAI,CAAC,yEAAyE,CAAC;IACvF;IACA;IACAL,SAAS,GAAGM,IAAI,CAACC,IAAI,CAACR,MAAM,CAACG,MAAM,GAAG,CAAC,CAAC;EAC5C;EACA,MAAMM,SAAS,GAAGX,mBAAmB,CAACH,SAAS,CAAC;EAChD,OAAOc,SAAS,GAAGR,SAAS;AAChC,CAAC;AACD,MAAMS,YAAY,GAAGA,CAAA,KAAM,KAAK;AAChC;AACA;AACA;AACA,OAAO,MAAMC,aAAa,SAASlB,QAAQ,CAAC;EACxC,IAAImB,aAAaA,CAAA,EAAG;IAChB,OAAO;MACHC,SAAS,EAAEC,SAAS;MACpBC,OAAO,EAAED;IACb,CAAC;EACL;EACAE,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb;AACR;AACA;IACQC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,SAAS,EAAE;MACnCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,UAAU,EAAE;MACpCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACR,OAAO,GAAGE,MAAM,CAACF,OAAO,IAAIL,YAAY,CAAC,CAAC;IAC/C,IAAI,CAACG,SAAS,GAAGI,MAAM,CAACJ,SAAS;IACjC,IAAI,CAACW,IAAI,GAAGP,MAAM,CAACO,IAAI,IAAI,EAAE;IAC7B,IAAI,CAACC,QAAQ,GAAGR,MAAM,CAACQ,QAAQ,IAAI,CAAC,CAAC;EACzC;AACJ;AACA;AACA;AACA;AACA,OAAO,MAAMC,iBAAiB,SAASf,aAAa,CAAC;EACjD;AACJ;AACA;EACI,IAAIgB,QAAQA,CAAA,EAAG;IACX,OAAO,CAAC,MAAM,EAAE,SAAS,EAAE,QAAQ,EAAE,MAAM,EAAE,UAAU,EAAE,WAAW,CAAC;EACzE;EACAX,WAAWA,CAAC;IAAEH,SAAS;IAAEe,eAAe;IAAE,GAAGX;EAAO,CAAC,EAAE;IACnD,KAAK,CAAC;MACFJ,SAAS,EAAEA,SAAS,IAAIe,eAAe;MACvC,GAAGX;IACP,CAAC,CAAC;IACF;AACR;AACA;AACA;IACQC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,OAAO,EAAE;MACjCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,OAAON,MAAM,CAACY,KAAK,KAAK,QAAQ,EAAE;MAClC,IAAI,CAACA,KAAK,GAAGZ,MAAM,CAACY,KAAK;IAC7B,CAAC,MACI,IAAIZ,MAAM,CAACY,KAAK,EAAE;MACnB,IAAI,CAACA,KAAK,GAAG1C,aAAa,CAAC2C,MAAM,CAAC,CAAC;IACvC,CAAC,MACI;MACD,IAAI,CAACD,KAAK,GAAGf,SAAS;IAC1B;IACA,IAAI,CAACiB,MAAM,GAAG,IAAIxC,WAAW,CAAC0B,MAAM,IAAI,CAAC,CAAC,CAAC;EAC/C;EACA,MAAMe,YAAYA,CAACC,OAAO,EAAE;IACxB;IACA,IAAI,OAAOA,OAAO,KAAK,QAAQ,EAAE;MAC7B,OAAO,CAAC;IACZ;IACA;IACA,IAAIhC,SAAS,GAAGM,IAAI,CAACC,IAAI,CAACyB,OAAO,CAAC9B,MAAM,GAAG,CAAC,CAAC;IAC7C,IAAI,CAAC,IAAI,CAAC+B,SAAS,EAAE;MACjB,IAAI;QACA,IAAI,CAACA,SAAS,GAAG,MAAM1C,gBAAgB,CAAC,WAAW,IAAI,IAAI,GACrDE,uBAAuB,CAAC,IAAI,CAACC,SAAS,CAAC,GACvC,MAAM,CAAC;MACjB,CAAC,CACD,OAAOS,KAAK,EAAE;QACVC,OAAO,CAACC,IAAI,CAAC,yEAAyE,EAAEF,KAAK,CAAC;MAClG;IACJ;IACA,IAAI,IAAI,CAAC8B,SAAS,EAAE;MAChB,IAAI;QACAjC,SAAS,GAAG,IAAI,CAACiC,SAAS,CAAChC,MAAM,CAAC+B,OAAO,CAAC,CAAC9B,MAAM;MACrD,CAAC,CACD,OAAOC,KAAK,EAAE;QACVC,OAAO,CAACC,IAAI,CAAC,yEAAyE,EAAEF,KAAK,CAAC;MAClG;IACJ;IACA,OAAOH,SAAS;EACpB;EACA,OAAOkC,0BAA0BA,CAACC,KAAK,EAAE;IACrC,IAAI,OAAOA,KAAK,KAAK,QAAQ,EAAE;MAC3B,OAAO,IAAIhD,iBAAiB,CAACgD,KAAK,CAAC;IACvC,CAAC,MACI,IAAIC,KAAK,CAACC,OAAO,CAACF,KAAK,CAAC,EAAE;MAC3B,OAAO,IAAI/C,eAAe,CAAC+C,KAAK,CAACG,GAAG,CAACjD,0BAA0B,CAAC,CAAC;IACrE,CAAC,MACI;MACD,OAAO8C,KAAK;IAChB;EACJ;EACA;AACJ;AACA;EACI;EACAI,kBAAkBA,CAAA,EAAG;IACjB,OAAO,CAAC,CAAC;EACb;EACA;AACJ;AACA;AACA;AACA;EACIC,uCAAuCA,CAACC,WAAW,EAAE;IACjD;IACA,MAAMzB,MAAM,GAAG;MACX,GAAG,IAAI,CAACuB,kBAAkB,CAAC,CAAC;MAC5B,GAAGE,WAAW;MACdC,KAAK,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC;MACtBC,MAAM,EAAE,IAAI,CAACC,UAAU,CAAC;IAC5B,CAAC;IACD,MAAMC,eAAe,GAAG7B,MAAM,CAAC8B,OAAO,CAAC/B,MAAM,CAAC,CAACgC,MAAM,CAAC,CAAC,CAACC,CAAC,EAAE3B,KAAK,CAAC,KAAKA,KAAK,KAAKT,SAAS,CAAC;IAC1F,MAAMqC,iBAAiB,GAAGJ,eAAe,CACpCR,GAAG,CAAC,CAAC,CAACa,GAAG,EAAE7B,KAAK,CAAC,KAAM,GAAE6B,GAAI,IAAGC,IAAI,CAACC,SAAS,CAAC/B,KAAK,CAAE,EAAC,CAAC,CACxDgC,IAAI,CAAC,CAAC,CACNC,IAAI,CAAC,GAAG,CAAC;IACd,OAAOL,iBAAiB;EAC5B;EACA;AACJ;AACA;AACA;EACIM,SAASA,CAAA,EAAG;IACR,OAAO;MACH,GAAG,IAAI,CAACjB,kBAAkB,CAAC,CAAC;MAC5BG,KAAK,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC;MACtBC,MAAM,EAAE,IAAI,CAACC,UAAU,CAAC;IAC5B,CAAC;EACL;EACA;AACJ;AACA;AACA;EACI,aAAaY,WAAWA,CAACC,KAAK,EAAE;IAC5B,MAAM,IAAIC,KAAK,CAAC,uBAAuB,CAAC;EAC5C;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}