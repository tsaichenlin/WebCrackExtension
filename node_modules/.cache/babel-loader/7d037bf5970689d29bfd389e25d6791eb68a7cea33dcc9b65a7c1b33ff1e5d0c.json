{"ast":null,"code":"import { OpenAI as OpenAIClient } from \"openai\";\nimport { AIMessage, AIMessageChunk, ChatMessage, ChatMessageChunk, FunctionMessageChunk, HumanMessageChunk, SystemMessageChunk, ToolMessageChunk } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { BaseChatModel } from \"@langchain/core/language_models/chat_models\";\nimport { convertToOpenAITool } from \"@langchain/core/utils/function_calling\";\nimport { getEndpoint } from \"./utils/azure.js\";\nimport { wrapOpenAIClientError } from \"./utils/openai.js\";\nimport { formatFunctionDefinitions } from \"./utils/openai-format-fndef.js\";\nfunction extractGenericMessageCustomRole(message) {\n  if (message.role !== \"system\" && message.role !== \"assistant\" && message.role !== \"user\" && message.role !== \"function\" && message.role !== \"tool\") {\n    console.warn(\"Unknown message role: \".concat(message.role));\n  }\n  return message.role;\n}\nexport function messageToOpenAIRole(message) {\n  const type = message._getType();\n  switch (type) {\n    case \"system\":\n      return \"system\";\n    case \"ai\":\n      return \"assistant\";\n    case \"human\":\n      return \"user\";\n    case \"function\":\n      return \"function\";\n    case \"tool\":\n      return \"tool\";\n    case \"generic\":\n      {\n        if (!ChatMessage.isInstance(message)) throw new Error(\"Invalid generic chat message\");\n        return extractGenericMessageCustomRole(message);\n      }\n    default:\n      throw new Error(\"Unknown message type: \".concat(type));\n  }\n}\nfunction openAIResponseToChatMessage(message) {\n  var _message$role;\n  switch (message.role) {\n    case \"assistant\":\n      return new AIMessage(message.content || \"\", {\n        function_call: message.function_call,\n        tool_calls: message.tool_calls\n      });\n    default:\n      return new ChatMessage(message.content || \"\", (_message$role = message.role) !== null && _message$role !== void 0 ? _message$role : \"unknown\");\n  }\n}\nfunction _convertDeltaToMessageChunk(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ndelta, defaultRole) {\n  var _delta$role, _delta$content;\n  const role = (_delta$role = delta.role) !== null && _delta$role !== void 0 ? _delta$role : defaultRole;\n  const content = (_delta$content = delta.content) !== null && _delta$content !== void 0 ? _delta$content : \"\";\n  let additional_kwargs;\n  if (delta.function_call) {\n    additional_kwargs = {\n      function_call: delta.function_call\n    };\n  } else if (delta.tool_calls) {\n    additional_kwargs = {\n      tool_calls: delta.tool_calls\n    };\n  } else {\n    additional_kwargs = {};\n  }\n  if (role === \"user\") {\n    return new HumanMessageChunk({\n      content\n    });\n  } else if (role === \"assistant\") {\n    return new AIMessageChunk({\n      content,\n      additional_kwargs\n    });\n  } else if (role === \"system\") {\n    return new SystemMessageChunk({\n      content\n    });\n  } else if (role === \"function\") {\n    return new FunctionMessageChunk({\n      content,\n      additional_kwargs,\n      name: delta.name\n    });\n  } else if (role === \"tool\") {\n    return new ToolMessageChunk({\n      content,\n      additional_kwargs,\n      tool_call_id: delta.tool_call_id\n    });\n  } else {\n    return new ChatMessageChunk({\n      content,\n      role\n    });\n  }\n}\nfunction convertMessagesToOpenAIParams(messages) {\n  // TODO: Function messages do not support array content, fix cast\n  return messages.map(message => ({\n    role: messageToOpenAIRole(message),\n    content: message.content,\n    name: message.name,\n    function_call: message.additional_kwargs.function_call,\n    tool_calls: message.additional_kwargs.tool_calls,\n    tool_call_id: message.tool_call_id\n  }));\n}\n/**\n * Wrapper around OpenAI large language models that use the Chat endpoint.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure you should have the `openai` package installed, with the\n * `AZURE_OPENAI_API_KEY`,\n * `AZURE_OPENAI_API_INSTANCE_NAME`,\n * `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n * and `AZURE_OPENAI_API_VERSION` environment variable set.\n * `AZURE_OPENAI_BASE_PATH` is optional and will override `AZURE_OPENAI_API_INSTANCE_NAME` if you need to use a custom endpoint.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/chat/create |\n * `openai.createChatCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n * @example\n * ```typescript\n * // Create a new instance of ChatOpenAI with specific temperature and model name settings\n * const model = new ChatOpenAI({\n *   temperature: 0.9,\n *   modelName: \"ft:gpt-3.5-turbo-0613:{ORG_NAME}::{MODEL_ID}\",\n * });\n *\n * // Invoke the model with a message and await the response\n * const message = await model.invoke(\"Hi there!\");\n *\n * // Log the response to the console\n * console.log(message);\n *\n * ```\n */\nexport class ChatOpenAI extends BaseChatModel {\n  static lc_name() {\n    return \"ChatOpenAI\";\n  }\n  get callKeys() {\n    return [...super.callKeys, \"options\", \"function_call\", \"functions\", \"tools\", \"tool_choice\", \"promptIndex\", \"response_format\", \"seed\"];\n  }\n  get lc_secrets() {\n    return {\n      openAIApiKey: \"OPENAI_API_KEY\",\n      azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n      organization: \"OPENAI_ORGANIZATION\"\n    };\n  }\n  get lc_aliases() {\n    return {\n      modelName: \"model\",\n      openAIApiKey: \"openai_api_key\",\n      azureOpenAIApiVersion: \"azure_openai_api_version\",\n      azureOpenAIApiKey: \"azure_openai_api_key\",\n      azureOpenAIApiInstanceName: \"azure_openai_api_instance_name\",\n      azureOpenAIApiDeploymentName: \"azure_openai_api_deployment_name\"\n    };\n  }\n  constructor(fields, /** @deprecated */\n  configuration) {\n    var _fields$openAIApiKey, _fields$azureOpenAIAp, _fields$azureOpenAIAp2, _fields$azureOpenAIAp3, _fields$azureOpenAIAp4, _fields$azureOpenAIBa, _fields$configuration, _fields$configuration2, _fields$modelName, _fields$modelKwargs, _fields$temperature, _fields$topP, _fields$frequencyPena, _fields$presencePenal, _fields$n, _fields$streaming, _configuration$basePa, _fields$configuration3, _configuration$baseOp, _configuration$baseOp2, _fields$configuration4, _configuration$baseOp3, _configuration$baseOp4, _fields$configuration5;\n    super(fields !== null && fields !== void 0 ? fields : {});\n    Object.defineProperty(this, \"lc_serializable\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: true\n    });\n    Object.defineProperty(this, \"temperature\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"topP\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"frequencyPenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0\n    });\n    Object.defineProperty(this, \"presencePenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0\n    });\n    Object.defineProperty(this, \"n\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"logitBias\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"modelName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"gpt-3.5-turbo\"\n    });\n    Object.defineProperty(this, \"modelKwargs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"stop\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"user\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"timeout\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"streaming\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    Object.defineProperty(this, \"maxTokens\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"logprobs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"topLogprobs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"openAIApiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiVersion\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiInstanceName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiDeploymentName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIBasePath\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"organization\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"client\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"clientConfig\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.openAIApiKey = (_fields$openAIApiKey = fields === null || fields === void 0 ? void 0 : fields.openAIApiKey) !== null && _fields$openAIApiKey !== void 0 ? _fields$openAIApiKey : getEnvironmentVariable(\"OPENAI_API_KEY\");\n    this.azureOpenAIApiKey = (_fields$azureOpenAIAp = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiKey) !== null && _fields$azureOpenAIAp !== void 0 ? _fields$azureOpenAIAp : getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n    if (!this.azureOpenAIApiKey && !this.openAIApiKey) {\n      throw new Error(\"OpenAI or Azure OpenAI API key not found\");\n    }\n    this.azureOpenAIApiInstanceName = (_fields$azureOpenAIAp2 = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiInstanceName) !== null && _fields$azureOpenAIAp2 !== void 0 ? _fields$azureOpenAIAp2 : getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n    this.azureOpenAIApiDeploymentName = (_fields$azureOpenAIAp3 = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiDeploymentName) !== null && _fields$azureOpenAIAp3 !== void 0 ? _fields$azureOpenAIAp3 : getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\");\n    this.azureOpenAIApiVersion = (_fields$azureOpenAIAp4 = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiVersion) !== null && _fields$azureOpenAIAp4 !== void 0 ? _fields$azureOpenAIAp4 : getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n    this.azureOpenAIBasePath = (_fields$azureOpenAIBa = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIBasePath) !== null && _fields$azureOpenAIBa !== void 0 ? _fields$azureOpenAIBa : getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n    this.organization = (_fields$configuration = fields === null || fields === void 0 || (_fields$configuration2 = fields.configuration) === null || _fields$configuration2 === void 0 ? void 0 : _fields$configuration2.organization) !== null && _fields$configuration !== void 0 ? _fields$configuration : getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n    this.modelName = (_fields$modelName = fields === null || fields === void 0 ? void 0 : fields.modelName) !== null && _fields$modelName !== void 0 ? _fields$modelName : this.modelName;\n    this.modelKwargs = (_fields$modelKwargs = fields === null || fields === void 0 ? void 0 : fields.modelKwargs) !== null && _fields$modelKwargs !== void 0 ? _fields$modelKwargs : {};\n    this.timeout = fields === null || fields === void 0 ? void 0 : fields.timeout;\n    this.temperature = (_fields$temperature = fields === null || fields === void 0 ? void 0 : fields.temperature) !== null && _fields$temperature !== void 0 ? _fields$temperature : this.temperature;\n    this.topP = (_fields$topP = fields === null || fields === void 0 ? void 0 : fields.topP) !== null && _fields$topP !== void 0 ? _fields$topP : this.topP;\n    this.frequencyPenalty = (_fields$frequencyPena = fields === null || fields === void 0 ? void 0 : fields.frequencyPenalty) !== null && _fields$frequencyPena !== void 0 ? _fields$frequencyPena : this.frequencyPenalty;\n    this.presencePenalty = (_fields$presencePenal = fields === null || fields === void 0 ? void 0 : fields.presencePenalty) !== null && _fields$presencePenal !== void 0 ? _fields$presencePenal : this.presencePenalty;\n    this.maxTokens = fields === null || fields === void 0 ? void 0 : fields.maxTokens;\n    this.logprobs = fields === null || fields === void 0 ? void 0 : fields.logprobs;\n    this.topLogprobs = fields === null || fields === void 0 ? void 0 : fields.topLogprobs;\n    this.n = (_fields$n = fields === null || fields === void 0 ? void 0 : fields.n) !== null && _fields$n !== void 0 ? _fields$n : this.n;\n    this.logitBias = fields === null || fields === void 0 ? void 0 : fields.logitBias;\n    this.stop = fields === null || fields === void 0 ? void 0 : fields.stop;\n    this.user = fields === null || fields === void 0 ? void 0 : fields.user;\n    this.streaming = (_fields$streaming = fields === null || fields === void 0 ? void 0 : fields.streaming) !== null && _fields$streaming !== void 0 ? _fields$streaming : false;\n    if (this.azureOpenAIApiKey) {\n      var _this$openAIApiKey;\n      if (!this.azureOpenAIApiInstanceName && !this.azureOpenAIBasePath) {\n        throw new Error(\"Azure OpenAI API instance name not found\");\n      }\n      if (!this.azureOpenAIApiDeploymentName) {\n        throw new Error(\"Azure OpenAI API deployment name not found\");\n      }\n      if (!this.azureOpenAIApiVersion) {\n        throw new Error(\"Azure OpenAI API version not found\");\n      }\n      this.openAIApiKey = (_this$openAIApiKey = this.openAIApiKey) !== null && _this$openAIApiKey !== void 0 ? _this$openAIApiKey : \"\";\n    }\n    this.clientConfig = {\n      apiKey: this.openAIApiKey,\n      organization: this.organization,\n      baseURL: (_configuration$basePa = configuration === null || configuration === void 0 ? void 0 : configuration.basePath) !== null && _configuration$basePa !== void 0 ? _configuration$basePa : fields === null || fields === void 0 || (_fields$configuration3 = fields.configuration) === null || _fields$configuration3 === void 0 ? void 0 : _fields$configuration3.basePath,\n      dangerouslyAllowBrowser: true,\n      defaultHeaders: (_configuration$baseOp = configuration === null || configuration === void 0 || (_configuration$baseOp2 = configuration.baseOptions) === null || _configuration$baseOp2 === void 0 ? void 0 : _configuration$baseOp2.headers) !== null && _configuration$baseOp !== void 0 ? _configuration$baseOp : fields === null || fields === void 0 || (_fields$configuration4 = fields.configuration) === null || _fields$configuration4 === void 0 || (_fields$configuration4 = _fields$configuration4.baseOptions) === null || _fields$configuration4 === void 0 ? void 0 : _fields$configuration4.headers,\n      defaultQuery: (_configuration$baseOp3 = configuration === null || configuration === void 0 || (_configuration$baseOp4 = configuration.baseOptions) === null || _configuration$baseOp4 === void 0 ? void 0 : _configuration$baseOp4.params) !== null && _configuration$baseOp3 !== void 0 ? _configuration$baseOp3 : fields === null || fields === void 0 || (_fields$configuration5 = fields.configuration) === null || _fields$configuration5 === void 0 || (_fields$configuration5 = _fields$configuration5.baseOptions) === null || _fields$configuration5 === void 0 ? void 0 : _fields$configuration5.params,\n      ...configuration,\n      ...(fields === null || fields === void 0 ? void 0 : fields.configuration)\n    };\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams(options) {\n    var _options$stop;\n    function isStructuredToolArray(tools) {\n      return tools !== undefined && tools.every(tool => Array.isArray(tool.lc_namespace));\n    }\n    const params = {\n      model: this.modelName,\n      temperature: this.temperature,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      max_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n      logprobs: this.logprobs,\n      top_logprobs: this.topLogprobs,\n      n: this.n,\n      logit_bias: this.logitBias,\n      stop: (_options$stop = options === null || options === void 0 ? void 0 : options.stop) !== null && _options$stop !== void 0 ? _options$stop : this.stop,\n      user: this.user,\n      stream: this.streaming,\n      functions: options === null || options === void 0 ? void 0 : options.functions,\n      function_call: options === null || options === void 0 ? void 0 : options.function_call,\n      tools: isStructuredToolArray(options === null || options === void 0 ? void 0 : options.tools) ? options === null || options === void 0 ? void 0 : options.tools.map(convertToOpenAITool) : options === null || options === void 0 ? void 0 : options.tools,\n      tool_choice: options === null || options === void 0 ? void 0 : options.tool_choice,\n      response_format: options === null || options === void 0 ? void 0 : options.response_format,\n      seed: options === null || options === void 0 ? void 0 : options.seed,\n      ...this.modelKwargs\n    };\n    return params;\n  }\n  /** @ignore */\n  _identifyingParams() {\n    return {\n      model_name: this.modelName,\n      ...this.invocationParams(),\n      ...this.clientConfig\n    };\n  }\n  async *_streamResponseChunks(messages, options, runManager) {\n    var _options$signal;\n    const messagesMapped = convertMessagesToOpenAIParams(messages);\n    const params = {\n      ...this.invocationParams(options),\n      messages: messagesMapped,\n      stream: true\n    };\n    let defaultRole;\n    const streamIterable = await this.completionWithRetry(params, options);\n    for await (const data of streamIterable) {\n      var _delta$role2, _options$promptIndex, _choice$index, _generationChunk$text;\n      const choice = data === null || data === void 0 ? void 0 : data.choices[0];\n      if (!choice) {\n        continue;\n      }\n      const {\n        delta\n      } = choice;\n      if (!delta) {\n        continue;\n      }\n      const chunk = _convertDeltaToMessageChunk(delta, defaultRole);\n      defaultRole = (_delta$role2 = delta.role) !== null && _delta$role2 !== void 0 ? _delta$role2 : defaultRole;\n      const newTokenIndices = {\n        prompt: (_options$promptIndex = options.promptIndex) !== null && _options$promptIndex !== void 0 ? _options$promptIndex : 0,\n        completion: (_choice$index = choice.index) !== null && _choice$index !== void 0 ? _choice$index : 0\n      };\n      if (typeof chunk.content !== \"string\") {\n        console.log(\"[WARNING]: Received non-string content from OpenAI. This is currently not supported.\");\n        continue;\n      }\n      const generationChunk = new ChatGenerationChunk({\n        message: chunk,\n        text: chunk.content,\n        generationInfo: newTokenIndices\n      });\n      yield generationChunk;\n      // eslint-disable-next-line no-void\n      void (runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMNewToken((_generationChunk$text = generationChunk.text) !== null && _generationChunk$text !== void 0 ? _generationChunk$text : \"\", newTokenIndices, undefined, undefined, undefined, {\n        chunk: generationChunk\n      }));\n    }\n    if ((_options$signal = options.signal) !== null && _options$signal !== void 0 && _options$signal.aborted) {\n      throw new Error(\"AbortError\");\n    }\n  }\n  /**\n   * Get the identifying parameters for the model\n   *\n   */\n  identifyingParams() {\n    return this._identifyingParams();\n  }\n  /** @ignore */\n  async _generate(messages, options, runManager) {\n    const tokenUsage = {};\n    const params = this.invocationParams(options);\n    const messagesMapped = convertMessagesToOpenAIParams(messages);\n    if (params.stream) {\n      const stream = this._streamResponseChunks(messages, options, runManager);\n      const finalChunks = {};\n      for await (const chunk of stream) {\n        var _chunk$generationInfo, _chunk$generationInfo2;\n        const index = (_chunk$generationInfo = (_chunk$generationInfo2 = chunk.generationInfo) === null || _chunk$generationInfo2 === void 0 ? void 0 : _chunk$generationInfo2.completion) !== null && _chunk$generationInfo !== void 0 ? _chunk$generationInfo : 0;\n        if (finalChunks[index] === undefined) {\n          finalChunks[index] = chunk;\n        } else {\n          finalChunks[index] = finalChunks[index].concat(chunk);\n        }\n      }\n      const generations = Object.entries(finalChunks).sort((_ref, _ref2) => {\n        let [aKey] = _ref;\n        let [bKey] = _ref2;\n        return parseInt(aKey, 10) - parseInt(bKey, 10);\n      }).map(_ref3 => {\n        let [_, value] = _ref3;\n        return value;\n      });\n      const {\n        functions,\n        function_call\n      } = this.invocationParams(options);\n      // OpenAI does not support token usage report under stream mode,\n      // fallback to estimation.\n      const promptTokenUsage = await this.getEstimatedTokenCountFromPrompt(messages, functions, function_call);\n      const completionTokenUsage = await this.getNumTokensFromGenerations(generations);\n      tokenUsage.promptTokens = promptTokenUsage;\n      tokenUsage.completionTokens = completionTokenUsage;\n      tokenUsage.totalTokens = promptTokenUsage + completionTokenUsage;\n      return {\n        generations,\n        llmOutput: {\n          estimatedTokenUsage: tokenUsage\n        }\n      };\n    } else {\n      var _data$usage;\n      const data = await this.completionWithRetry({\n        ...params,\n        stream: false,\n        messages: messagesMapped\n      }, {\n        signal: options === null || options === void 0 ? void 0 : options.signal,\n        ...(options === null || options === void 0 ? void 0 : options.options)\n      });\n      const {\n        completion_tokens: completionTokens,\n        prompt_tokens: promptTokens,\n        total_tokens: totalTokens\n      } = (_data$usage = data === null || data === void 0 ? void 0 : data.usage) !== null && _data$usage !== void 0 ? _data$usage : {};\n      if (completionTokens) {\n        var _tokenUsage$completio;\n        tokenUsage.completionTokens = ((_tokenUsage$completio = tokenUsage.completionTokens) !== null && _tokenUsage$completio !== void 0 ? _tokenUsage$completio : 0) + completionTokens;\n      }\n      if (promptTokens) {\n        var _tokenUsage$promptTok;\n        tokenUsage.promptTokens = ((_tokenUsage$promptTok = tokenUsage.promptTokens) !== null && _tokenUsage$promptTok !== void 0 ? _tokenUsage$promptTok : 0) + promptTokens;\n      }\n      if (totalTokens) {\n        var _tokenUsage$totalToke;\n        tokenUsage.totalTokens = ((_tokenUsage$totalToke = tokenUsage.totalTokens) !== null && _tokenUsage$totalToke !== void 0 ? _tokenUsage$totalToke : 0) + totalTokens;\n      }\n      const generations = [];\n      for (const part of (_data$choices = data === null || data === void 0 ? void 0 : data.choices) !== null && _data$choices !== void 0 ? _data$choices : []) {\n        var _data$choices, _part$message$content, _part$message, _part$message2;\n        const text = (_part$message$content = (_part$message = part.message) === null || _part$message === void 0 ? void 0 : _part$message.content) !== null && _part$message$content !== void 0 ? _part$message$content : \"\";\n        const generation = {\n          text,\n          message: openAIResponseToChatMessage((_part$message2 = part.message) !== null && _part$message2 !== void 0 ? _part$message2 : {\n            role: \"assistant\"\n          })\n        };\n        generation.generationInfo = {\n          ...(part.finish_reason ? {\n            finish_reason: part.finish_reason\n          } : {}),\n          ...(part.logprobs ? {\n            logprobs: part.logprobs\n          } : {})\n        };\n        generations.push(generation);\n      }\n      return {\n        generations,\n        llmOutput: {\n          tokenUsage\n        }\n      };\n    }\n  }\n  /**\n   * Estimate the number of tokens a prompt will use.\n   * Modified from: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts\n   */\n  async getEstimatedTokenCountFromPrompt(messages, functions, function_call) {\n    // It appears that if functions are present, the first system message is padded with a trailing newline. This\n    // was inferred by trying lots of combinations of messages and functions and seeing what the token counts were.\n    let tokens = (await this.getNumTokensFromMessages(messages)).totalCount;\n    // If there are functions, add the function definitions as they count towards token usage\n    if (functions && function_call !== \"auto\") {\n      const promptDefinitions = formatFunctionDefinitions(functions);\n      tokens += await this.getNumTokens(promptDefinitions);\n      tokens += 9; // Add nine per completion\n    }\n    // If there's a system message _and_ functions are present, subtract four tokens. I assume this is because\n    // functions typically add a system message, but reuse the first one if it's already there. This offsets\n    // the extra 9 tokens added by the function definitions.\n    if (functions && messages.find(m => m._getType() === \"system\")) {\n      tokens -= 4;\n    }\n    // If function_call is 'none', add one token.\n    // If it's a FunctionCall object, add 4 + the number of tokens in the function name.\n    // If it's undefined or 'auto', don't add anything.\n    if (function_call === \"none\") {\n      tokens += 1;\n    } else if (typeof function_call === \"object\") {\n      tokens += (await this.getNumTokens(function_call.name)) + 4;\n    }\n    return tokens;\n  }\n  /**\n   * Estimate the number of tokens an array of generations have used.\n   */\n  async getNumTokensFromGenerations(generations) {\n    const generationUsages = await Promise.all(generations.map(async generation => {\n      var _generation$message$a;\n      if ((_generation$message$a = generation.message.additional_kwargs) !== null && _generation$message$a !== void 0 && _generation$message$a.function_call) {\n        return (await this.getNumTokensFromMessages([generation.message])).countPerMessage[0];\n      } else {\n        return await this.getNumTokens(generation.message.content);\n      }\n    }));\n    return generationUsages.reduce((a, b) => a + b, 0);\n  }\n  async getNumTokensFromMessages(messages) {\n    let totalCount = 0;\n    let tokensPerMessage = 0;\n    let tokensPerName = 0;\n    // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n    if (this.modelName === \"gpt-3.5-turbo-0301\") {\n      tokensPerMessage = 4;\n      tokensPerName = -1;\n    } else {\n      tokensPerMessage = 3;\n      tokensPerName = 1;\n    }\n    const countPerMessage = await Promise.all(messages.map(async message => {\n      var _openAIMessage$additi, _openAIMessage$additi2, _openAIMessage$additi4;\n      const textCount = await this.getNumTokens(message.content);\n      const roleCount = await this.getNumTokens(messageToOpenAIRole(message));\n      const nameCount = message.name !== undefined ? tokensPerName + (await this.getNumTokens(message.name)) : 0;\n      let count = textCount + tokensPerMessage + roleCount + nameCount;\n      // From: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts messageTokenEstimate\n      const openAIMessage = message;\n      if (openAIMessage._getType() === \"function\") {\n        count -= 2;\n      }\n      if ((_openAIMessage$additi = openAIMessage.additional_kwargs) !== null && _openAIMessage$additi !== void 0 && _openAIMessage$additi.function_call) {\n        count += 3;\n      }\n      if (openAIMessage !== null && openAIMessage !== void 0 && (_openAIMessage$additi2 = openAIMessage.additional_kwargs.function_call) !== null && _openAIMessage$additi2 !== void 0 && _openAIMessage$additi2.name) {\n        var _openAIMessage$additi3;\n        count += await this.getNumTokens((_openAIMessage$additi3 = openAIMessage.additional_kwargs.function_call) === null || _openAIMessage$additi3 === void 0 ? void 0 : _openAIMessage$additi3.name);\n      }\n      if ((_openAIMessage$additi4 = openAIMessage.additional_kwargs.function_call) !== null && _openAIMessage$additi4 !== void 0 && _openAIMessage$additi4.arguments) {\n        var _openAIMessage$additi5;\n        count += await this.getNumTokens(\n        // Remove newlines and spaces\n        JSON.stringify(JSON.parse((_openAIMessage$additi5 = openAIMessage.additional_kwargs.function_call) === null || _openAIMessage$additi5 === void 0 ? void 0 : _openAIMessage$additi5.arguments)));\n      }\n      totalCount += count;\n      return count;\n    }));\n    totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n    return {\n      totalCount,\n      countPerMessage\n    };\n  }\n  async completionWithRetry(request, options) {\n    const requestOptions = this._getClientOptions(options);\n    return this.caller.call(async () => {\n      try {\n        const res = await this.client.chat.completions.create(request, requestOptions);\n        return res;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n  _getClientOptions(options) {\n    if (!this.client) {\n      const openAIEndpointConfig = {\n        azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n        azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n        azureOpenAIApiKey: this.azureOpenAIApiKey,\n        azureOpenAIBasePath: this.azureOpenAIBasePath,\n        baseURL: this.clientConfig.baseURL\n      };\n      const endpoint = getEndpoint(openAIEndpointConfig);\n      const params = {\n        ...this.clientConfig,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0\n      };\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n      this.client = new OpenAIClient(params);\n    }\n    const requestOptions = {\n      ...this.clientConfig,\n      ...options\n    };\n    if (this.azureOpenAIApiKey) {\n      requestOptions.headers = {\n        \"api-key\": this.azureOpenAIApiKey,\n        ...requestOptions.headers\n      };\n      requestOptions.query = {\n        \"api-version\": this.azureOpenAIApiVersion,\n        ...requestOptions.query\n      };\n    }\n    return requestOptions;\n  }\n  _llmType() {\n    return \"openai\";\n  }\n  /** @ignore */\n  _combineLLMOutput() {\n    for (var _len = arguments.length, llmOutputs = new Array(_len), _key = 0; _key < _len; _key++) {\n      llmOutputs[_key] = arguments[_key];\n    }\n    return llmOutputs.reduce((acc, llmOutput) => {\n      if (llmOutput && llmOutput.tokenUsage) {\n        var _llmOutput$tokenUsage, _llmOutput$tokenUsage2, _llmOutput$tokenUsage3;\n        acc.tokenUsage.completionTokens += (_llmOutput$tokenUsage = llmOutput.tokenUsage.completionTokens) !== null && _llmOutput$tokenUsage !== void 0 ? _llmOutput$tokenUsage : 0;\n        acc.tokenUsage.promptTokens += (_llmOutput$tokenUsage2 = llmOutput.tokenUsage.promptTokens) !== null && _llmOutput$tokenUsage2 !== void 0 ? _llmOutput$tokenUsage2 : 0;\n        acc.tokenUsage.totalTokens += (_llmOutput$tokenUsage3 = llmOutput.tokenUsage.totalTokens) !== null && _llmOutput$tokenUsage3 !== void 0 ? _llmOutput$tokenUsage3 : 0;\n      }\n      return acc;\n    }, {\n      tokenUsage: {\n        completionTokens: 0,\n        promptTokens: 0,\n        totalTokens: 0\n      }\n    });\n  }\n}","map":{"version":3,"names":["OpenAI","OpenAIClient","AIMessage","AIMessageChunk","ChatMessage","ChatMessageChunk","FunctionMessageChunk","HumanMessageChunk","SystemMessageChunk","ToolMessageChunk","ChatGenerationChunk","getEnvironmentVariable","BaseChatModel","convertToOpenAITool","getEndpoint","wrapOpenAIClientError","formatFunctionDefinitions","extractGenericMessageCustomRole","message","role","console","warn","concat","messageToOpenAIRole","type","_getType","isInstance","Error","openAIResponseToChatMessage","_message$role","content","function_call","tool_calls","_convertDeltaToMessageChunk","delta","defaultRole","_delta$role","_delta$content","additional_kwargs","name","tool_call_id","convertMessagesToOpenAIParams","messages","map","ChatOpenAI","lc_name","callKeys","lc_secrets","openAIApiKey","azureOpenAIApiKey","organization","lc_aliases","modelName","azureOpenAIApiVersion","azureOpenAIApiInstanceName","azureOpenAIApiDeploymentName","constructor","fields","configuration","_fields$openAIApiKey","_fields$azureOpenAIAp","_fields$azureOpenAIAp2","_fields$azureOpenAIAp3","_fields$azureOpenAIAp4","_fields$azureOpenAIBa","_fields$configuration","_fields$configuration2","_fields$modelName","_fields$modelKwargs","_fields$temperature","_fields$topP","_fields$frequencyPena","_fields$presencePenal","_fields$n","_fields$streaming","_configuration$basePa","_fields$configuration3","_configuration$baseOp","_configuration$baseOp2","_fields$configuration4","_configuration$baseOp3","_configuration$baseOp4","_fields$configuration5","Object","defineProperty","enumerable","configurable","writable","value","azureOpenAIBasePath","modelKwargs","timeout","temperature","topP","frequencyPenalty","presencePenalty","maxTokens","logprobs","topLogprobs","n","logitBias","stop","user","streaming","_this$openAIApiKey","clientConfig","apiKey","baseURL","basePath","dangerouslyAllowBrowser","defaultHeaders","baseOptions","headers","defaultQuery","params","invocationParams","options","_options$stop","isStructuredToolArray","tools","undefined","every","tool","Array","isArray","lc_namespace","model","top_p","frequency_penalty","presence_penalty","max_tokens","top_logprobs","logit_bias","stream","functions","tool_choice","response_format","seed","_identifyingParams","model_name","_streamResponseChunks","runManager","_options$signal","messagesMapped","streamIterable","completionWithRetry","data","_delta$role2","_options$promptIndex","_choice$index","_generationChunk$text","choice","choices","chunk","newTokenIndices","prompt","promptIndex","completion","index","log","generationChunk","text","generationInfo","handleLLMNewToken","signal","aborted","identifyingParams","_generate","tokenUsage","finalChunks","_chunk$generationInfo","_chunk$generationInfo2","generations","entries","sort","_ref","_ref2","aKey","bKey","parseInt","_ref3","_","promptTokenUsage","getEstimatedTokenCountFromPrompt","completionTokenUsage","getNumTokensFromGenerations","promptTokens","completionTokens","totalTokens","llmOutput","estimatedTokenUsage","_data$usage","completion_tokens","prompt_tokens","total_tokens","usage","_tokenUsage$completio","_tokenUsage$promptTok","_tokenUsage$totalToke","part","_data$choices","_part$message$content","_part$message","_part$message2","generation","finish_reason","push","tokens","getNumTokensFromMessages","totalCount","promptDefinitions","getNumTokens","find","m","generationUsages","Promise","all","_generation$message$a","countPerMessage","reduce","a","b","tokensPerMessage","tokensPerName","_openAIMessage$additi","_openAIMessage$additi2","_openAIMessage$additi4","textCount","roleCount","nameCount","count","openAIMessage","_openAIMessage$additi3","arguments","_openAIMessage$additi5","JSON","stringify","parse","request","requestOptions","_getClientOptions","caller","call","res","client","chat","completions","create","e","error","openAIEndpointConfig","endpoint","maxRetries","query","_llmType","_combineLLMOutput","_len","length","llmOutputs","_key","acc","_llmOutput$tokenUsage","_llmOutput$tokenUsage2","_llmOutput$tokenUsage3"],"sources":["/Users/mandylin/Desktop/WebCrack React/webcrack/node_modules/@langchain/openai/dist/chat_models.js"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport { AIMessage, AIMessageChunk, ChatMessage, ChatMessageChunk, FunctionMessageChunk, HumanMessageChunk, SystemMessageChunk, ToolMessageChunk, } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { BaseChatModel, } from \"@langchain/core/language_models/chat_models\";\nimport { convertToOpenAITool } from \"@langchain/core/utils/function_calling\";\nimport { getEndpoint } from \"./utils/azure.js\";\nimport { wrapOpenAIClientError } from \"./utils/openai.js\";\nimport { formatFunctionDefinitions, } from \"./utils/openai-format-fndef.js\";\nfunction extractGenericMessageCustomRole(message) {\n    if (message.role !== \"system\" &&\n        message.role !== \"assistant\" &&\n        message.role !== \"user\" &&\n        message.role !== \"function\" &&\n        message.role !== \"tool\") {\n        console.warn(`Unknown message role: ${message.role}`);\n    }\n    return message.role;\n}\nexport function messageToOpenAIRole(message) {\n    const type = message._getType();\n    switch (type) {\n        case \"system\":\n            return \"system\";\n        case \"ai\":\n            return \"assistant\";\n        case \"human\":\n            return \"user\";\n        case \"function\":\n            return \"function\";\n        case \"tool\":\n            return \"tool\";\n        case \"generic\": {\n            if (!ChatMessage.isInstance(message))\n                throw new Error(\"Invalid generic chat message\");\n            return extractGenericMessageCustomRole(message);\n        }\n        default:\n            throw new Error(`Unknown message type: ${type}`);\n    }\n}\nfunction openAIResponseToChatMessage(message) {\n    switch (message.role) {\n        case \"assistant\":\n            return new AIMessage(message.content || \"\", {\n                function_call: message.function_call,\n                tool_calls: message.tool_calls,\n            });\n        default:\n            return new ChatMessage(message.content || \"\", message.role ?? \"unknown\");\n    }\n}\nfunction _convertDeltaToMessageChunk(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ndelta, defaultRole) {\n    const role = delta.role ?? defaultRole;\n    const content = delta.content ?? \"\";\n    let additional_kwargs;\n    if (delta.function_call) {\n        additional_kwargs = {\n            function_call: delta.function_call,\n        };\n    }\n    else if (delta.tool_calls) {\n        additional_kwargs = {\n            tool_calls: delta.tool_calls,\n        };\n    }\n    else {\n        additional_kwargs = {};\n    }\n    if (role === \"user\") {\n        return new HumanMessageChunk({ content });\n    }\n    else if (role === \"assistant\") {\n        return new AIMessageChunk({ content, additional_kwargs });\n    }\n    else if (role === \"system\") {\n        return new SystemMessageChunk({ content });\n    }\n    else if (role === \"function\") {\n        return new FunctionMessageChunk({\n            content,\n            additional_kwargs,\n            name: delta.name,\n        });\n    }\n    else if (role === \"tool\") {\n        return new ToolMessageChunk({\n            content,\n            additional_kwargs,\n            tool_call_id: delta.tool_call_id,\n        });\n    }\n    else {\n        return new ChatMessageChunk({ content, role });\n    }\n}\nfunction convertMessagesToOpenAIParams(messages) {\n    // TODO: Function messages do not support array content, fix cast\n    return messages.map((message) => ({\n        role: messageToOpenAIRole(message),\n        content: message.content,\n        name: message.name,\n        function_call: message.additional_kwargs.function_call,\n        tool_calls: message.additional_kwargs.tool_calls,\n        tool_call_id: message.tool_call_id,\n    }));\n}\n/**\n * Wrapper around OpenAI large language models that use the Chat endpoint.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure you should have the `openai` package installed, with the\n * `AZURE_OPENAI_API_KEY`,\n * `AZURE_OPENAI_API_INSTANCE_NAME`,\n * `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n * and `AZURE_OPENAI_API_VERSION` environment variable set.\n * `AZURE_OPENAI_BASE_PATH` is optional and will override `AZURE_OPENAI_API_INSTANCE_NAME` if you need to use a custom endpoint.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/chat/create |\n * `openai.createChatCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n * @example\n * ```typescript\n * // Create a new instance of ChatOpenAI with specific temperature and model name settings\n * const model = new ChatOpenAI({\n *   temperature: 0.9,\n *   modelName: \"ft:gpt-3.5-turbo-0613:{ORG_NAME}::{MODEL_ID}\",\n * });\n *\n * // Invoke the model with a message and await the response\n * const message = await model.invoke(\"Hi there!\");\n *\n * // Log the response to the console\n * console.log(message);\n *\n * ```\n */\nexport class ChatOpenAI extends BaseChatModel {\n    static lc_name() {\n        return \"ChatOpenAI\";\n    }\n    get callKeys() {\n        return [\n            ...super.callKeys,\n            \"options\",\n            \"function_call\",\n            \"functions\",\n            \"tools\",\n            \"tool_choice\",\n            \"promptIndex\",\n            \"response_format\",\n            \"seed\",\n        ];\n    }\n    get lc_secrets() {\n        return {\n            openAIApiKey: \"OPENAI_API_KEY\",\n            azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n            organization: \"OPENAI_ORGANIZATION\",\n        };\n    }\n    get lc_aliases() {\n        return {\n            modelName: \"model\",\n            openAIApiKey: \"openai_api_key\",\n            azureOpenAIApiVersion: \"azure_openai_api_version\",\n            azureOpenAIApiKey: \"azure_openai_api_key\",\n            azureOpenAIApiInstanceName: \"azure_openai_api_instance_name\",\n            azureOpenAIApiDeploymentName: \"azure_openai_api_deployment_name\",\n        };\n    }\n    constructor(fields, \n    /** @deprecated */\n    configuration) {\n        super(fields ?? {});\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"frequencyPenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"presencePenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"n\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"logitBias\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"gpt-3.5-turbo\"\n        });\n        Object.defineProperty(this, \"modelKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stop\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"user\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"timeout\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"logprobs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"topLogprobs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"openAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiVersion\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiInstanceName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiDeploymentName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIBasePath\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"organization\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"clientConfig\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.openAIApiKey =\n            fields?.openAIApiKey ?? getEnvironmentVariable(\"OPENAI_API_KEY\");\n        this.azureOpenAIApiKey =\n            fields?.azureOpenAIApiKey ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n        if (!this.azureOpenAIApiKey && !this.openAIApiKey) {\n            throw new Error(\"OpenAI or Azure OpenAI API key not found\");\n        }\n        this.azureOpenAIApiInstanceName =\n            fields?.azureOpenAIApiInstanceName ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n        this.azureOpenAIApiDeploymentName =\n            fields?.azureOpenAIApiDeploymentName ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\");\n        this.azureOpenAIApiVersion =\n            fields?.azureOpenAIApiVersion ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n        this.azureOpenAIBasePath =\n            fields?.azureOpenAIBasePath ??\n                getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n        this.organization =\n            fields?.configuration?.organization ??\n                getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n        this.modelName = fields?.modelName ?? this.modelName;\n        this.modelKwargs = fields?.modelKwargs ?? {};\n        this.timeout = fields?.timeout;\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.topP = fields?.topP ?? this.topP;\n        this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n        this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n        this.maxTokens = fields?.maxTokens;\n        this.logprobs = fields?.logprobs;\n        this.topLogprobs = fields?.topLogprobs;\n        this.n = fields?.n ?? this.n;\n        this.logitBias = fields?.logitBias;\n        this.stop = fields?.stop;\n        this.user = fields?.user;\n        this.streaming = fields?.streaming ?? false;\n        if (this.azureOpenAIApiKey) {\n            if (!this.azureOpenAIApiInstanceName && !this.azureOpenAIBasePath) {\n                throw new Error(\"Azure OpenAI API instance name not found\");\n            }\n            if (!this.azureOpenAIApiDeploymentName) {\n                throw new Error(\"Azure OpenAI API deployment name not found\");\n            }\n            if (!this.azureOpenAIApiVersion) {\n                throw new Error(\"Azure OpenAI API version not found\");\n            }\n            this.openAIApiKey = this.openAIApiKey ?? \"\";\n        }\n        this.clientConfig = {\n            apiKey: this.openAIApiKey,\n            organization: this.organization,\n            baseURL: configuration?.basePath ?? fields?.configuration?.basePath,\n            dangerouslyAllowBrowser: true,\n            defaultHeaders: configuration?.baseOptions?.headers ??\n                fields?.configuration?.baseOptions?.headers,\n            defaultQuery: configuration?.baseOptions?.params ??\n                fields?.configuration?.baseOptions?.params,\n            ...configuration,\n            ...fields?.configuration,\n        };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams(options) {\n        function isStructuredToolArray(tools) {\n            return (tools !== undefined &&\n                tools.every((tool) => Array.isArray(tool.lc_namespace)));\n        }\n        const params = {\n            model: this.modelName,\n            temperature: this.temperature,\n            top_p: this.topP,\n            frequency_penalty: this.frequencyPenalty,\n            presence_penalty: this.presencePenalty,\n            max_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n            logprobs: this.logprobs,\n            top_logprobs: this.topLogprobs,\n            n: this.n,\n            logit_bias: this.logitBias,\n            stop: options?.stop ?? this.stop,\n            user: this.user,\n            stream: this.streaming,\n            functions: options?.functions,\n            function_call: options?.function_call,\n            tools: isStructuredToolArray(options?.tools)\n                ? options?.tools.map(convertToOpenAITool)\n                : options?.tools,\n            tool_choice: options?.tool_choice,\n            response_format: options?.response_format,\n            seed: options?.seed,\n            ...this.modelKwargs,\n        };\n        return params;\n    }\n    /** @ignore */\n    _identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    async *_streamResponseChunks(messages, options, runManager) {\n        const messagesMapped = convertMessagesToOpenAIParams(messages);\n        const params = {\n            ...this.invocationParams(options),\n            messages: messagesMapped,\n            stream: true,\n        };\n        let defaultRole;\n        const streamIterable = await this.completionWithRetry(params, options);\n        for await (const data of streamIterable) {\n            const choice = data?.choices[0];\n            if (!choice) {\n                continue;\n            }\n            const { delta } = choice;\n            if (!delta) {\n                continue;\n            }\n            const chunk = _convertDeltaToMessageChunk(delta, defaultRole);\n            defaultRole = delta.role ?? defaultRole;\n            const newTokenIndices = {\n                prompt: options.promptIndex ?? 0,\n                completion: choice.index ?? 0,\n            };\n            if (typeof chunk.content !== \"string\") {\n                console.log(\"[WARNING]: Received non-string content from OpenAI. This is currently not supported.\");\n                continue;\n            }\n            const generationChunk = new ChatGenerationChunk({\n                message: chunk,\n                text: chunk.content,\n                generationInfo: newTokenIndices,\n            });\n            yield generationChunk;\n            // eslint-disable-next-line no-void\n            void runManager?.handleLLMNewToken(generationChunk.text ?? \"\", newTokenIndices, undefined, undefined, undefined, { chunk: generationChunk });\n        }\n        if (options.signal?.aborted) {\n            throw new Error(\"AbortError\");\n        }\n    }\n    /**\n     * Get the identifying parameters for the model\n     *\n     */\n    identifyingParams() {\n        return this._identifyingParams();\n    }\n    /** @ignore */\n    async _generate(messages, options, runManager) {\n        const tokenUsage = {};\n        const params = this.invocationParams(options);\n        const messagesMapped = convertMessagesToOpenAIParams(messages);\n        if (params.stream) {\n            const stream = this._streamResponseChunks(messages, options, runManager);\n            const finalChunks = {};\n            for await (const chunk of stream) {\n                const index = chunk.generationInfo?.completion ?? 0;\n                if (finalChunks[index] === undefined) {\n                    finalChunks[index] = chunk;\n                }\n                else {\n                    finalChunks[index] = finalChunks[index].concat(chunk);\n                }\n            }\n            const generations = Object.entries(finalChunks)\n                .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))\n                .map(([_, value]) => value);\n            const { functions, function_call } = this.invocationParams(options);\n            // OpenAI does not support token usage report under stream mode,\n            // fallback to estimation.\n            const promptTokenUsage = await this.getEstimatedTokenCountFromPrompt(messages, functions, function_call);\n            const completionTokenUsage = await this.getNumTokensFromGenerations(generations);\n            tokenUsage.promptTokens = promptTokenUsage;\n            tokenUsage.completionTokens = completionTokenUsage;\n            tokenUsage.totalTokens = promptTokenUsage + completionTokenUsage;\n            return { generations, llmOutput: { estimatedTokenUsage: tokenUsage } };\n        }\n        else {\n            const data = await this.completionWithRetry({\n                ...params,\n                stream: false,\n                messages: messagesMapped,\n            }, {\n                signal: options?.signal,\n                ...options?.options,\n            });\n            const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = data?.usage ?? {};\n            if (completionTokens) {\n                tokenUsage.completionTokens =\n                    (tokenUsage.completionTokens ?? 0) + completionTokens;\n            }\n            if (promptTokens) {\n                tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n            }\n            if (totalTokens) {\n                tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n            }\n            const generations = [];\n            for (const part of data?.choices ?? []) {\n                const text = part.message?.content ?? \"\";\n                const generation = {\n                    text,\n                    message: openAIResponseToChatMessage(part.message ?? { role: \"assistant\" }),\n                };\n                generation.generationInfo = {\n                    ...(part.finish_reason ? { finish_reason: part.finish_reason } : {}),\n                    ...(part.logprobs ? { logprobs: part.logprobs } : {}),\n                };\n                generations.push(generation);\n            }\n            return {\n                generations,\n                llmOutput: { tokenUsage },\n            };\n        }\n    }\n    /**\n     * Estimate the number of tokens a prompt will use.\n     * Modified from: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts\n     */\n    async getEstimatedTokenCountFromPrompt(messages, functions, function_call) {\n        // It appears that if functions are present, the first system message is padded with a trailing newline. This\n        // was inferred by trying lots of combinations of messages and functions and seeing what the token counts were.\n        let tokens = (await this.getNumTokensFromMessages(messages)).totalCount;\n        // If there are functions, add the function definitions as they count towards token usage\n        if (functions && function_call !== \"auto\") {\n            const promptDefinitions = formatFunctionDefinitions(functions);\n            tokens += await this.getNumTokens(promptDefinitions);\n            tokens += 9; // Add nine per completion\n        }\n        // If there's a system message _and_ functions are present, subtract four tokens. I assume this is because\n        // functions typically add a system message, but reuse the first one if it's already there. This offsets\n        // the extra 9 tokens added by the function definitions.\n        if (functions && messages.find((m) => m._getType() === \"system\")) {\n            tokens -= 4;\n        }\n        // If function_call is 'none', add one token.\n        // If it's a FunctionCall object, add 4 + the number of tokens in the function name.\n        // If it's undefined or 'auto', don't add anything.\n        if (function_call === \"none\") {\n            tokens += 1;\n        }\n        else if (typeof function_call === \"object\") {\n            tokens += (await this.getNumTokens(function_call.name)) + 4;\n        }\n        return tokens;\n    }\n    /**\n     * Estimate the number of tokens an array of generations have used.\n     */\n    async getNumTokensFromGenerations(generations) {\n        const generationUsages = await Promise.all(generations.map(async (generation) => {\n            if (generation.message.additional_kwargs?.function_call) {\n                return (await this.getNumTokensFromMessages([generation.message]))\n                    .countPerMessage[0];\n            }\n            else {\n                return await this.getNumTokens(generation.message.content);\n            }\n        }));\n        return generationUsages.reduce((a, b) => a + b, 0);\n    }\n    async getNumTokensFromMessages(messages) {\n        let totalCount = 0;\n        let tokensPerMessage = 0;\n        let tokensPerName = 0;\n        // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n        if (this.modelName === \"gpt-3.5-turbo-0301\") {\n            tokensPerMessage = 4;\n            tokensPerName = -1;\n        }\n        else {\n            tokensPerMessage = 3;\n            tokensPerName = 1;\n        }\n        const countPerMessage = await Promise.all(messages.map(async (message) => {\n            const textCount = await this.getNumTokens(message.content);\n            const roleCount = await this.getNumTokens(messageToOpenAIRole(message));\n            const nameCount = message.name !== undefined\n                ? tokensPerName + (await this.getNumTokens(message.name))\n                : 0;\n            let count = textCount + tokensPerMessage + roleCount + nameCount;\n            // From: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts messageTokenEstimate\n            const openAIMessage = message;\n            if (openAIMessage._getType() === \"function\") {\n                count -= 2;\n            }\n            if (openAIMessage.additional_kwargs?.function_call) {\n                count += 3;\n            }\n            if (openAIMessage?.additional_kwargs.function_call?.name) {\n                count += await this.getNumTokens(openAIMessage.additional_kwargs.function_call?.name);\n            }\n            if (openAIMessage.additional_kwargs.function_call?.arguments) {\n                count += await this.getNumTokens(\n                // Remove newlines and spaces\n                JSON.stringify(JSON.parse(openAIMessage.additional_kwargs.function_call?.arguments)));\n            }\n            totalCount += count;\n            return count;\n        }));\n        totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n        return { totalCount, countPerMessage };\n    }\n    async completionWithRetry(request, options) {\n        const requestOptions = this._getClientOptions(options);\n        return this.caller.call(async () => {\n            try {\n                const res = await this.client.chat.completions.create(request, requestOptions);\n                return res;\n            }\n            catch (e) {\n                const error = wrapOpenAIClientError(e);\n                throw error;\n            }\n        });\n    }\n    _getClientOptions(options) {\n        if (!this.client) {\n            const openAIEndpointConfig = {\n                azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n                azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n                azureOpenAIApiKey: this.azureOpenAIApiKey,\n                azureOpenAIBasePath: this.azureOpenAIBasePath,\n                baseURL: this.clientConfig.baseURL,\n            };\n            const endpoint = getEndpoint(openAIEndpointConfig);\n            const params = {\n                ...this.clientConfig,\n                baseURL: endpoint,\n                timeout: this.timeout,\n                maxRetries: 0,\n            };\n            if (!params.baseURL) {\n                delete params.baseURL;\n            }\n            this.client = new OpenAIClient(params);\n        }\n        const requestOptions = {\n            ...this.clientConfig,\n            ...options,\n        };\n        if (this.azureOpenAIApiKey) {\n            requestOptions.headers = {\n                \"api-key\": this.azureOpenAIApiKey,\n                ...requestOptions.headers,\n            };\n            requestOptions.query = {\n                \"api-version\": this.azureOpenAIApiVersion,\n                ...requestOptions.query,\n            };\n        }\n        return requestOptions;\n    }\n    _llmType() {\n        return \"openai\";\n    }\n    /** @ignore */\n    _combineLLMOutput(...llmOutputs) {\n        return llmOutputs.reduce((acc, llmOutput) => {\n            if (llmOutput && llmOutput.tokenUsage) {\n                acc.tokenUsage.completionTokens +=\n                    llmOutput.tokenUsage.completionTokens ?? 0;\n                acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n                acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n            }\n            return acc;\n        }, {\n            tokenUsage: {\n                completionTokens: 0,\n                promptTokens: 0,\n                totalTokens: 0,\n            },\n        });\n    }\n}\n"],"mappings":"AAAA,SAASA,MAAM,IAAIC,YAAY,QAAQ,QAAQ;AAC/C,SAASC,SAAS,EAAEC,cAAc,EAAEC,WAAW,EAAEC,gBAAgB,EAAEC,oBAAoB,EAAEC,iBAAiB,EAAEC,kBAAkB,EAAEC,gBAAgB,QAAS,0BAA0B;AACnL,SAASC,mBAAmB,QAAS,yBAAyB;AAC9D,SAASC,sBAAsB,QAAQ,2BAA2B;AAClE,SAASC,aAAa,QAAS,6CAA6C;AAC5E,SAASC,mBAAmB,QAAQ,wCAAwC;AAC5E,SAASC,WAAW,QAAQ,kBAAkB;AAC9C,SAASC,qBAAqB,QAAQ,mBAAmB;AACzD,SAASC,yBAAyB,QAAS,gCAAgC;AAC3E,SAASC,+BAA+BA,CAACC,OAAO,EAAE;EAC9C,IAAIA,OAAO,CAACC,IAAI,KAAK,QAAQ,IACzBD,OAAO,CAACC,IAAI,KAAK,WAAW,IAC5BD,OAAO,CAACC,IAAI,KAAK,MAAM,IACvBD,OAAO,CAACC,IAAI,KAAK,UAAU,IAC3BD,OAAO,CAACC,IAAI,KAAK,MAAM,EAAE;IACzBC,OAAO,CAACC,IAAI,0BAAAC,MAAA,CAA0BJ,OAAO,CAACC,IAAI,CAAE,CAAC;EACzD;EACA,OAAOD,OAAO,CAACC,IAAI;AACvB;AACA,OAAO,SAASI,mBAAmBA,CAACL,OAAO,EAAE;EACzC,MAAMM,IAAI,GAAGN,OAAO,CAACO,QAAQ,CAAC,CAAC;EAC/B,QAAQD,IAAI;IACR,KAAK,QAAQ;MACT,OAAO,QAAQ;IACnB,KAAK,IAAI;MACL,OAAO,WAAW;IACtB,KAAK,OAAO;MACR,OAAO,MAAM;IACjB,KAAK,UAAU;MACX,OAAO,UAAU;IACrB,KAAK,MAAM;MACP,OAAO,MAAM;IACjB,KAAK,SAAS;MAAE;QACZ,IAAI,CAACpB,WAAW,CAACsB,UAAU,CAACR,OAAO,CAAC,EAChC,MAAM,IAAIS,KAAK,CAAC,8BAA8B,CAAC;QACnD,OAAOV,+BAA+B,CAACC,OAAO,CAAC;MACnD;IACA;MACI,MAAM,IAAIS,KAAK,0BAAAL,MAAA,CAA0BE,IAAI,CAAE,CAAC;EACxD;AACJ;AACA,SAASI,2BAA2BA,CAACV,OAAO,EAAE;EAAA,IAAAW,aAAA;EAC1C,QAAQX,OAAO,CAACC,IAAI;IAChB,KAAK,WAAW;MACZ,OAAO,IAAIjB,SAAS,CAACgB,OAAO,CAACY,OAAO,IAAI,EAAE,EAAE;QACxCC,aAAa,EAAEb,OAAO,CAACa,aAAa;QACpCC,UAAU,EAAEd,OAAO,CAACc;MACxB,CAAC,CAAC;IACN;MACI,OAAO,IAAI5B,WAAW,CAACc,OAAO,CAACY,OAAO,IAAI,EAAE,GAAAD,aAAA,GAAEX,OAAO,CAACC,IAAI,cAAAU,aAAA,cAAAA,aAAA,GAAI,SAAS,CAAC;EAChF;AACJ;AACA,SAASI,2BAA2BA;AACpC;AACAC,KAAK,EAAEC,WAAW,EAAE;EAAA,IAAAC,WAAA,EAAAC,cAAA;EAChB,MAAMlB,IAAI,IAAAiB,WAAA,GAAGF,KAAK,CAACf,IAAI,cAAAiB,WAAA,cAAAA,WAAA,GAAID,WAAW;EACtC,MAAML,OAAO,IAAAO,cAAA,GAAGH,KAAK,CAACJ,OAAO,cAAAO,cAAA,cAAAA,cAAA,GAAI,EAAE;EACnC,IAAIC,iBAAiB;EACrB,IAAIJ,KAAK,CAACH,aAAa,EAAE;IACrBO,iBAAiB,GAAG;MAChBP,aAAa,EAAEG,KAAK,CAACH;IACzB,CAAC;EACL,CAAC,MACI,IAAIG,KAAK,CAACF,UAAU,EAAE;IACvBM,iBAAiB,GAAG;MAChBN,UAAU,EAAEE,KAAK,CAACF;IACtB,CAAC;EACL,CAAC,MACI;IACDM,iBAAiB,GAAG,CAAC,CAAC;EAC1B;EACA,IAAInB,IAAI,KAAK,MAAM,EAAE;IACjB,OAAO,IAAIZ,iBAAiB,CAAC;MAAEuB;IAAQ,CAAC,CAAC;EAC7C,CAAC,MACI,IAAIX,IAAI,KAAK,WAAW,EAAE;IAC3B,OAAO,IAAIhB,cAAc,CAAC;MAAE2B,OAAO;MAAEQ;IAAkB,CAAC,CAAC;EAC7D,CAAC,MACI,IAAInB,IAAI,KAAK,QAAQ,EAAE;IACxB,OAAO,IAAIX,kBAAkB,CAAC;MAAEsB;IAAQ,CAAC,CAAC;EAC9C,CAAC,MACI,IAAIX,IAAI,KAAK,UAAU,EAAE;IAC1B,OAAO,IAAIb,oBAAoB,CAAC;MAC5BwB,OAAO;MACPQ,iBAAiB;MACjBC,IAAI,EAAEL,KAAK,CAACK;IAChB,CAAC,CAAC;EACN,CAAC,MACI,IAAIpB,IAAI,KAAK,MAAM,EAAE;IACtB,OAAO,IAAIV,gBAAgB,CAAC;MACxBqB,OAAO;MACPQ,iBAAiB;MACjBE,YAAY,EAAEN,KAAK,CAACM;IACxB,CAAC,CAAC;EACN,CAAC,MACI;IACD,OAAO,IAAInC,gBAAgB,CAAC;MAAEyB,OAAO;MAAEX;IAAK,CAAC,CAAC;EAClD;AACJ;AACA,SAASsB,6BAA6BA,CAACC,QAAQ,EAAE;EAC7C;EACA,OAAOA,QAAQ,CAACC,GAAG,CAAEzB,OAAO,KAAM;IAC9BC,IAAI,EAAEI,mBAAmB,CAACL,OAAO,CAAC;IAClCY,OAAO,EAAEZ,OAAO,CAACY,OAAO;IACxBS,IAAI,EAAErB,OAAO,CAACqB,IAAI;IAClBR,aAAa,EAAEb,OAAO,CAACoB,iBAAiB,CAACP,aAAa;IACtDC,UAAU,EAAEd,OAAO,CAACoB,iBAAiB,CAACN,UAAU;IAChDQ,YAAY,EAAEtB,OAAO,CAACsB;EAC1B,CAAC,CAAC,CAAC;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMI,UAAU,SAAShC,aAAa,CAAC;EAC1C,OAAOiC,OAAOA,CAAA,EAAG;IACb,OAAO,YAAY;EACvB;EACA,IAAIC,QAAQA,CAAA,EAAG;IACX,OAAO,CACH,GAAG,KAAK,CAACA,QAAQ,EACjB,SAAS,EACT,eAAe,EACf,WAAW,EACX,OAAO,EACP,aAAa,EACb,aAAa,EACb,iBAAiB,EACjB,MAAM,CACT;EACL;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO;MACHC,YAAY,EAAE,gBAAgB;MAC9BC,iBAAiB,EAAE,sBAAsB;MACzCC,YAAY,EAAE;IAClB,CAAC;EACL;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO;MACHC,SAAS,EAAE,OAAO;MAClBJ,YAAY,EAAE,gBAAgB;MAC9BK,qBAAqB,EAAE,0BAA0B;MACjDJ,iBAAiB,EAAE,sBAAsB;MACzCK,0BAA0B,EAAE,gCAAgC;MAC5DC,4BAA4B,EAAE;IAClC,CAAC;EACL;EACAC,WAAWA,CAACC,MAAM,EAClB;EACAC,aAAa,EAAE;IAAA,IAAAC,oBAAA,EAAAC,qBAAA,EAAAC,sBAAA,EAAAC,sBAAA,EAAAC,sBAAA,EAAAC,qBAAA,EAAAC,qBAAA,EAAAC,sBAAA,EAAAC,iBAAA,EAAAC,mBAAA,EAAAC,mBAAA,EAAAC,YAAA,EAAAC,qBAAA,EAAAC,qBAAA,EAAAC,SAAA,EAAAC,iBAAA,EAAAC,qBAAA,EAAAC,sBAAA,EAAAC,qBAAA,EAAAC,sBAAA,EAAAC,sBAAA,EAAAC,sBAAA,EAAAC,sBAAA,EAAAC,sBAAA;IACX,KAAK,CAACzB,MAAM,aAANA,MAAM,cAANA,MAAM,GAAI,CAAC,CAAC,CAAC;IACnB0B,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,kBAAkB,EAAE;MAC5CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,GAAG,EAAE;MAC7BC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,SAAS,EAAE;MACnCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,UAAU,EAAE;MACpCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,uBAAuB,EAAE;MACjDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,mBAAmB,EAAE;MAC7CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,4BAA4B,EAAE;MACtDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,8BAA8B,EAAE;MACxDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,qBAAqB,EAAE;MAC/CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACxC,YAAY,IAAAW,oBAAA,GACbF,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAET,YAAY,cAAAW,oBAAA,cAAAA,oBAAA,GAAIhD,sBAAsB,CAAC,gBAAgB,CAAC;IACpE,IAAI,CAACsC,iBAAiB,IAAAW,qBAAA,GAClBH,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAER,iBAAiB,cAAAW,qBAAA,cAAAA,qBAAA,GACrBjD,sBAAsB,CAAC,sBAAsB,CAAC;IACtD,IAAI,CAAC,IAAI,CAACsC,iBAAiB,IAAI,CAAC,IAAI,CAACD,YAAY,EAAE;MAC/C,MAAM,IAAIrB,KAAK,CAAC,0CAA0C,CAAC;IAC/D;IACA,IAAI,CAAC2B,0BAA0B,IAAAO,sBAAA,GAC3BJ,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEH,0BAA0B,cAAAO,sBAAA,cAAAA,sBAAA,GAC9BlD,sBAAsB,CAAC,gCAAgC,CAAC;IAChE,IAAI,CAAC4C,4BAA4B,IAAAO,sBAAA,GAC7BL,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEF,4BAA4B,cAAAO,sBAAA,cAAAA,sBAAA,GAChCnD,sBAAsB,CAAC,kCAAkC,CAAC;IAClE,IAAI,CAAC0C,qBAAqB,IAAAU,sBAAA,GACtBN,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEJ,qBAAqB,cAAAU,sBAAA,cAAAA,sBAAA,GACzBpD,sBAAsB,CAAC,0BAA0B,CAAC;IAC1D,IAAI,CAAC8E,mBAAmB,IAAAzB,qBAAA,GACpBP,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEgC,mBAAmB,cAAAzB,qBAAA,cAAAA,qBAAA,GACvBrD,sBAAsB,CAAC,wBAAwB,CAAC;IACxD,IAAI,CAACuC,YAAY,IAAAe,qBAAA,GACbR,MAAM,aAANA,MAAM,gBAAAS,sBAAA,GAANT,MAAM,CAAEC,aAAa,cAAAQ,sBAAA,uBAArBA,sBAAA,CAAuBhB,YAAY,cAAAe,qBAAA,cAAAA,qBAAA,GAC/BtD,sBAAsB,CAAC,qBAAqB,CAAC;IACrD,IAAI,CAACyC,SAAS,IAAAe,iBAAA,GAAGV,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEL,SAAS,cAAAe,iBAAA,cAAAA,iBAAA,GAAI,IAAI,CAACf,SAAS;IACpD,IAAI,CAACsC,WAAW,IAAAtB,mBAAA,GAAGX,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEiC,WAAW,cAAAtB,mBAAA,cAAAA,mBAAA,GAAI,CAAC,CAAC;IAC5C,IAAI,CAACuB,OAAO,GAAGlC,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEkC,OAAO;IAC9B,IAAI,CAACC,WAAW,IAAAvB,mBAAA,GAAGZ,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEmC,WAAW,cAAAvB,mBAAA,cAAAA,mBAAA,GAAI,IAAI,CAACuB,WAAW;IAC1D,IAAI,CAACC,IAAI,IAAAvB,YAAA,GAAGb,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEoC,IAAI,cAAAvB,YAAA,cAAAA,YAAA,GAAI,IAAI,CAACuB,IAAI;IACrC,IAAI,CAACC,gBAAgB,IAAAvB,qBAAA,GAAGd,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEqC,gBAAgB,cAAAvB,qBAAA,cAAAA,qBAAA,GAAI,IAAI,CAACuB,gBAAgB;IACzE,IAAI,CAACC,eAAe,IAAAvB,qBAAA,GAAGf,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEsC,eAAe,cAAAvB,qBAAA,cAAAA,qBAAA,GAAI,IAAI,CAACuB,eAAe;IACtE,IAAI,CAACC,SAAS,GAAGvC,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEuC,SAAS;IAClC,IAAI,CAACC,QAAQ,GAAGxC,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEwC,QAAQ;IAChC,IAAI,CAACC,WAAW,GAAGzC,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEyC,WAAW;IACtC,IAAI,CAACC,CAAC,IAAA1B,SAAA,GAAGhB,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAE0C,CAAC,cAAA1B,SAAA,cAAAA,SAAA,GAAI,IAAI,CAAC0B,CAAC;IAC5B,IAAI,CAACC,SAAS,GAAG3C,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAE2C,SAAS;IAClC,IAAI,CAACC,IAAI,GAAG5C,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAE4C,IAAI;IACxB,IAAI,CAACC,IAAI,GAAG7C,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAE6C,IAAI;IACxB,IAAI,CAACC,SAAS,IAAA7B,iBAAA,GAAGjB,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAE8C,SAAS,cAAA7B,iBAAA,cAAAA,iBAAA,GAAI,KAAK;IAC3C,IAAI,IAAI,CAACzB,iBAAiB,EAAE;MAAA,IAAAuD,kBAAA;MACxB,IAAI,CAAC,IAAI,CAAClD,0BAA0B,IAAI,CAAC,IAAI,CAACmC,mBAAmB,EAAE;QAC/D,MAAM,IAAI9D,KAAK,CAAC,0CAA0C,CAAC;MAC/D;MACA,IAAI,CAAC,IAAI,CAAC4B,4BAA4B,EAAE;QACpC,MAAM,IAAI5B,KAAK,CAAC,4CAA4C,CAAC;MACjE;MACA,IAAI,CAAC,IAAI,CAAC0B,qBAAqB,EAAE;QAC7B,MAAM,IAAI1B,KAAK,CAAC,oCAAoC,CAAC;MACzD;MACA,IAAI,CAACqB,YAAY,IAAAwD,kBAAA,GAAG,IAAI,CAACxD,YAAY,cAAAwD,kBAAA,cAAAA,kBAAA,GAAI,EAAE;IAC/C;IACA,IAAI,CAACC,YAAY,GAAG;MAChBC,MAAM,EAAE,IAAI,CAAC1D,YAAY;MACzBE,YAAY,EAAE,IAAI,CAACA,YAAY;MAC/ByD,OAAO,GAAAhC,qBAAA,GAAEjB,aAAa,aAAbA,aAAa,uBAAbA,aAAa,CAAEkD,QAAQ,cAAAjC,qBAAA,cAAAA,qBAAA,GAAIlB,MAAM,aAANA,MAAM,gBAAAmB,sBAAA,GAANnB,MAAM,CAAEC,aAAa,cAAAkB,sBAAA,uBAArBA,sBAAA,CAAuBgC,QAAQ;MACnEC,uBAAuB,EAAE,IAAI;MAC7BC,cAAc,GAAAjC,qBAAA,GAAEnB,aAAa,aAAbA,aAAa,gBAAAoB,sBAAA,GAAbpB,aAAa,CAAEqD,WAAW,cAAAjC,sBAAA,uBAA1BA,sBAAA,CAA4BkC,OAAO,cAAAnC,qBAAA,cAAAA,qBAAA,GAC/CpB,MAAM,aAANA,MAAM,gBAAAsB,sBAAA,GAANtB,MAAM,CAAEC,aAAa,cAAAqB,sBAAA,gBAAAA,sBAAA,GAArBA,sBAAA,CAAuBgC,WAAW,cAAAhC,sBAAA,uBAAlCA,sBAAA,CAAoCiC,OAAO;MAC/CC,YAAY,GAAAjC,sBAAA,GAAEtB,aAAa,aAAbA,aAAa,gBAAAuB,sBAAA,GAAbvB,aAAa,CAAEqD,WAAW,cAAA9B,sBAAA,uBAA1BA,sBAAA,CAA4BiC,MAAM,cAAAlC,sBAAA,cAAAA,sBAAA,GAC5CvB,MAAM,aAANA,MAAM,gBAAAyB,sBAAA,GAANzB,MAAM,CAAEC,aAAa,cAAAwB,sBAAA,gBAAAA,sBAAA,GAArBA,sBAAA,CAAuB6B,WAAW,cAAA7B,sBAAA,uBAAlCA,sBAAA,CAAoCgC,MAAM;MAC9C,GAAGxD,aAAa;MAChB,IAAGD,MAAM,aAANA,MAAM,uBAANA,MAAM,CAAEC,aAAa;IAC5B,CAAC;EACL;EACA;AACJ;AACA;EACIyD,gBAAgBA,CAACC,OAAO,EAAE;IAAA,IAAAC,aAAA;IACtB,SAASC,qBAAqBA,CAACC,KAAK,EAAE;MAClC,OAAQA,KAAK,KAAKC,SAAS,IACvBD,KAAK,CAACE,KAAK,CAAEC,IAAI,IAAKC,KAAK,CAACC,OAAO,CAACF,IAAI,CAACG,YAAY,CAAC,CAAC;IAC/D;IACA,MAAMX,MAAM,GAAG;MACXY,KAAK,EAAE,IAAI,CAAC1E,SAAS;MACrBwC,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BmC,KAAK,EAAE,IAAI,CAAClC,IAAI;MAChBmC,iBAAiB,EAAE,IAAI,CAAClC,gBAAgB;MACxCmC,gBAAgB,EAAE,IAAI,CAAClC,eAAe;MACtCmC,UAAU,EAAE,IAAI,CAAClC,SAAS,KAAK,CAAC,CAAC,GAAGwB,SAAS,GAAG,IAAI,CAACxB,SAAS;MAC9DC,QAAQ,EAAE,IAAI,CAACA,QAAQ;MACvBkC,YAAY,EAAE,IAAI,CAACjC,WAAW;MAC9BC,CAAC,EAAE,IAAI,CAACA,CAAC;MACTiC,UAAU,EAAE,IAAI,CAAChC,SAAS;MAC1BC,IAAI,GAAAgB,aAAA,GAAED,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEf,IAAI,cAAAgB,aAAA,cAAAA,aAAA,GAAI,IAAI,CAAChB,IAAI;MAChCC,IAAI,EAAE,IAAI,CAACA,IAAI;MACf+B,MAAM,EAAE,IAAI,CAAC9B,SAAS;MACtB+B,SAAS,EAAElB,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEkB,SAAS;MAC7BvG,aAAa,EAAEqF,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAErF,aAAa;MACrCwF,KAAK,EAAED,qBAAqB,CAACF,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEG,KAAK,CAAC,GACtCH,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEG,KAAK,CAAC5E,GAAG,CAAC9B,mBAAmB,CAAC,GACvCuG,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEG,KAAK;MACpBgB,WAAW,EAAEnB,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEmB,WAAW;MACjCC,eAAe,EAAEpB,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEoB,eAAe;MACzCC,IAAI,EAAErB,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEqB,IAAI;MACnB,GAAG,IAAI,CAAC/C;IACZ,CAAC;IACD,OAAOwB,MAAM;EACjB;EACA;EACAwB,kBAAkBA,CAAA,EAAG;IACjB,OAAO;MACHC,UAAU,EAAE,IAAI,CAACvF,SAAS;MAC1B,GAAG,IAAI,CAAC+D,gBAAgB,CAAC,CAAC;MAC1B,GAAG,IAAI,CAACV;IACZ,CAAC;EACL;EACA,OAAOmC,qBAAqBA,CAAClG,QAAQ,EAAE0E,OAAO,EAAEyB,UAAU,EAAE;IAAA,IAAAC,eAAA;IACxD,MAAMC,cAAc,GAAGtG,6BAA6B,CAACC,QAAQ,CAAC;IAC9D,MAAMwE,MAAM,GAAG;MACX,GAAG,IAAI,CAACC,gBAAgB,CAACC,OAAO,CAAC;MACjC1E,QAAQ,EAAEqG,cAAc;MACxBV,MAAM,EAAE;IACZ,CAAC;IACD,IAAIlG,WAAW;IACf,MAAM6G,cAAc,GAAG,MAAM,IAAI,CAACC,mBAAmB,CAAC/B,MAAM,EAAEE,OAAO,CAAC;IACtE,WAAW,MAAM8B,IAAI,IAAIF,cAAc,EAAE;MAAA,IAAAG,YAAA,EAAAC,oBAAA,EAAAC,aAAA,EAAAC,qBAAA;MACrC,MAAMC,MAAM,GAAGL,IAAI,aAAJA,IAAI,uBAAJA,IAAI,CAAEM,OAAO,CAAC,CAAC,CAAC;MAC/B,IAAI,CAACD,MAAM,EAAE;QACT;MACJ;MACA,MAAM;QAAErH;MAAM,CAAC,GAAGqH,MAAM;MACxB,IAAI,CAACrH,KAAK,EAAE;QACR;MACJ;MACA,MAAMuH,KAAK,GAAGxH,2BAA2B,CAACC,KAAK,EAAEC,WAAW,CAAC;MAC7DA,WAAW,IAAAgH,YAAA,GAAGjH,KAAK,CAACf,IAAI,cAAAgI,YAAA,cAAAA,YAAA,GAAIhH,WAAW;MACvC,MAAMuH,eAAe,GAAG;QACpBC,MAAM,GAAAP,oBAAA,GAAEhC,OAAO,CAACwC,WAAW,cAAAR,oBAAA,cAAAA,oBAAA,GAAI,CAAC;QAChCS,UAAU,GAAAR,aAAA,GAAEE,MAAM,CAACO,KAAK,cAAAT,aAAA,cAAAA,aAAA,GAAI;MAChC,CAAC;MACD,IAAI,OAAOI,KAAK,CAAC3H,OAAO,KAAK,QAAQ,EAAE;QACnCV,OAAO,CAAC2I,GAAG,CAAC,sFAAsF,CAAC;QACnG;MACJ;MACA,MAAMC,eAAe,GAAG,IAAItJ,mBAAmB,CAAC;QAC5CQ,OAAO,EAAEuI,KAAK;QACdQ,IAAI,EAAER,KAAK,CAAC3H,OAAO;QACnBoI,cAAc,EAAER;MACpB,CAAC,CAAC;MACF,MAAMM,eAAe;MACrB;MACA,MAAKnB,UAAU,aAAVA,UAAU,uBAAVA,UAAU,CAAEsB,iBAAiB,EAAAb,qBAAA,GAACU,eAAe,CAACC,IAAI,cAAAX,qBAAA,cAAAA,qBAAA,GAAI,EAAE,EAAEI,eAAe,EAAElC,SAAS,EAAEA,SAAS,EAAEA,SAAS,EAAE;QAAEiC,KAAK,EAAEO;MAAgB,CAAC,CAAC;IAChJ;IACA,KAAAlB,eAAA,GAAI1B,OAAO,CAACgD,MAAM,cAAAtB,eAAA,eAAdA,eAAA,CAAgBuB,OAAO,EAAE;MACzB,MAAM,IAAI1I,KAAK,CAAC,YAAY,CAAC;IACjC;EACJ;EACA;AACJ;AACA;AACA;EACI2I,iBAAiBA,CAAA,EAAG;IAChB,OAAO,IAAI,CAAC5B,kBAAkB,CAAC,CAAC;EACpC;EACA;EACA,MAAM6B,SAASA,CAAC7H,QAAQ,EAAE0E,OAAO,EAAEyB,UAAU,EAAE;IAC3C,MAAM2B,UAAU,GAAG,CAAC,CAAC;IACrB,MAAMtD,MAAM,GAAG,IAAI,CAACC,gBAAgB,CAACC,OAAO,CAAC;IAC7C,MAAM2B,cAAc,GAAGtG,6BAA6B,CAACC,QAAQ,CAAC;IAC9D,IAAIwE,MAAM,CAACmB,MAAM,EAAE;MACf,MAAMA,MAAM,GAAG,IAAI,CAACO,qBAAqB,CAAClG,QAAQ,EAAE0E,OAAO,EAAEyB,UAAU,CAAC;MACxE,MAAM4B,WAAW,GAAG,CAAC,CAAC;MACtB,WAAW,MAAMhB,KAAK,IAAIpB,MAAM,EAAE;QAAA,IAAAqC,qBAAA,EAAAC,sBAAA;QAC9B,MAAMb,KAAK,IAAAY,qBAAA,IAAAC,sBAAA,GAAGlB,KAAK,CAACS,cAAc,cAAAS,sBAAA,uBAApBA,sBAAA,CAAsBd,UAAU,cAAAa,qBAAA,cAAAA,qBAAA,GAAI,CAAC;QACnD,IAAID,WAAW,CAACX,KAAK,CAAC,KAAKtC,SAAS,EAAE;UAClCiD,WAAW,CAACX,KAAK,CAAC,GAAGL,KAAK;QAC9B,CAAC,MACI;UACDgB,WAAW,CAACX,KAAK,CAAC,GAAGW,WAAW,CAACX,KAAK,CAAC,CAACxI,MAAM,CAACmI,KAAK,CAAC;QACzD;MACJ;MACA,MAAMmB,WAAW,GAAGzF,MAAM,CAAC0F,OAAO,CAACJ,WAAW,CAAC,CAC1CK,IAAI,CAAC,CAAAC,IAAA,EAAAC,KAAA;QAAA,IAAC,CAACC,IAAI,CAAC,GAAAF,IAAA;QAAA,IAAE,CAACG,IAAI,CAAC,GAAAF,KAAA;QAAA,OAAKG,QAAQ,CAACF,IAAI,EAAE,EAAE,CAAC,GAAGE,QAAQ,CAACD,IAAI,EAAE,EAAE,CAAC;MAAA,EAAC,CACjEvI,GAAG,CAACyI,KAAA;QAAA,IAAC,CAACC,CAAC,EAAE7F,KAAK,CAAC,GAAA4F,KAAA;QAAA,OAAK5F,KAAK;MAAA,EAAC;MAC/B,MAAM;QAAE8C,SAAS;QAAEvG;MAAc,CAAC,GAAG,IAAI,CAACoF,gBAAgB,CAACC,OAAO,CAAC;MACnE;MACA;MACA,MAAMkE,gBAAgB,GAAG,MAAM,IAAI,CAACC,gCAAgC,CAAC7I,QAAQ,EAAE4F,SAAS,EAAEvG,aAAa,CAAC;MACxG,MAAMyJ,oBAAoB,GAAG,MAAM,IAAI,CAACC,2BAA2B,CAACb,WAAW,CAAC;MAChFJ,UAAU,CAACkB,YAAY,GAAGJ,gBAAgB;MAC1Cd,UAAU,CAACmB,gBAAgB,GAAGH,oBAAoB;MAClDhB,UAAU,CAACoB,WAAW,GAAGN,gBAAgB,GAAGE,oBAAoB;MAChE,OAAO;QAAEZ,WAAW;QAAEiB,SAAS,EAAE;UAAEC,mBAAmB,EAAEtB;QAAW;MAAE,CAAC;IAC1E,CAAC,MACI;MAAA,IAAAuB,WAAA;MACD,MAAM7C,IAAI,GAAG,MAAM,IAAI,CAACD,mBAAmB,CAAC;QACxC,GAAG/B,MAAM;QACTmB,MAAM,EAAE,KAAK;QACb3F,QAAQ,EAAEqG;MACd,CAAC,EAAE;QACCqB,MAAM,EAAEhD,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEgD,MAAM;QACvB,IAAGhD,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEA,OAAO;MACvB,CAAC,CAAC;MACF,MAAM;QAAE4E,iBAAiB,EAAEL,gBAAgB;QAAEM,aAAa,EAAEP,YAAY;QAAEQ,YAAY,EAAEN;MAAa,CAAC,IAAAG,WAAA,GAAG7C,IAAI,aAAJA,IAAI,uBAAJA,IAAI,CAAEiD,KAAK,cAAAJ,WAAA,cAAAA,WAAA,GAAI,CAAC,CAAC;MAC1H,IAAIJ,gBAAgB,EAAE;QAAA,IAAAS,qBAAA;QAClB5B,UAAU,CAACmB,gBAAgB,GACvB,EAAAS,qBAAA,GAAC5B,UAAU,CAACmB,gBAAgB,cAAAS,qBAAA,cAAAA,qBAAA,GAAI,CAAC,IAAIT,gBAAgB;MAC7D;MACA,IAAID,YAAY,EAAE;QAAA,IAAAW,qBAAA;QACd7B,UAAU,CAACkB,YAAY,GAAG,EAAAW,qBAAA,GAAC7B,UAAU,CAACkB,YAAY,cAAAW,qBAAA,cAAAA,qBAAA,GAAI,CAAC,IAAIX,YAAY;MAC3E;MACA,IAAIE,WAAW,EAAE;QAAA,IAAAU,qBAAA;QACb9B,UAAU,CAACoB,WAAW,GAAG,EAAAU,qBAAA,GAAC9B,UAAU,CAACoB,WAAW,cAAAU,qBAAA,cAAAA,qBAAA,GAAI,CAAC,IAAIV,WAAW;MACxE;MACA,MAAMhB,WAAW,GAAG,EAAE;MACtB,KAAK,MAAM2B,IAAI,KAAAC,aAAA,GAAItD,IAAI,aAAJA,IAAI,uBAAJA,IAAI,CAAEM,OAAO,cAAAgD,aAAA,cAAAA,aAAA,GAAI,EAAE,EAAE;QAAA,IAAAA,aAAA,EAAAC,qBAAA,EAAAC,aAAA,EAAAC,cAAA;QACpC,MAAM1C,IAAI,IAAAwC,qBAAA,IAAAC,aAAA,GAAGH,IAAI,CAACrL,OAAO,cAAAwL,aAAA,uBAAZA,aAAA,CAAc5K,OAAO,cAAA2K,qBAAA,cAAAA,qBAAA,GAAI,EAAE;QACxC,MAAMG,UAAU,GAAG;UACf3C,IAAI;UACJ/I,OAAO,EAAEU,2BAA2B,EAAA+K,cAAA,GAACJ,IAAI,CAACrL,OAAO,cAAAyL,cAAA,cAAAA,cAAA,GAAI;YAAExL,IAAI,EAAE;UAAY,CAAC;QAC9E,CAAC;QACDyL,UAAU,CAAC1C,cAAc,GAAG;UACxB,IAAIqC,IAAI,CAACM,aAAa,GAAG;YAAEA,aAAa,EAAEN,IAAI,CAACM;UAAc,CAAC,GAAG,CAAC,CAAC,CAAC;UACpE,IAAIN,IAAI,CAACtG,QAAQ,GAAG;YAAEA,QAAQ,EAAEsG,IAAI,CAACtG;UAAS,CAAC,GAAG,CAAC,CAAC;QACxD,CAAC;QACD2E,WAAW,CAACkC,IAAI,CAACF,UAAU,CAAC;MAChC;MACA,OAAO;QACHhC,WAAW;QACXiB,SAAS,EAAE;UAAErB;QAAW;MAC5B,CAAC;IACL;EACJ;EACA;AACJ;AACA;AACA;EACI,MAAMe,gCAAgCA,CAAC7I,QAAQ,EAAE4F,SAAS,EAAEvG,aAAa,EAAE;IACvE;IACA;IACA,IAAIgL,MAAM,GAAG,CAAC,MAAM,IAAI,CAACC,wBAAwB,CAACtK,QAAQ,CAAC,EAAEuK,UAAU;IACvE;IACA,IAAI3E,SAAS,IAAIvG,aAAa,KAAK,MAAM,EAAE;MACvC,MAAMmL,iBAAiB,GAAGlM,yBAAyB,CAACsH,SAAS,CAAC;MAC9DyE,MAAM,IAAI,MAAM,IAAI,CAACI,YAAY,CAACD,iBAAiB,CAAC;MACpDH,MAAM,IAAI,CAAC,CAAC,CAAC;IACjB;IACA;IACA;IACA;IACA,IAAIzE,SAAS,IAAI5F,QAAQ,CAAC0K,IAAI,CAAEC,CAAC,IAAKA,CAAC,CAAC5L,QAAQ,CAAC,CAAC,KAAK,QAAQ,CAAC,EAAE;MAC9DsL,MAAM,IAAI,CAAC;IACf;IACA;IACA;IACA;IACA,IAAIhL,aAAa,KAAK,MAAM,EAAE;MAC1BgL,MAAM,IAAI,CAAC;IACf,CAAC,MACI,IAAI,OAAOhL,aAAa,KAAK,QAAQ,EAAE;MACxCgL,MAAM,IAAI,CAAC,MAAM,IAAI,CAACI,YAAY,CAACpL,aAAa,CAACQ,IAAI,CAAC,IAAI,CAAC;IAC/D;IACA,OAAOwK,MAAM;EACjB;EACA;AACJ;AACA;EACI,MAAMtB,2BAA2BA,CAACb,WAAW,EAAE;IAC3C,MAAM0C,gBAAgB,GAAG,MAAMC,OAAO,CAACC,GAAG,CAAC5C,WAAW,CAACjI,GAAG,CAAC,MAAOiK,UAAU,IAAK;MAAA,IAAAa,qBAAA;MAC7E,KAAAA,qBAAA,GAAIb,UAAU,CAAC1L,OAAO,CAACoB,iBAAiB,cAAAmL,qBAAA,eAApCA,qBAAA,CAAsC1L,aAAa,EAAE;QACrD,OAAO,CAAC,MAAM,IAAI,CAACiL,wBAAwB,CAAC,CAACJ,UAAU,CAAC1L,OAAO,CAAC,CAAC,EAC5DwM,eAAe,CAAC,CAAC,CAAC;MAC3B,CAAC,MACI;QACD,OAAO,MAAM,IAAI,CAACP,YAAY,CAACP,UAAU,CAAC1L,OAAO,CAACY,OAAO,CAAC;MAC9D;IACJ,CAAC,CAAC,CAAC;IACH,OAAOwL,gBAAgB,CAACK,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,EAAE,CAAC,CAAC;EACtD;EACA,MAAMb,wBAAwBA,CAACtK,QAAQ,EAAE;IACrC,IAAIuK,UAAU,GAAG,CAAC;IAClB,IAAIa,gBAAgB,GAAG,CAAC;IACxB,IAAIC,aAAa,GAAG,CAAC;IACrB;IACA,IAAI,IAAI,CAAC3K,SAAS,KAAK,oBAAoB,EAAE;MACzC0K,gBAAgB,GAAG,CAAC;MACpBC,aAAa,GAAG,CAAC,CAAC;IACtB,CAAC,MACI;MACDD,gBAAgB,GAAG,CAAC;MACpBC,aAAa,GAAG,CAAC;IACrB;IACA,MAAML,eAAe,GAAG,MAAMH,OAAO,CAACC,GAAG,CAAC9K,QAAQ,CAACC,GAAG,CAAC,MAAOzB,OAAO,IAAK;MAAA,IAAA8M,qBAAA,EAAAC,sBAAA,EAAAC,sBAAA;MACtE,MAAMC,SAAS,GAAG,MAAM,IAAI,CAAChB,YAAY,CAACjM,OAAO,CAACY,OAAO,CAAC;MAC1D,MAAMsM,SAAS,GAAG,MAAM,IAAI,CAACjB,YAAY,CAAC5L,mBAAmB,CAACL,OAAO,CAAC,CAAC;MACvE,MAAMmN,SAAS,GAAGnN,OAAO,CAACqB,IAAI,KAAKiF,SAAS,GACtCuG,aAAa,IAAI,MAAM,IAAI,CAACZ,YAAY,CAACjM,OAAO,CAACqB,IAAI,CAAC,CAAC,GACvD,CAAC;MACP,IAAI+L,KAAK,GAAGH,SAAS,GAAGL,gBAAgB,GAAGM,SAAS,GAAGC,SAAS;MAChE;MACA,MAAME,aAAa,GAAGrN,OAAO;MAC7B,IAAIqN,aAAa,CAAC9M,QAAQ,CAAC,CAAC,KAAK,UAAU,EAAE;QACzC6M,KAAK,IAAI,CAAC;MACd;MACA,KAAAN,qBAAA,GAAIO,aAAa,CAACjM,iBAAiB,cAAA0L,qBAAA,eAA/BA,qBAAA,CAAiCjM,aAAa,EAAE;QAChDuM,KAAK,IAAI,CAAC;MACd;MACA,IAAIC,aAAa,aAAbA,aAAa,gBAAAN,sBAAA,GAAbM,aAAa,CAAEjM,iBAAiB,CAACP,aAAa,cAAAkM,sBAAA,eAA9CA,sBAAA,CAAgD1L,IAAI,EAAE;QAAA,IAAAiM,sBAAA;QACtDF,KAAK,IAAI,MAAM,IAAI,CAACnB,YAAY,EAAAqB,sBAAA,GAACD,aAAa,CAACjM,iBAAiB,CAACP,aAAa,cAAAyM,sBAAA,uBAA7CA,sBAAA,CAA+CjM,IAAI,CAAC;MACzF;MACA,KAAA2L,sBAAA,GAAIK,aAAa,CAACjM,iBAAiB,CAACP,aAAa,cAAAmM,sBAAA,eAA7CA,sBAAA,CAA+CO,SAAS,EAAE;QAAA,IAAAC,sBAAA;QAC1DJ,KAAK,IAAI,MAAM,IAAI,CAACnB,YAAY;QAChC;QACAwB,IAAI,CAACC,SAAS,CAACD,IAAI,CAACE,KAAK,EAAAH,sBAAA,GAACH,aAAa,CAACjM,iBAAiB,CAACP,aAAa,cAAA2M,sBAAA,uBAA7CA,sBAAA,CAA+CD,SAAS,CAAC,CAAC,CAAC;MACzF;MACAxB,UAAU,IAAIqB,KAAK;MACnB,OAAOA,KAAK;IAChB,CAAC,CAAC,CAAC;IACHrB,UAAU,IAAI,CAAC,CAAC,CAAC;IACjB,OAAO;MAAEA,UAAU;MAAES;IAAgB,CAAC;EAC1C;EACA,MAAMzE,mBAAmBA,CAAC6F,OAAO,EAAE1H,OAAO,EAAE;IACxC,MAAM2H,cAAc,GAAG,IAAI,CAACC,iBAAiB,CAAC5H,OAAO,CAAC;IACtD,OAAO,IAAI,CAAC6H,MAAM,CAACC,IAAI,CAAC,YAAY;MAChC,IAAI;QACA,MAAMC,GAAG,GAAG,MAAM,IAAI,CAACC,MAAM,CAACC,IAAI,CAACC,WAAW,CAACC,MAAM,CAACT,OAAO,EAAEC,cAAc,CAAC;QAC9E,OAAOI,GAAG;MACd,CAAC,CACD,OAAOK,CAAC,EAAE;QACN,MAAMC,KAAK,GAAG1O,qBAAqB,CAACyO,CAAC,CAAC;QACtC,MAAMC,KAAK;MACf;IACJ,CAAC,CAAC;EACN;EACAT,iBAAiBA,CAAC5H,OAAO,EAAE;IACvB,IAAI,CAAC,IAAI,CAACgI,MAAM,EAAE;MACd,MAAMM,oBAAoB,GAAG;QACzBnM,4BAA4B,EAAE,IAAI,CAACA,4BAA4B;QAC/DD,0BAA0B,EAAE,IAAI,CAACA,0BAA0B;QAC3DL,iBAAiB,EAAE,IAAI,CAACA,iBAAiB;QACzCwC,mBAAmB,EAAE,IAAI,CAACA,mBAAmB;QAC7CkB,OAAO,EAAE,IAAI,CAACF,YAAY,CAACE;MAC/B,CAAC;MACD,MAAMgJ,QAAQ,GAAG7O,WAAW,CAAC4O,oBAAoB,CAAC;MAClD,MAAMxI,MAAM,GAAG;QACX,GAAG,IAAI,CAACT,YAAY;QACpBE,OAAO,EAAEgJ,QAAQ;QACjBhK,OAAO,EAAE,IAAI,CAACA,OAAO;QACrBiK,UAAU,EAAE;MAChB,CAAC;MACD,IAAI,CAAC1I,MAAM,CAACP,OAAO,EAAE;QACjB,OAAOO,MAAM,CAACP,OAAO;MACzB;MACA,IAAI,CAACyI,MAAM,GAAG,IAAInP,YAAY,CAACiH,MAAM,CAAC;IAC1C;IACA,MAAM6H,cAAc,GAAG;MACnB,GAAG,IAAI,CAACtI,YAAY;MACpB,GAAGW;IACP,CAAC;IACD,IAAI,IAAI,CAACnE,iBAAiB,EAAE;MACxB8L,cAAc,CAAC/H,OAAO,GAAG;QACrB,SAAS,EAAE,IAAI,CAAC/D,iBAAiB;QACjC,GAAG8L,cAAc,CAAC/H;MACtB,CAAC;MACD+H,cAAc,CAACc,KAAK,GAAG;QACnB,aAAa,EAAE,IAAI,CAACxM,qBAAqB;QACzC,GAAG0L,cAAc,CAACc;MACtB,CAAC;IACL;IACA,OAAOd,cAAc;EACzB;EACAe,QAAQA,CAAA,EAAG;IACP,OAAO,QAAQ;EACnB;EACA;EACAC,iBAAiBA,CAAA,EAAgB;IAAA,SAAAC,IAAA,GAAAvB,SAAA,CAAAwB,MAAA,EAAZC,UAAU,OAAAvI,KAAA,CAAAqI,IAAA,GAAAG,IAAA,MAAAA,IAAA,GAAAH,IAAA,EAAAG,IAAA;MAAVD,UAAU,CAAAC,IAAA,IAAA1B,SAAA,CAAA0B,IAAA;IAAA;IAC3B,OAAOD,UAAU,CAACvC,MAAM,CAAC,CAACyC,GAAG,EAAEvE,SAAS,KAAK;MACzC,IAAIA,SAAS,IAAIA,SAAS,CAACrB,UAAU,EAAE;QAAA,IAAA6F,qBAAA,EAAAC,sBAAA,EAAAC,sBAAA;QACnCH,GAAG,CAAC5F,UAAU,CAACmB,gBAAgB,KAAA0E,qBAAA,GAC3BxE,SAAS,CAACrB,UAAU,CAACmB,gBAAgB,cAAA0E,qBAAA,cAAAA,qBAAA,GAAI,CAAC;QAC9CD,GAAG,CAAC5F,UAAU,CAACkB,YAAY,KAAA4E,sBAAA,GAAIzE,SAAS,CAACrB,UAAU,CAACkB,YAAY,cAAA4E,sBAAA,cAAAA,sBAAA,GAAI,CAAC;QACrEF,GAAG,CAAC5F,UAAU,CAACoB,WAAW,KAAA2E,sBAAA,GAAI1E,SAAS,CAACrB,UAAU,CAACoB,WAAW,cAAA2E,sBAAA,cAAAA,sBAAA,GAAI,CAAC;MACvE;MACA,OAAOH,GAAG;IACd,CAAC,EAAE;MACC5F,UAAU,EAAE;QACRmB,gBAAgB,EAAE,CAAC;QACnBD,YAAY,EAAE,CAAC;QACfE,WAAW,EAAE;MACjB;IACJ,CAAC,CAAC;EACN;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}